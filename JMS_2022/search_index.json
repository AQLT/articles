[["index.html", "Estimation en temps réel de la tendance-cycle : apport de lutilisation des filtres asymétriques dans la détection des points de retournement Résumé", " Estimation en temps réel de la tendance-cycle : apport de lutilisation des filtres asymétriques dans la détection des points de retournement Alain Quartier-la-Tente 2022-03-17 Résumé Résumé Lanalyse des cycles économiques, et en particulier la détection précoce des points de retournement, est un sujet majeur dans lanalyse de la conjoncture. Les moyennes mobiles, ou les filtres linéaires, sont omniprésents dans les méthodes dextraction de la tendance-cycle et dajustement saisonnier. Au centre de la série, des moyennes mobiles symétriques sont appliquées1. Cependant, en raison du manque dobservations futures, les estimations en temps réel doivent sappuyer sur des moyennes mobiles asymétriques. Cest ce qui est par exemple fait dans les méthodes de désaisonnalisation les plus utilisées, TRAMO-SEATS et X-13ARIMA, qui prolongent la série sur 1 an par un modèle ARIMA. Les prévisions étant des combinaisons linéaires du passé, cela revient en réalité à utiliser des moyennes mobiles asymétriques dont les coefficients sont optimisés par rapport à la prévision avec une longueur davance  one-step ahead forecasting. La construction de moyennes mobiles asymétriques performantes en termes de fidélité (préservation du signal), de révision, de lissage et de déphasage (délais dans la détection de points de retournement) est un sujet de recherche toujours ouvert. Cette étude décrit et compare des approches récentes pour la construction de moyennes mobiles asymétriques, utilisées pour lestimation en temps réel de la tendance-cycle : filtres polynomiaux locaux (Proietti et Luati 2008; Gray et Thomson 1996) ; méthodes basées sur une optimisation sous contrainte dune somme pondérée de critères de qualité des moyennes mobiles (Grun-Rehomme, Guggemos, et Ladiray 2018; Wildi et McElroy 2019) ; et filtres basés sur les espaces de Hilbert à noyau reproduisant (RKHS) Dagum et Bianconcini (2008). Elle montre également comment les filtres polynomiaux locaux peuvent être étendus pour inclure un critère de temporalité afin de minimiser le déphasage. Enfin, cette étude montre quil est possible détablir une approche unificatrice générale qui permet de reproduire lensemble des méthodes étudiées. La comparaison des méthodes sur séries simulées montre quil est important dadapter la longueur du filtre à la variabilité de la série. Par ailleurs, même si certains filtres RKHS sont sujet à des problèmes doptimisation et conduisent à des estimations intermédiaires erratiques, dautres semblent donner des résultats satisfaisant en termes de délais dans la détection des points de retournement et de révisions. De plus, lorsque la longueur du filtre est adapté à la variabilité de la série, chercher à conserver des tendances polynomiales de degré supérieur à un semble introduire de la variance dans les estimations (et donc plus de révisions) sans gain significatif en termes de détection de point de retournement. Cette étude est reproductible. En particulier, toutes les méthodes décrites sont implémentées dans le package rjdfilters (https://github.com/palatej/rjdfilters) et tous les codes utilisées sont disponibles sous https://github.com/AQLT/articles. Références "],["abstract.html", "Abstract", " Abstract Abstract This paper describes and compares different approaches to build asymmetric filters: local polynomials filters, methods based on an optimization of filters properties (Fidelity-Smoothness-Timeliness, FST, approach and a data-dependent filter) and filters based on Reproducing Kernel Hilbert Space. It also describes how local polynomials filters can be extended to include a timeliness criterion to minimize phase shift. All these methods can be seen as a special case of a general unifying framework to derive linear filters. This paper shows that, when the length of the filter is adapted to the variability of the series, constraining asymmetric filters to preserve constant trends (and not necessarily polynomial ones) reduce revision error and time lag. Therefore, future studies on the subject can focus on these filters. Moreover, with RKHS filters some optimisation issues can occurs and they might lead to erratic estimation; however, some of them seem to produce satisfying results in terms of phase-shift and revisions. All the methods are implemented in the package rjdfilters and the results can be easily reproduced. The programs used, and a web version of this report, are available at https://github.com/AQLT/articles. "],["introduction.html", "Introduction", " Introduction Lanalyse du cycle économique, et en particulier la détection rapide des points de retournement dune série, est un sujet de première importance dans lanalyse de la conjoncture économique. Pour cela, les indicateurs économiques sont généralement corrigés des variations saisonnières. Toutefois, afin daméliorer leur lisibilité, il peut être nécessaire deffectuer un lissage supplémentaire afin de réduire le bruit qui altère lanalyse de la composante tendance-cycle. Par construction, les méthodes dextraction de tendance-cycle sont étroitement liées aux méthodes de désaisonnalisation puisquelles sont généralement appliquées sur des séries corrigées des variations saisonnières. Les moyennes mobiles, ou les filtres linéaires, sont omniprésents dans les méthodes dextraction du cycle économique et dajustement saisonnier2. Ainsi, la méthode de désaisonnalisation X-13ARIMA-SEATS utilise des moyennes mobiles de Henderson et des moyennes mobiles composites pour estimer les principales composantes dune série chronologique, tandis que TRAMO-SEATS utilise des filtres de Wiener-Kolmogorov. Au centre de la série, des filtres symétriques sont appliqués. En revanche, en raison du manque dobservations futures, pour estimer les points les plus récents, toutes ces méthodes doivent sappuyer sur des filtres asymétriques. Par exemple, même si X-13ARIMA-SEATS et TRAMO-SEATS appliquent des moyennes symétriques aux prévisions obtenues à partir dun modèle ARIMA, cela revient à appliquer des filtres asymétriques en fin de série, car les valeurs prédites sont des combinaisons linéaires de valeurs passées. Si ces moyennes mobiles asymétriques ont de bonnes propriétés concernant la taille des révisions futures induites par le processus de lissage3, elles induisent également des déphasages qui retardent en général la détection en temps réel des points de retournement. Lobjectif de cette étude est de décrire et de comparer les approches récentes permettant lextraction de tendance-cycle : filtres polynomiaux locaux, méthodes basées sur une optimisation des propriétés des filtres et filtres basés sur les espaces de Hilbert à noyau reproduisant (RKHS). Toutes ces méthodes sont implémentées dans le package rjdfilters4, tous les résultats de cette étude sont facilement reproductibles. En raison du lien entre la désaisonnalisation et lextraction de tendance-cycle (section 1), nous nous concentrons sur les méthodes non paramétriques qui peuvent être incluses dans X-13ARIMA-SEATS. Après une description des propriétés générales dun filtre linéaire (section 2), nous décrivons une approche générale qui permet denglober les différentes méthodes développées par Proietti et Luati (2008), Gray et Thomson (1996), Grun-Rehomme, Guggemos, et Ladiray (2018), Wildi et McElroy (2019) et Dagum et Bianconcini (2008) (sections 3 à 5) et nous mettrons également en exergue les liens théoriques entre ces différentes variables. Enfin, dans la section 6, nous comparons les différentes méthodes en termes de déphasage et de révisions, en les appliquant sur des séries simulées et réelles. Références "],["sec-SAtoTCE.html", "Chapitre 1 De la désaisonnalisation à lestimation tendance-cycle", " Chapitre 1 De la désaisonnalisation à lestimation tendance-cycle La plupart des indicateurs macroéconomiques (PIB, production, consommation, etc.) sont affectés par des effets saisonniers et des effets jours ouvrables qui perturbent lanalyse des évolutions infra-annuelles et les comparaisons spatiales. Cest pourquoi les séries chronologiques sont généralement corrigées des variations saisonnières et des jours ouvrables, la désaisonnalisation étant le processus consistant à supprimer leurs effets. Pour effectuer la désaisonnalisation, les méthodes de désaisonnalisation les plus populaires sont TRAMO-SEATS, une méthode paramétrique basée sur les modèles ARIMA (voir par exemple Maravall et Caporello (2004)), et X-13ARIMA-SEATS, une méthode non-paramétrique basée sur les moyennes mobiles (voir par exemple Ladiray et Quenneville (2011)). Ces méthodes supposent que toute série temporelle \\(X_t\\) peut se décomposer en quatre composantes : Une composante saisonnière \\(S_t\\). Une composante jours ouvrables \\(D_t\\). Une composante tendance-cycle \\(TC_t\\) qui contient la tendance (qui représente les évolutions de long terme) et le cycle (qui représente les évolutions cycliques autour de la tendance). La tendance et le cycle nétant pas observés et étant difficiles à séparer, ils sont estimés de manière conjointes dans la désaisonnalisation. Une composante irrégulière \\(I_t\\) qui contient toutes les autres fluctuations. Toutes ces composantes étant inobservées, lestimation de lune dépend de lestimation des autres. Ainsi, même si dans ce rapport, nous nous intéresserons aux méthodes dextraction de tendance-cycle, celles-ci ne peuvent sétudier indépendamment du processus du désaisonnalisation. Ce lien explique également que toutes les méthodes utilisées dans ce rapport sont implémentées dans les bibliothèques de Java de JDemetra+5, le logiciel de désaisonnalisation recommandé par Eurostat. Une interface à lensemble des a été développée autour des méthodes présentées dans cette étude, grâce au package rjdfilters6. Les filtres linéaires (ou moyennes mobiles) sont omniprésents dans la désaisonnalisation et lestimation des différentes composantes. Au centre de la série, des filtres dits symétriques sont appliqués (pour estimer une composante à la date \\(t\\), on utilise autant de points après \\(t\\) quavant \\(t\\)). Pour extraire la tendance-cycle, le filtre symétrique le plus connu est celui de Henderson (1916), notamment utilisé dans lalgorithme X-13ARIMA. Cependant, en raison du manque dobservations futures, les estimations en temps réel doivent sappuyer sur des moyennes mobiles asymétriques. Les moyennes mobiles asymétriques classiques minimisent les erreurs de révision mais introduisent des retards dans la détection des points de retournement (appelé déphasage, voir section 2). Dans la littérature, différentes approches ont été envisagées pour lextraction de tendance-cycle en temps réel7. Parmi les plus récentes, on peut citer : Les Model-Based Approach  approches basées sur les modèles  supposent la spécification dun modèle stochastique pour la tendance (modèle ARIMA, modèle despace détat, etc.) et les estimations sont obtenues en minimisant une fonction de pénalité, généralement lerreur quadratique moyenne. Cest par exemple le cas du filtre de Kalman, du filtre de Wiener-Kolmogorov (utilisé dans TRAMO-SEATS) et de lApproche par Filtre Direct de Wildi et McElroy (2019) (section 4). Les méthodes dextraction non paramétriques ne supposent pas que la structure dun modèle est fixe et peuvent être facilement appliquées à nimporte quelle série temporelle. Cest par exemple le cas des filtres dHenderson et de Musgrave (1964) (utilisés dans X-13ARIMA). Les méthodes classiques peuvent être vues comme des régressions polynomiales locales, approche généralisée par Proietti et Luati (2008) (section 5.1). Les estimateurs non paramétriques peuvent également être reproduits en exploitant la méthodologie de lespace de Hilbert du noyau reproducteur (RKHS), comme cela est fait par Dagum et Bianconcini (2008) (section 5.4). Bien que ces auteurs aient proposé des approches générales pour construire des filtres linéaires, ils ne font référence quaux méthodes les plus classiques (Henderson, Musgrave, Hodrick-Prescott, etc.) sans faire le lien avec les autres méthodes récentes. Dans cette étude, nous proposons une approche unificatrice générale qui permettrait de reproduire lensemble de ces méthodes. Cela a un double intérêt. Dune part, cela permet de faire une première revue de la littérature sur les méthodes de construction des filtres linéaires pour lanalyse conjoncturelle. Dautre part, cela permet de montrer les liens entre toutes ces approches et de les comparer en utilisant une même méthodologie. Pour cela nous utiliserons lapproche générale de Grun-Rehomme, Guggemos, et Ladiray (2018) (section 3.1) qui ont également proposé une procédure globale pour construire des moyennes mobiles asymétriques permettant de minimiser les effets de déphasage (section 3.2). Des diagrammes synthétiques des liens entre les différentes méthodes étudiées sont présentés dans lannexe A. Nous nous intéresserons uniquement aux estimations intermédiaires de la tendance-cycle faites en temps réels. Le filtre symétrique utilisé pour les estimations finales ne sera pas remis en cause et sera celui dHenderson. Nous nous concentrons également sur les méthodes qui pourraient être implémentées dans X-13ARIMA. Pour maintenir la cohérence avec lapproche non paramétrique de X-13ARIMA, nous nous concentrons sur les méthodes dextraction non paramétriques. Cest pourquoi ni les filtres de lapproche de Wildi et McElroy (2019) (section 4), ni dautres approches basées sur des modèles ne sont, pour linstant, utilisées dans les simulations. Même si les différentes méthodes sont comparées en les appliquant sur des séries déjà désaisonnalisée, elles peuvent être intégrées dans lalgorithme de désaisonnalisation X-11 grâce à la fonction rjdfilters::x11(). Lensemble des filtres utilisés sont résumés dans lannexe B. Références "],["sec-propMM.html", "Chapitre 2 Quelques propriétés sur les moyennes mobiles 2.1 Gain et fonction de déphasage 2.2 Propriétés souhaitables dune moyenne mobile 2.3 Estimation en temps réel et moyennes mobiles asymétriques", " Chapitre 2 Quelques propriétés sur les moyennes mobiles Cette section présente les définitions et les propriétés des moyennes mobiles utiles pour comprendre les méthodes présentées dans les prochaines sections. Pour plus de détails sur les moyennes mobiles, voir par exemple Ladiray (2018). Soient deux entiers \\(p\\) et \\(f\\). Une moyenne mobile \\(M_\\theta\\) ou \\(M\\) est un opérateur linéaire définit par un ensemble de coefficients \\(\\theta=(\\theta_{-p},\\dots,\\theta_{f})&#39;\\) qui transforme toute série temporelle \\(X_t\\) en : \\[ M_\\theta(X_t)=\\sum_{k=-p}^{+f}\\theta_kX_{t+k}. \\] On a les définitions suivantes : La quantité \\(p+f+1\\) est appelée ordre de la moyenne mobile. Lorsque \\(p=f\\) la moyenne mobile est dite centrée. Si de plus on a \\(\\forall k:\\:\\theta_{-k} = \\theta_k\\), la moyenne mobile \\(M_\\theta\\) est dite symétrique. Dans ce cas, la quantité \\(h=p=f\\) est appelée fenêtre (bandwidth). 2.1 Gain et fonction de déphasage Soit \\(X_t=\\e^{-i\\omega t}\\) avec \\(\\omega\\in[0,\\pi]\\). La moyenne mobile \\(M_\\theta\\) transforme \\(X_t\\) en : \\[ Y_t = M_{\\theta}X_t = \\sum_{k=-p}^{+f} \\theta_k \\e^{-i \\omega (t+k)} = \\left(\\sum_{k=-p}^{+f} \\theta_k \\e^{-i \\omega k}\\right)\\cdot X_t. \\] La fonction \\(\\Gamma_\\theta(\\omega)=\\sum_{k=-p}^{+f} \\theta_k e^{-i \\omega k}\\) est appelée fonction de transfert ou fonction de réponse en fréquence (frequency response function)8. Elle peut être réécrite en : \\[ \\Gamma_\\theta(\\omega) = G_\\theta(\\omega)\\e^{-i\\Phi_\\theta(\\omega)}, \\] où \\(G_\\theta(\\omega)=\\lvert\\Gamma_\\theta(\\omega)\\rvert\\) est la fonction de gain ou damplitude et \\(\\Phi_\\theta(\\omega)\\) est le déphasage (phase shift ou time shift)9. Pour tous les filtres symétriques on a \\(\\Phi_\\theta(\\omega)\\equiv 0 \\;(modulo\\;{\\pi})\\). En somme, appliquer une moyenne mobile à une série harmonique la modifie de deux façons : en la multipliant par un coefficient égal à \\(G_{\\theta}\\left(\\omega\\right)\\) (gain) ; en la « décalant » dans le temps de \\(\\Phi_\\theta(\\omega)/\\omega\\), ce qui a un impact sur la détection des points de retournement (déphasage).10 2.2 Propriétés souhaitables dune moyenne mobile Pour décomposer une série temporelle en une composante saisonnière, une tendance-cycle et lirrégulier, lalgorithme de décomposition X-11 (utilisé dans X-13ARIMA) utilise une succession de moyennes mobiles ayant toutes des contraintes spécifiques. Dans cette sous-section nous décrivons deux types de contraintes : la préservation de certaines tendances ; la réduction du bruit. 2.2.1 Préservation de tendances Il est souvent souhaitable quune moyenne mobile conserve certaines tendances. Une moyenne mobile \\(M_\\theta\\) conserve une fonction du temps \\(f(t)\\) si \\(\\forall t:\\:M_\\theta f(t)=f(t)\\). Nous avons les propriétés suivantes pour la moyenne mobile \\(M_\\theta\\) : Pour conserver les constantes \\(X_t=a\\) il faut que \\[ \\forall t:M_\\theta(X_t)=\\sum_{k=-p}^{+f}\\theta_kX_{t+k}=\\sum_{k=-p}^{+f}\\theta_ka=a\\sum_{k=-p}^{+f}\\theta_k=a. \\] Cest-à-dire quil faut que la somme des coefficients \\(\\sum_{k=-p}^{+f}\\theta_k\\) soit égale à \\(1\\). Pour conserver les tendances linéaires \\(X_t=at+b\\) il faut que : \\[ \\forall t:\\:M_\\theta(X_t)=\\sum_{k=-p}^{+f}\\theta_kX_{t+k}=\\sum_{k=-p}^{+f}\\theta_k[a(t+k)+b]=a\\sum_{k=-p}^{+f}k\\theta_k+(at+b)\\sum_{k=-p}^{+f}\\theta_k=at+b. \\] Ce qui est équivalent à : \\[ \\sum_{k=-p}^{+f}\\theta_k=1 \\quad\\text{et}\\quad \\sum_{k=-p}^{+f}k\\theta_k=0. \\] De manière générale, \\(M_\\theta\\) conserves les tendances de degré \\(d\\) si et seulement si : \\[ \\sum_{k=-p}^{+f}\\theta_k=1 \\text{ et } \\forall j \\in \\left\\llbracket 1,d\\right\\rrbracket:\\: \\sum_{k=-p}^{+f}k^j\\theta_k=0. \\] Si \\(M_\\theta\\) est symétrique (\\(p=f\\) et \\(\\theta_{-k} = \\theta_k\\)) et conserve les tendances de degré \\(2d\\) alors elle conserve aussi les tendances de degré \\(2d+1\\). 2.2.2 Réduction du bruit Toutes les séries temporelles sont affectées par du bruit qui peut brouiller lextraction du signal. Cest pourquoi on cherche à réduire ce bruit (en réduisant sa variance) tout en conservant le signal (en utilisant les propriétés vues dans les sections précédentes). La somme des carrés des coefficients \\(\\sum_{k=-p}^{+f}\\theta_k^2\\) est le rapport de réduction de la variance. En effet, soit \\(\\{\\varepsilon_t\\}\\) une suite de variables aléatoires indépendantes avec \\(\\E{\\varepsilon_t}=0\\), \\(\\V{\\varepsilon_t}=\\sigma^2\\). On a : \\[ \\V{M_\\theta\\varepsilon_t}=\\V{\\sum_{k=-p}^{+f} \\theta_k \\varepsilon_{t+k}} = \\sum_{k=-p}^{+f} \\theta_k^2 \\V{\\varepsilon_{t+k}}= \\sigma^2\\sum_{k=-p}^{+f} \\theta_k^2. \\] 2.3 Estimation en temps réel et moyennes mobiles asymétriques Pour les filtres symétriques, la fonction de déphasage est égale à zéro (modulo \\(\\pi\\)). Il ny a donc aucun retard dans la détection de points de retournement. Ils ne peuvent toutefois pas être utilisés au début et à la fin de la série car aucune valeur passée/future nest connue. 2.3.1 Moyennes mobiles asymétriques et prévision En début et en fin de série, les moyennes mobiles asymétriques ne peuvent être utilisées du fait du manque de données disponibles. Une solution est de prolonger la série par prévision pour ensuite appliquer le filtre symétrique. Cette méthode semble remonter à De Forest (1877) qui suggère également de modéliser en fin de période une tendance polynomiale de degré au plus trois : « As the first \\(m\\) and last \\(m\\) terms of the series cannot be reached directly by the formula, the series should be graphically extended by m terms at both ends, first plotting the observations on paper as ordinates, and then extending the curve along what seems to be its probable course, and measuring the ordinates of the extended portions. It is not necessary that this extension should coincide with what would be the true course of the curve in those parts. The important point is that the m terms thus added, taken together with the \\(m+1\\) adjacent given terms, should follow a curve whose form is approximately algebraic and of a degree not higher than the third. » Cest également lapproche utilisée dans les méthodes de désaisonnalisation TRAMO-SEATS et X-13ARIMA qui prolongent la série sur 1 an par un modèle ARIMA. In fine, cela revient à utiliser des moyennes mobiles asymétriques puisque les prévisions sont des combinaisons linéaires du passé ! Inversement, à partir dune moyenne mobile symétrique de référence, on peut déduire les prévisions implicites dune moyenne mobile asymétrique. Notons \\(v=(v_{-h},\\dots, v_{h})\\) la moyenne mobile symétrique de référence et \\(w^0,\\dots w^{h-1}\\) une suite de moyennes mobiles asymétriques, dordre \\(h+1\\) à \\(2h\\) utilisée pour lestimation des \\(h\\) derniers points avec, pour convention, \\(w_t^q=0\\) pour \\(t&gt;q\\). Cest-à-dire que \\(w^0=(w_{-h}^0,\\dots, w_{0}^0)\\) est utilisée pour lestimation en temps réel (lorsque lon ne connait aucun point dans le futur), \\(w^1=(w_{-h}^1,\\dots, w_{1}^1)\\) pour lestimation de lavant-dernier point (lorsque lon ne connait quun point dans le futur), etc. Notons également \\(y_{-h},\\dots,y_{0}\\) la série étudiée observée et \\(y_{1}^*,\\dots y_h^*\\) la prévision implicite induite par \\(w^0,\\dots w^{h-1}\\). Cela signifie, que pour tout \\(q\\) on a : \\[ \\forall q, \\quad \\underbrace{\\sum_{i=-h}^0 v_iy_i + \\sum_{i=1}^h v_iy_i*}_{\\text{lissage par }v\\text{ de la série prolongée}} =\\underbrace{\\sum_{i=-h}^0 w_i^qy_i + \\sum_{i=1}^h w_i^qy_i*}_{\\text{lissage par }w^q\\text{ de la série prolongée}}. \\] Ce qui est équivalent à : \\[ \\forall q, \\quad \\sum_{i=1}^h (v_i- w_i^q) y_i^** =\\sum_{i=-h}^0 (w_i^q-v_i)y_i. \\] En somme, matriciellement, cela revient donc à résoudre : \\[\\scriptstyle \\begin{pmatrix} v_1 &amp; v_2 &amp; \\cdots &amp; v_h \\\\ v_1 - w_1^1 &amp; v_2 &amp; \\cdots &amp; v_h \\\\ \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots \\\\ v_1 - w_1^{h-1} &amp; v_2-w_2^{h-1} &amp; \\cdots &amp; v_h \\end{pmatrix} \\begin{pmatrix}y_1^* \\\\ \\vdots \\\\ y_h^*\\end{pmatrix}= \\begin{pmatrix} w_{-h}^0 - v_{-h} &amp; w_{-(h-1)}^0 - v_{-(h-1)} &amp; \\cdots &amp; w_{0}^0 - v_{0} \\\\ w_{-h}^1 - v_{-h} &amp; w_{-(h-1)}^1 - v_{-(h-1)} &amp; \\cdots &amp; w_{0}^1 - v_{0} \\\\ \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots \\\\ w_{-h}^{h-1} - v_{-h} &amp; w_{-(h-1)}^{h-1} - v_{-(h-1)} &amp; \\cdots &amp; w_{0}^{h-1} - v_{0} \\end{pmatrix} \\begin{pmatrix}y_{-h} \\\\ \\vdots \\\\ y_0\\end{pmatrix}.\\] Cest ce qui implémenté dans la fonction rjdfilters::implicit_forecast. Comme notamment souligné par Wildi et Schips (2004), étendre la série par prévision dun modèle ARIMA revient à calculer des filtres asymétriques dont les coefficients sont optimisés par rapport à la prévision avec une longueur davance  one-step ahead forecasting. Autrement dit, on cherche à minimiser les révisions entre la première et la dernière estimation (avec le filtre symétrique). Cependant, puisque les coefficients du filtre symétrique décroissent lentement, il faudrait également sintéresser à la performance des prévisions avec plusieurs longueurs davance  multi-step ahead forecasting. Par ailleurs, le déphasage induit par les filtres asymétriques nest pas contrôlé : on pourrait préférer avoir une détection plus rapide des points de retournement et une révision plus grande plutôt que de juste minimiser les révisions entre la première et la dernière estimation. Cest pourquoi il peut être nécessaire de définir des critères alternatifs pour juger la qualité des moyennes mobiles asymétriques. 2.3.2 Indicateurs de qualité des moyennes mobiles asymétriques Pour les filtres asymétriques, la majorité des critères proviennent de ceux définis par Grun-Rehomme, Guggemos, et Ladiray (2018) et Wildi et McElroy (2019) pour construire les filtres asymétriques. Ils sont résumés dans le tableau 2.1 et calculables avec la fonction fonction rjdfilters::diagnostic_matrix. Grun-Rehomme, Guggemos, et Ladiray (2018) proposent une approche générale pour dériver des filtres linéaires, basée sur un problème doptimisation de trois critères : Fidelity (\\(F_g\\), réduction de la variance), Smoothness (\\(S_g\\), lissage) et Timeliness (\\(T_g\\), déphasage). Voir section 3.2 pour plus de détails. Wildi et McElroy (2019) proposent une approche basée sur la décomposition de lerreur quadratique moyenne entre le filtre symétrique et le filtre asymétrique en quatre quantités : Accuracy (\\(A_w\\), précision), Timeliness (\\(T_w\\), déphasage), Smoothness (\\(S_w\\), lissage) et Residual (\\(R_w\\), résidus). Voir section 4 pour plus de détails. Table 2.1 : Critères de qualité dune moyenne mobile \\(\\theta=(\\theta_k)_{-p\\leq k\\leq f}\\) définie par une fonction de gain \\(\\rho_{\\theta}\\) et une fonction de déphasage \\(\\varphi_\\theta\\). Sigle Description Formule \\(b_c\\) Biais constant \\(\\sum_{k=-p}^{+f}\\theta_{k}-1\\) \\(b_l\\) Biais linéaire \\(\\sum_{k=-p}^{+f}k\\theta_{k}\\) \\(b_q\\) Biais quadratique \\(\\sum_{k=-p}^{+f}k^{2}\\theta_{k}\\) \\(F_g\\) Réduction de la variance / Fidelity (Guggemos) \\(\\sum_{k=-p}^{+f}\\theta_{k}^{2}\\) \\(S_g\\) Smoothness (Guggemos) \\(\\sum_{j}(\\nabla^{3}\\theta_{j})^{2}\\) \\(T_g\\) Timeliness (Guggemos) \\(\\int_{0}^{\\omega_1}\\rho_{\\theta}(\\omega)\\sin(\\varphi_{\\theta}(\\omega))^{2}\\ud\\omega\\) \\(A_w\\) Accuracy (Wildi) \\(2\\int_0^{\\omega_1}\\left(\\rho_{s}(\\omega)-\\rho_{\\theta}(\\omega)\\right)^{2}h(\\omega)\\ud\\omega\\) \\(T_w\\) Timeliness (Wildi) \\(8\\int_0^{\\omega_1} \\rho_{s}(\\omega)\\rho_{\\theta}(\\omega)\\sin^{2}\\left(\\frac{\\varphi_s(\\omega)-\\varphi_\\theta(\\omega)}{2}\\right)h(\\omega)\\ud\\omega\\) \\(S_w\\) Smoothness (Wildi) \\(2\\int_{\\omega_1}^{\\pi}\\left(\\rho_{s}(\\omega)-\\rho_{\\theta}(\\omega)\\right)^{2}h(\\omega)\\ud\\omega\\) \\(R_w\\) Residual (Wildi) \\(8\\int_{\\omega_1}^{\\pi} \\rho_{s}(\\omega)\\rho_{\\theta}(\\omega)\\sin^{2}\\left(\\frac{\\varphi_s(\\omega)-\\varphi_\\theta(\\omega)}{2}\\right)h(\\omega)\\ud\\omega\\) \\(X_g\\) critères provenant de Grun-Rehomme, Guggemos, et Ladiray (2018) et \\(X_w\\) critères provenant de Wildi et McElroy (2019). \\(\\rho_s\\) et \\(\\varphi_s\\) représentent le gain et la fonction de déphasage du filtre symétrique dHenderson. \\(h\\) est la densité spectrale de la série en entrée, fixée celle dun bruit blanc, \\(h_{WN}(x)=1\\), ou dune marche aléatoire, \\(h_{RW}(x)=\\frac{1}{2(1-\\cos(x))}\\). Références "],["sec-theoriegen.html", "Chapitre 3 Dune théorie générale sur la construction des filtres asymétriques à lapproche FST 3.1 Théorie générale de construction des filtres asymétriques 3.2 Approche Fidelity-Smoothness-Timeliness (FST)", " Chapitre 3 Dune théorie générale sur la construction des filtres asymétriques à lapproche FST 3.1 Théorie générale de construction des filtres asymétriques Pour établir une théorie générale englobant les principaux filtres linéaires, Grun-Rehomme, Guggemos, et Ladiray (2018) définissent deux critères. En changeant légèrement les notations utilisées par les auteurs afin davoir une formulation plus générale, ces deux critères peuvent sécrire : \\[\\begin{align} I(\\theta,q,y_t,u_t)&amp;=\\E{(\\Delta^{q}M_\\theta y_t-u_t)^{2}} \\tag{3.1} \\\\ J(\\theta,f, \\omega_1,\\omega_2)&amp;=\\int_{\\omega_1}^{\\omega_2} f\\left[\\phi_\\theta(\\omega), \\varphi_\\theta (\\omega), \\omega\\right] \\ud \\omega \\tag{3.2} \\end{align}\\] où \\(y_t\\) est la série étudiée, \\(u_t\\) une série de référence et \\(\\Delta\\) est lopérateur différence (\\(\\Delta y_t=y_t-y_{t-1}\\) et \\(\\Delta^q=\\underbrace{\\Delta \\circ \\dots \\circ \\Delta}_{q\\text{ fois}}\\) pour \\(q\\in\\N\\)). Dans la majorité des cas, la fonction \\(f\\) ne dépendra que de la fonction de gain, \\(\\phi_\\theta\\), et de la fonction de déphasage, \\(\\varphi_\\theta\\). Dans ce cas, par simplification on écrira \\(f\\left[\\phi_\\theta(\\omega), \\varphi_\\theta (\\omega), \\omega\\right] = f\\left[\\phi_\\theta(\\omega), \\varphi_\\theta (\\omega)\\right]\\). Comme montré dans cette étude, la majorité des filtres linéaires peut sobtenir par une minimisation dune somme pondérée de ces critères, sous contrainte linéaire sur les coefficients : \\[ \\begin{cases} \\underset{\\theta}{\\min} &amp; \\sum \\alpha_i I(\\theta,\\, q_i,\\, y_t,\\, u_t^{(i)})+ \\beta_iJ(\\theta,\\, f_i,\\, \\omega_{1,i},\\, \\omega_{2,i})\\\\ s.t. &amp; C\\theta=a \\end{cases} \\] Cest en particulier le cas de lapproche Fidelity-Smoothness-Timeliness (FST) développée par les mêmes auteurs. 3.2 Approche Fidelity-Smoothness-Timeliness (FST) Pour construire les moyennes mobiles symétriques, Grun-Rehomme et Ladiray (1994) et Gray et Thomson (1996) proposent un programme de minimisation sous contrainte qui fait un compromis entre réduction de la variance et « lissage » de la tendance. Grun-Rehomme, Guggemos, et Ladiray (2018) étendent ces approches en les appliquant à la construction des filtres asymétriques et en ajoutant un critère permettant de contrôler le déphasage. Il sagit de lapproche Fidelity-Smoothness-Timeliness  Fidélité-Lissage-Temporalité  (FST). Pour la construction des filtres asymétriques, un quatrième critère pourrait également être rajouté qui prendrait en compte les révisions par rapport à lutilisation dun filtre symétrique de référence (cette méthode pourrait alors être appelée la méthode FRST  Fidelity-Revisions-Smoothness-Timeliness). Cependant, le package rjdfilters nayant implémenté que lapproche FST, nous nous restreignons dans cette étude quà la description de lapproche sans critère de révision. Les trois critères utilisés sont les suivants : Fidelity (fidélité), \\(F_g\\) : cest le rapport de réduction de la variance. Plus il est petit et plus le signal de sortie (tendance-cycle estimée) est un bon estimateur du signal à estimer (tendance-cycle). \\[ F_g(\\theta) = \\sum_{k=-p}^{+f}\\theta_{k}^{2}. \\] \\(F_g\\) peut également être écrite comme une forme quadratique positive : \\(F_g(\\theta)=\\theta&#39;F\\theta\\) avec \\(F\\) la matrice identité dordre \\(p+f+1\\). Smoothness (lissage), \\(S_g\\) : \\[ S_g(\\theta) = \\sum_{j}(\\nabla^{d}\\theta_{j})^{2}. \\] Ce critère mesure la proximité du signal de sortie à une tendance polynomiale de degré \\(d-1\\). Avec \\(d=3\\), Henderson utilise ce critère pour construire des moyennes mobiles conservant des polynômes de degré 2. \\(S_g\\) peut également sécrire sous une forme quadratique positive \\(S_g(\\theta)=\\theta&#39;S\\theta\\) avec \\(S\\) une matrice symétrique dordre \\(p+f+1\\) (voir section 5.5). Timeliness (temporalité), \\(T_g\\) : il mesure le déphasage entre le signal dentrée et le signal de sortie à des fréquences spécifiques. Lorsquun filtre linéaire est appliqué, le niveau du signal dentrée est également modifié par la fonction de gain : il est donc intuitif de considérer que plus le gain est élevé, plus limpact du déphasage le sera. Cest pourquoi le critère de déphasage dépend des fonctions de gain et de déphasage (\\(\\rho_\\theta\\) et \\(\\varphi_{\\theta}\\)), le lien entre les deux fonctions étant fait à partir dune fonction de pénalité \\(f\\)11 : \\[ \\int_{\\omega_{1}}^{\\omega_{2}}f(\\rho_{\\theta}(\\omega),\\varphi_{\\theta}(\\omega))\\ud\\omega. \\] Comme fonction de pénalité, les auteurs suggèrent de prendre \\(f\\colon(\\rho,\\varphi)\\mapsto\\rho^2\\sin(\\varphi)^2\\). Cela permet notamment davoir une timeliness qui peut sécrire comme une forme quadratique positive (\\(T_g(\\theta)=\\theta&#39;T\\theta\\) avec \\(T\\) une matrice carré symétrique dordre \\(p+f+1\\), voir Grun-Rehomme, Guggemos, et Ladiray (2018) pour la démonstration). Dans cet article nous utilisons \\(\\omega_1=0\\) et \\(\\omega_2=2\\pi/12\\) : on ne sintéresse quà des séries mensuelles et au déphasage qui impactent les cycles dau minimum 12 mois. En somme, lapproche FST consiste à minimiser une somme pondérée de ces trois critères sous certaines contraintes (généralement préservation polynomiale). \\[\\begin{equation} \\begin{cases} \\underset{\\theta}{\\min} &amp; \\alpha F_g(\\theta)+\\beta S_g(\\theta)+\\gamma T_g(\\theta) = \\theta&#39;(F+S+T)\\theta\\\\ s.t. &amp; C\\theta=a \\end{cases}. \\tag{3.3} \\end{equation}\\] Les conditions \\(0\\leq\\alpha,\\beta,\\gamma\\leq 1\\) et \\(\\alpha+\\beta\\ne0\\) garantissent que \\(\\alpha F_g(\\theta)+\\beta S_g(\\theta)+\\gamma T_g(\\theta)\\) soit strictement convexe et donc lunicité de la solution. Dans ce cas, la solution sécrit \\(\\hat \\theta = [\\alpha F+\\beta S+ \\gamma T]^{-1}C&#39;\\left(C[\\alpha F+\\beta S+ \\gamma T]^{-1}C&#39;\\right)^{-1}a\\). Un autre avantage de cette approche est que les filtres asymétriques construits sont totalement indépendants des données, de la date destimation et du filtre symétrique choisis. On obtient par exemple le filtre dHenderson avec les paramètres suivants : \\[C=\\begin{pmatrix} 1 &amp; \\cdots&amp;1\\\\ -h &amp; \\cdots&amp;h \\\\ (-h)^2 &amp; \\cdots&amp;h^2 \\end{pmatrix},\\quad a=\\begin{pmatrix} 1 \\\\0\\\\0 \\end{pmatrix},\\quad \\alpha=\\gamma=0,\\quad \\beta=1,\\quad d=3.\\] Un des inconvénients de cette approche est que les différents critères ne sont pas normalisés : leurs valeurs ne peuvent pas être comparées et nont donc pas de sens. Il ny a, par exemple, pas dinterprétation à donner un poids deux fois plus important à la timeliness quà la fidelity. Les trois critères utilisés dans le programme de minimisation (3.3) sont des cas particuliers de ceux définis dans la section 3.1. En effet, en notant \\(y_t=TC_t+\\varepsilon_t,\\quad\\varepsilon_t\\sim\\Norm(0,\\sigma^2)\\) avec \\(TC_t\\) une tendance déterministe, on a : \\[\\begin{align*} F_g(\\theta) &amp; = I(\\theta,\\,0,\\,y_t,\\,\\E{M_\\theta y_t})\\\\ S_g(\\theta) &amp; = I(\\theta,\\,q,\\,y_t,\\,\\E{M_\\theta y_t})\\\\ T_g(\\theta) &amp; = J(f\\colon(\\rho,\\varphi)\\mapsto\\rho^2\\sin(\\varphi)^2,\\,\\omega_1, \\,\\omega_2). \\end{align*}\\] Filtres FST - Grun-Rehomme, Guggemos, et Ladiray (2018) Avantages : Le filtre asymétrique est indépendant du filtre symétrique, des données et de la date destimation. Le problème doptimisation admet une solution unique. Inconvénients : Les différents critères ne sont pas normalisés : les poids accordés aux différents critères ne peuvent être comparés. Fonction : rjdfilters::fst_filter(). Références "],["sec-WildiMcLeroy.html", "Chapitre 4 Filtres dépendant des données : trilemme ATS", " Chapitre 4 Filtres dépendant des données : trilemme ATS Wildi et McElroy (2019) proposent une approche dépendante des données (data-dependent) pour calculer des filtres linéaires. Ils décomposent lerreur quadratique moyenne de révision en un trilemme de trois quantités appelées accuracy (précision), timeliness (temporalité) et smoothness (lissage), doù son nom ATS-trilemna. Soient : \\(\\left\\{ y_{t}\\right\\}\\) notre série temporelle en entrée12. \\(\\left\\{TC_{t}\\right\\}\\) le signal cible, i.e. la série lissée avec un filtre symétrique fini ou non, et soient respectivement \\(\\Gamma_s\\), \\(\\rho_s\\) et \\(\\varphi_s\\) les fonctions de transfert, de gain et de déphasage associées à ce filtre symétrique. \\(\\left\\{\\widehat{TC}_{t}\\right\\}\\) une estimation de \\(\\left\\{TC_{t}\\right\\}\\), i.e. le résultat dun filtre asymétrique (lorsque toutes les observations ne sont pas disponibles), et soient respectivement \\(\\Gamma_\\theta\\), \\(\\rho_\\theta\\) et \\(\\varphi_\\theta\\) les fonctions de transfert, de gain et de déphasage associées à ce filtre asymétrique. Une approche directe13, Direct Filter Approach (DFA), consiste à approcher directement le signal cible par minimisation de lerreur quadratique moyenne : \\[ \\underset{\\widehat{TC}_{t}}{\\min} \\E{(TC_{t}-\\widehat{TC}_{t})^{2}}. \\] Cette approche peut être approfondie en décomposant lerreur de quadratique moyenne en plusieurs éléments dintérêt. Si lon suppose que la série \\(\\left\\{ y_{t}\\right\\}\\) est faiblement stationnaire avec une densité spectrale continue \\(h\\), lerreur quadratique moyenne de révision, \\(\\E{(TC_{t}-\\widehat{TC}_{t})^{2}}\\), peut sécrire dans le domaine spectral comme: \\[\\begin{equation} \\E{(TC_{t}-\\widehat{TC}_{t})^{2}}=\\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}\\left|\\Gamma_s(\\omega)-{\\Gamma_\\theta}(\\omega)\\right|^{2}h(\\omega)\\ud\\omega=\\frac{1}{2\\pi}\\times2\\times\\int_{0}^{\\pi}\\left|\\Gamma_s(\\omega)-{\\Gamma_\\theta}(\\omega)\\right|^{2}h(\\omega)\\ud\\omega \\tag{4.1} \\end{equation}\\] Cette égalité peut également se généraliser aux processus intégrés non-stationnaires (par exemple en imposant une cointégration entre les deux signaux et en utilisant le pseudo-spectre, voir Wildi et McElroy (2013)). On a : \\[\\begin{align} \\left|\\Gamma_s(\\omega)-\\Gamma_\\theta(\\omega)\\right|^{2} &amp; =\\rho_s(\\omega)^{2}+\\rho_\\theta(\\omega)^{2}+2\\rho_s(\\omega)\\rho_\\theta(\\omega)\\left(1-\\cos(\\varphi_s(\\omega)-\\varphi_\\theta(\\omega)\\right) \\nonumber\\\\ &amp; =\\left(\\rho_s(\\omega)-\\rho_\\theta(\\omega)\\right)^{2}+4\\rho_s(\\omega)\\rho_\\theta(\\omega)\\sin^{2}\\left(\\frac{\\varphi_s(\\omega)-\\varphi_\\theta(\\omega)}{2}\\right) \\tag{4.2} \\end{align}\\] Lintervalle \\([0,\\pi]\\) peut être coupé en deux : une partie dite pass-band \\([0,\\omega_1]\\) (lintervalle de fréquences qui contient le signal cible) et une partie dite stop-band \\([\\omega_1,\\pi]\\) (lintervalle de fréquences associé aux résidus). Lerreur de léquation (4.1) peut être décomposée en 4 quantités : \\[\\begin{align*} Accuracy =A_w(\\theta)&amp;= 2\\int_0^{\\omega_1}\\left(\\rho_s(\\omega)-\\rho_\\theta(\\omega)\\right)^{2}h(\\omega)\\ud\\omega\\\\ Timeliness =T_w(\\theta)&amp;= 8\\int_0^{\\omega_1}\\rho_s(\\omega)\\rho_\\theta(\\omega)\\sin^{2}\\left(\\frac{\\varphi_\\theta(\\omega)}{2}\\right)h(\\omega)\\ud\\omega\\\\ Smoothness =S_w(\\theta)&amp;= 2\\int_{\\omega_1}^\\pi\\left(\\rho_s(\\omega)-\\rho_\\theta(\\omega)\\right)^{2}h(\\omega)\\ud\\omega\\\\ Residual =R_w(\\theta)&amp;= 8\\int_{\\omega_1}^\\pi\\rho_s(\\omega)\\rho_\\theta(\\omega)\\sin^{2}\\left(\\frac{\\varphi_\\theta(\\omega)}{2}\\right)h(\\omega)\\ud\\omega\\\\ \\end{align*}\\] Afin davoir des définitions cohérentes entre les différentes sections, les formules des quatre critères ont été légèrement modifiées par rapport à ceux définis par Wildi et McElroy (2019) : dans cette étude, lintervalle dintégration est \\([0,\\pi]\\) plutôt que \\([-\\pi,\\pi]\\) (toutes les fonctions étant paires cela revient à multiplier par 2 toutes les intégrales) ; dans larticle originel, lintervalle pass-band dépend de la fonction de gain du filtre symétrique (pass-band\\(=\\{\\omega |\\rho_s(\\omega)\\geq 0,5\\}\\)) : cela correspond donc à lintervalle contenant les fréquences conservées sans trop de distorsion par le filtre symétrique. Dans le cas du filtre symétrique dHenderson de 13 termes, cela correspond à lintervalle \\([0, 2\\pi/8]\\), cest-à-dire aux cycles de plus de 8 mois. En général, le résidu \\(R_w\\) est petit puisque \\(\\rho_s(\\omega)\\rho_\\theta(\\omega)\\) est proche de 0 dans lintervalle stop-band14. Il est de plus rare que les utilisateurs sintéressent aux propriétés de déphasage dans les fréquences stop-band. Cest pourquoi, pour la construction de filtres linéaires les auteurs suggèrent de faire une minimisation dune somme pondérée des trois premiers critères : \\[ \\mathcal{M}(\\vartheta_{1},\\vartheta_{2})=\\vartheta_{1}T_w(\\theta)+\\vartheta_{2}S_w(\\theta)+(1-\\vartheta_{1}-\\vartheta_{2})A_w(\\theta). \\] Comme le montrent McElroy et Wildi (2020), cette méthode peut également être étendue au cas multivarié, ce qui permet de prendre en compte les corrélations entre les composantes de différentes séries. Un des inconvénients de cette méthode est quil ny a pas de garantie dunicité de la solution. En revanche, son avantage par rapport à la méthode FST (section 3.2) est que la décomposition de lerreur quadratique moyenne permet de normaliser les différents indicateurs, et les coefficients \\(\\vartheta_{1}\\), \\(\\vartheta_{2}\\) et \\(1-\\vartheta_{1}-\\vartheta_{2}\\) peuvent être comparés entre eux. Les quatre critères \\(A_w\\), \\(T_w\\), \\(S_w\\) et \\(R_w\\) étant des cas particuliers du critère \\(J\\) défini dans léquation (3.1), cette méthode sinscrit dans le cadre de la théorie générale définie dans la section 3.1. En effet, en notant : \\[ \\begin{cases} f_1\\colon&amp;(\\rho,\\varphi, \\omega)\\mapsto2\\left(\\rho_s(\\omega)-\\rho\\right)^{2}h(\\omega) \\\\ f_2\\colon&amp;(\\rho,\\varphi, \\omega)\\mapsto8\\rho_s(\\omega)\\rho\\sin^{2}\\left(\\frac{\\varphi}{2}\\right)h(\\omega) \\end{cases} \\] on a : \\[\\begin{align*} A_w(\\theta)&amp;= J(\\theta,f_1,0,\\omega_1)\\\\ T_w(\\theta)&amp;= J(\\theta,f_2,0,\\omega_1)\\\\ S_w(\\theta)&amp;= J(\\theta,f_1,\\omega_1,\\pi)\\\\ R_w(\\theta)&amp;= J(\\theta,f_2,\\omega_1,\\pi). \\end{align*}\\] Cette méthode étant totalement dépendante des données, son intégration dans des algorithmes non-paramétriques tels que X-11 serait compliquée. Cest pourquoi elle nest pour linstant pas comparée aux autres. Pour avoir des critères qui ne dépendent pas des données, une idée serait de fixer la densité spectrale, par exemple à celle dun bruit blanc (\\(h_{WN}(x)=1\\)) ou dune marche aléatoire (\\(h_{RW}(x)=\\frac{1}{2(1-\\cos(x))}\\)). Cest ce qui est implémenté dans la fonction rjdfilters::dfa_filter(). ATS-trilemna - Wildi et McElroy (2019) Avantages : Les valeurs des différents critères peuvent être comparées et les poids associés peuvent être interprétés. Méthode généralisable aux cas multivariés. Inconvénients : Les filtres dépendent des données, de la date destimation et du filtre symétrique utilisé. Il peut y avoir des problèmes doptimisation (plusieurs minimums, etc.). : package MDFA https://github.com/wiaidp/MDFA ou rjdfilters::dfa_filter() (version simplifiée). Références "],["sec-nonparamreg.html", "Chapitre 5 Régression non paramétrique et régression polynomiale locale 5.1 Régression polynomiale : approche de Proietti et Luati 5.2 Extension avec le critère de timeliness 5.3 Régression polynomiale : Gray et Thomson 5.4 Reproducing Kernel Hilbert Space (RKHS) : approche de Dagum et Bianconcini 5.5 Liens entre les différentes méthodes", " Chapitre 5 Régression non paramétrique et régression polynomiale locale Comme notamment montré par Loader (1999), la régression locale est un cas particulier de la régression non paramétrique. Supposons que lon ait un ensemble de points \\((x_i,y_i)_{1\\leq i\\leq n}\\). La régression non paramétrique consiste à supposer quil existe une fonction \\(\\mu\\), à estimer, telle que \\(y_i=\\mu(x_i)+\\varepsilon_i\\) avec \\(\\varepsilon_i\\) un terme derreur. Daprès le théorème de Taylor, pour tout point \\(x_0\\), si \\(\\mu\\) est différentiable \\(d\\) fois, alors : \\[ \\forall x \\::\\:\\mu(x) = \\mu(x_0) + \\mu&#39;(x_0)(x-x_0)+\\dots + \\frac{\\mu^{(d)}(x_0)}{d!}(x-a)^d+R_d(x), \\] où \\(R_d\\) est un terme résiduel négligeable au voisinage de \\(x_0\\). Dans un voisinage \\(h(x_0)\\) autour de \\(x_0\\), \\(\\mu\\) peut être approchée par un polynôme de degré \\(d\\). La quantité \\(h(x_0)\\) est appelée fenêtre (bandwidth). Si \\(\\varepsilon_i\\) est un bruit blanc, on peut donc estimer par les moindres carrés \\(\\mu(x_0)\\) en utilisant les observations qui sont dans \\(\\left[x_0-h(x_0),x_0+h(x_0)\\right]\\). 5.1 Régression polynomiale : approche de Proietti et Luati 5.1.1 Filtres symétriques Reprenons maintenant les notations de Proietti et Luati (2008) : supposons que notre série temporelle \\(y_t\\) peut être décomposée en \\[ y_t=\\mu_t+\\varepsilon_t, \\] où \\(\\mu_t\\) est la tendance et \\(\\varepsilon_{t}\\overset{i.i.d}{\\sim}\\mathcal{N}(0,\\sigma^{2})\\) est le bruit15. La tendance \\(\\mu_t\\) est localement approchée par un polynôme de degré \\(d\\), de sorte que dans un voisinage \\(h\\) de \\(t\\) \\(\\mu_t\\simeq m_{t}\\) avec : \\[ \\forall j\\in\\left\\llbracket -h,h\\right\\rrbracket :\\: y_{t+j}=m_{t+j}+\\varepsilon_{t+j},\\quad m_{t+j}=\\sum_{i=0}^{d}\\beta_{i}j^{i}. \\] Le problème dextraction de la tendance est équivalent à lestimation de \\(m_t=\\beta_0\\). En notation matricielle : \\[ \\underbrace{\\begin{pmatrix}y_{t-h}\\\\ y_{t-(h-1)}\\\\ \\vdots\\\\ y_{t}\\\\ \\vdots\\\\ y_{t+(h-1)}\\\\ y_{t+h} \\end{pmatrix}}_{y}=\\underbrace{\\begin{pmatrix}1 &amp; -h &amp; h^{2} &amp; \\cdots &amp; (-h)^{d}\\\\ 1 &amp; -(h-1) &amp; (h-1)^{2} &amp; \\cdots &amp; (-(h-1))^{d}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots\\\\ 1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots\\\\ 1 &amp; h-1 &amp; (h-1)^{2} &amp; \\cdots &amp; (h-1)^{d}\\\\ 1 &amp; h &amp; h^{2} &amp; \\cdots &amp; h^{d} \\end{pmatrix}}_{X}\\underbrace{\\begin{pmatrix}\\beta_{0}\\\\ \\beta_{1}\\\\ \\vdots\\\\ \\vdots\\\\ \\vdots\\\\ \\vdots\\\\ \\beta_{d} \\end{pmatrix}}_{\\beta}+\\underbrace{\\begin{pmatrix}\\varepsilon_{t-h}\\\\ \\varepsilon_{t-(h-1)}\\\\ \\vdots\\\\ \\varepsilon_{t}\\\\ \\vdots\\\\ \\varepsilon_{t+(h-1)}\\\\ \\varepsilon_{t+h} \\end{pmatrix}}_{\\varepsilon} \\] Pour estimer \\(\\beta\\) il faut \\(H\\geq d+1\\) et lestimation est faite par moindres carrés pondérés  weighted least squares (WLS) , ce qui revient à minimiser la fonction objectif suivante : \\[ S(\\hat{\\beta}_{0},\\dots,\\hat{\\beta}_{d})=\\sum_{j=-h}^{h}\\kappa_{j}(y_{t+j}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}j-\\dots-\\hat{\\beta}_{d}j^{d})^{2} \\] où \\(\\kappa_j\\) est un ensemble de poids appelés noyaux (kernel). On a \\(\\kappa_j\\geq 0:\\kappa_{-j}=\\kappa_j\\), et en notant \\(K=diag(\\kappa_{-h},\\dots,\\kappa_{h})\\), lestimateur \\(\\beta\\) peut sécrire \\(\\hat{\\beta}=(X&#39;KX)^{1}X&#39;Ky\\). Avec \\(e_{1}=\\begin{pmatrix}1&amp;0&amp;\\cdots&amp;0\\end{pmatrix}&#39;\\), lestimateur de la tendance peut donc sécrire : \\[ \\hat{m}_{t}=e_{1}\\hat{\\beta}=\\theta&#39;y=\\sum_{j=-h}^{h}\\theta_{j}y_{t-j}\\text{ avec }\\theta=KX(X&#39;KX)^{-1}e_{1} \\] En somme, lestimation de la tendance \\(\\hat{m}_{t}\\) est obtenue en appliquant une moyenne mobile symétrique \\(\\theta\\) à \\(y_t\\)16. De plus, \\(X&#39;\\theta=e_{1}\\) donc : \\[ \\sum_{j=-h}^{h}\\theta_{j}=1,\\quad\\forall r\\in\\left\\llbracket 1,d\\right\\rrbracket :\\sum_{j=-h}^{h}j^{r}\\theta_{j}=0. \\] Ainsi, la moyenne mobile \\(\\theta\\) préserve les polynômes de degré \\(d\\). Concernant le choix des paramètres, lidée générale qui prévaut est que le choix entre ces différents noyaux est secondaire17 et quil vaut mieux se concentrer sur deux autres paramètres : le degré du polynôme \\(d\\) : sil est trop petit on risque davoir des estimations biaisées de la tendance-cycle et sil est trop grand on risque davoir une trop grande variance dans les estimations (du fait dun sur-ajustement) ; le nombre de voisins \\(H=2h+1\\) (ou la fenêtre \\(h\\)) : sil est trop petit alors trop peu de données seront utilisées pour les estimations (ce qui conduira à une grande variance dans les estimations) et sil est trop grand alors lapproximation polynomiale sera vraisemblablement fausse ce qui conduira à avoir des estimations biaisées. 5.1.2 Les différents noyaux Dans les problèmes dextraction du signal, les observations sont généralement pondérées par rapport à leur distance à la date \\(t\\) : pour estimer la tendance-cycle à la date \\(t\\), on accorde généralement plus dimportance aux observations qui sont proches de \\(t\\). Dans le cas continu, un noyau \\(K\\) est une fonction positive, paire et intégrable telle que \\(\\int_{-\\infty}^{+\\infty}\\kappa(u) \\ud u=1\\) et \\(\\kappa(u)=\\kappa(-u)\\). Dans le cas discret, un noyau est un ensemble de poids \\(\\kappa_j\\), \\(j=0,\\pm1,\\dots,\\pm h\\) avec \\(\\kappa_j \\geq0\\) et \\(\\kappa_j=\\kappa_{-j}\\). Une classe importante de noyaux est celle des noyaux Beta. Dans le cas discret, à un facteur multiplicatif près (de sorte que \\(\\sum_{j=-h}^h\\kappa_j=1\\)) : \\[ \\kappa_j = \\left( 1- \\left\\lvert \\frac j {h+1} \\right\\lvert^r \\right)^s,\\quad\\text{avec }r&gt;0,s\\geq 0 \\] Cette classe englobe la majorité des noyaux présentés dans cette étude, à lexception des noyaux dHenderson, trapézoïdal et gaussien. Les principaux noyaux (qui sont également implémentés dans rjdfilters) sont : \\(r=1,s=0\\) noyau uniforme : \\[\\kappa_j^U=1\\] \\(r=s=1\\) noyau triangulaire : \\[\\kappa_j^T=\\left( 1- \\left\\lvert \\frac j {h+1} \\right\\lvert \\right)\\] \\(r=2,s=1\\) noyau dEpanechnikov (ou parabolique) : \\[\\kappa_j^E=\\left( 1- \\left\\lvert \\frac j {h+1} \\right\\lvert^2 \\right)\\] \\(r=s=2\\) noyau quadratique (biweight) : \\[\\kappa_j^{BW}=\\left( 1- \\left\\lvert \\frac j {h+1} \\right\\lvert^2 \\right)^2\\] \\(r = 2, s = 3\\) noyau cubique (triweight) : \\[\\kappa_j^{TW}=\\left( 1- \\left\\lvert \\frac j {h+1} \\right\\lvert^2 \\right)^3\\] \\(r = s = 3\\) noyau tricube : \\[\\kappa_j^{TC}=\\left( 1- \\left\\lvert \\frac j {h+1} \\right\\lvert^3 \\right)^3\\] noyau dHenderson (voir partie 5.1.3 pour plus de détails) : \\[ \\kappa_{j}=\\left[1-\\frac{j^2}{(h+1)^2}\\right] \\left[1-\\frac{j^2}{(h+2)^2}\\right] \\left[1-\\frac{j^2}{(h+3)^2}\\right] \\] noyau trapézoïdal : \\[ \\kappa_j^{TP}= \\begin{cases} \\frac{1}{3(2h-1)} &amp; \\text{ if }j=\\pm h \\\\ \\frac{2}{3(2h-1)} &amp; \\text{ if }j=\\pm (h-1)\\\\ \\frac{1}{2h-1}&amp; \\text{ otherwise} \\end{cases} \\] noyau gaussien18: \\[ \\kappa_j^G=\\exp\\left( -\\frac{ j^2 }{ 2\\sigma^2h^2 }\\right) \\] Les noyaux dHenderson, trapézoïdal et gaussien sont particuliers : Les fonctions noyau dHenderson et trapézoïdal changent avec la fenêtre (les autres dépendent uniquement du rapport \\(j/h+1\\)). Pour les noyaux trapézoïdal et gaussien, dautres définitions pourraient être utilisées et sont donc définis arbitrairement. Le noyau trapézoïdal est implémenté dans rjdfilters car il permet de calculer les moyennes mobiles utilisées dans lalgorithme X-13ARIMA pour lextraction des composantes saisonnières. Il nest pas adapté dans le cas de lextraction de la tendance-cycle. 5.1.3 Quelques filtres symétriques particuliers Lorsque \\(p=0\\) (ajustement local par une constante) on obtient lestimateur de Nadaraya-Watson (ou lestimateur par noyaux). Avec le noyau uniforme on obtient le filtre de Macaulay et al. (1931). Lorsque \\(p=0\\) ou \\(p=1\\), on retrouve la moyenne arithmétique : \\(w_j=w=\\frac{1}{2h+1}\\). Le noyau dEpanechnikov est souvent recommandé comme le noyau optimal car il minimise lerreur quadratique moyenne de lestimation par polynômes locaux. Le Loess, locally estimated scatterplot smoothing (utilisé dans la méthode STL), est une régression locale pondérée qui utilise le noyau tricube. Le filtre dHenderson est un cas particulier de lapproximation locale cubique (\\(p=3\\)), couramment utilisée pour lextraction de la tendance-cycle (cest par exemple le filtre utilisé dans le logiciel de désaisonnalisation X-13ARIMA). Pour une fenêtre fixée, Henderson a trouvé le noyau qui donnait lestimation la plus lisse de la tendance. Il montre léquivalence entre les trois problèmes suivants : minimiser la variance de la différence dordre trois de la série lissée par lapplication dune moyenne mobile ; minimiser la somme du carré de la différence dordre trois des coefficients du filtre, cest le critère de lissage (smoothness) : \\(S=\\sum_j(\\nabla^{3}\\theta_{j})^{2}\\) ; estimer une tendance localement cubique par les moindres carrés pondérés, où les poids sont choisis de sorte à minimiser la smoothness (cela conduit au noyau présenté dans la section 5.1.2). Le filtre dHenderson étant couramment utilisé pour lextraction de la tendance-cycle, nous nous intéresserons uniquement aux filtres issus du noyau dHenderson. 5.1.4 Filtres asymétriques Comme mentionné dans la partie 2.3.1, pour lestimation en temps réel, plusieurs approches peuvent être utilisées : Construire un filtre asymétrique par approximation polynomiale locale sur les observations disponibles (\\(y_{t}\\) pour \\(t\\in\\left\\llbracket n-h,n\\right\\rrbracket\\)). Appliquer les filtres symétriques sur les séries prolongées par prévision \\(\\hat{y}_{n+l\\mid n},l\\in\\left\\llbracket 1,h\\right\\rrbracket\\). Construire des filtres asymétriques qui minimisent lerreur quadratique moyenne de révision sous des contraintes de reproduction de tendances polynomiales. Proietti et Luati (2008) montrent que les deux premières approches sont équivalentes lorsque les prévisions sont faites par extrapolation polynomiale de degré \\(d\\). Elles sont également équivalentes à la troisième approche sous les mêmes contraintes que celles du filtre symétrique. Cette méthode est appelée direct asymmetric filter (DAF). Même si les estimations sont sans biais, cest au coût dune plus grande variance dans les estimations. Pour résoudre le problème de la variance des estimations des filtres temps réel, Proietti et Luati (2008) proposent une méthode générale pour construire les filtres asymétriques qui permet de faire un compromis biais-variance. Il sagit dune généralisation des filtres asymétriques de Musgrave (1964) (utilisés dans lalgorithme de désaisonnalisation X-13ARIMA). On modélise ici la série en entrée par : \\[\\begin{equation} y=U\\gamma+Z\\delta+\\varepsilon,\\quad \\varepsilon\\sim\\mathcal{N}(0,D) \\tag{5.1} \\end{equation}\\] où \\([U,Z]\\) est de rang plein et forme un sous-ensemble des colonnes de \\(X\\). Lobjectif est de trouver un filtre \\(v\\) qui minimisent lerreur quadratique moyenne de révision (au filtre symétrique \\(\\theta\\)) sous certaines contraintes. Ces contraintes sont représentées par la matrice \\(U=\\begin{pmatrix}U_{p}&#39;&amp;U_{f}&#39;\\end{pmatrix}&#39;\\) : \\(U_p&#39;v=U&#39;\\theta\\) (avec \\(U_p\\) la matrice \\((h+q+1)\\times (d+1)\\) qui contient les observations de la matrice \\(U\\) connues lors de lestimation par le filtre asymétrique). Le problème est équivalent à trouver \\(v\\) qui minimise : \\[\\begin{equation} \\varphi(v)= \\underbrace{ \\underbrace{(v-\\theta_{p})&#39;D_{p}(v-\\theta_{p})+ \\theta_{f}&#39;D_{f}\\theta_{f}}_\\text{variance de l&#39;erreur de révision}+ \\underbrace{[\\delta&#39;(Z_{p}&#39;v-Z&#39;\\theta)]^{2}}_{biais^2} }_\\text{Erreur quadratique moyenne de révision}+ \\underbrace{2l&#39;(U_{p}&#39;v-U&#39;\\theta)}_{\\text{contraintes}} \\tag{5.2} \\end{equation}\\] où \\(l\\) est le vecteur des multiplicateurs de Lagrange. Lorsque \\(U=X\\), la contrainte équivaut à préserver les polynômes de degré \\(d\\) : on retrouve les filtres directs asymétriques (DAF) lorsque \\(D=K^{-1}\\). Lorsque \\(U=\\begin{pmatrix}1&amp;\\cdots&amp;1\\end{pmatrix}&#39;\\), \\(Z=\\begin{pmatrix}-h&amp;\\cdots&amp;+h\\end{pmatrix}&#39;\\), \\(\\delta=\\delta_1\\), \\(D=\\sigma^2I\\) et lorsque le filtre symétrique est le filtre dHenderson, on retrouve les filtres asymétriques de Musgrave. Ce filtre suppose, que pour lestimation en temps réel, les données sont générées par un processus linéaire et que les filtres asymétriques préservent les constantes (\\(\\sum v_i=\\sum \\theta_i=1\\)). Ces filtres asymétriques dépendent du rapport \\(\\delta_1/\\sigma\\), qui est lié à lI-C ratio \\(R=\\frac{\\bar{I}}{\\bar{C}}=\\frac{\\sum\\lvert I_t-I_{t-1}\\rvert}{\\sum\\lvert C_t-C_{t-1}\\rvert}\\) (\\(\\delta_1/\\sigma=2/(R\\sqrt{\\pi})\\)), qui est notamment utilisé dans X-13ARIMA pour déterminer la longueur du filtre dHenderson19. Lorsque \\(U\\) correspond aux \\(d^*+1\\) premières colonnes de \\(X\\), \\(d^*&lt;d\\), la contrainte consiste à reproduire des tendances polynomiales de degré \\(d^*\\). Cela introduit du bais mais réduit la variance. Ainsi, Proietti et Luati (2008) proposent trois classes de filtres asymétriques : Linear-Constant (LC) : \\(y_t\\) linéaire (\\(d=1\\)) et \\(v\\) préserve les constantes (\\(d^*=0\\)). On obtient le filtre de Musgrave avec le filtre dHenderson comme filtre symétrique. Quadratic-Linear (QL) : \\(y_t\\) quadratique (\\(d=2\\)) et \\(v\\) préserve les tendances linéaires (\\(d^*=1\\)). Cubic-Quadratic (CQ) : \\(y_t\\) cubic (\\(d=3\\)) et \\(v\\) préserve les tendances quadratiques (\\(d^*=2\\)). Le tableau 5.1 compare les critères de qualité des différentes méthodes en utilisant le filtre dHenderson et \\(h=6\\) (filtre symétrique de 13 termes). Pour les filtres en temps réel (\\(q=0\\)), plus le filtre asymétrique est complexe (en termes de préservation polynomiale), moins la timeliness est élevée et plus la fidelity/smoothness est grande : la réduction du déphasage se fait au détriment dune augmentation de la variance. Ce résultat varie lorsque \\(q\\) augmente : pour \\(q=2\\) le filtre QL a une plus grande timeliness que le filtre LC. Ce résultat étonnant souligne le fait que le déphasage nest pas contrôlé par lapproche de Proietti et Luati (2008). En termes de révision, (\\(A_w+S_w+T_w+R_w\\)), les filtres LC et QL donnent toujours de meilleurs résultats que les filtres CQ et DAF. Table 5.1 : Critères de qualité des filters asymétriques (\\(q=0,1,2\\)) calculés par polynômes locaux en utilisant le noyau dHenderson avec \\(h=6\\) et \\(R=3,5\\). Method \\(b_c\\) \\(b_l\\) \\(b_q\\) \\(F_g\\) \\(S_g\\) \\(T_g \\times 10^{-3}\\) \\(A_w\\) \\(S_w\\) \\(T_w\\) \\(R_w\\) \\(q=0\\) LC 0 -0,41 -2,16 0,39 1,27 30,34 0,10 0,49 0,41 0,55 QL 0 0,00 -0,47 0,71 5,15 0,05 0,07 1,89 0,00 0,11 CQ 0 0,00 0,00 0,91 11,94 0,01 0,02 2,23 0,00 0,10 DAF 0 0,00 0,00 0,94 14,20 0,00 0,01 2,18 0,00 0,10 \\(q=1\\) LC 0 -0,12 -0,52 0,27 0,43 4,80 0,01 0,12 0,06 0,11 QL 0 0,00 -0,06 0,29 0,71 0,69 0,00 0,19 0,01 0,04 CQ 0 0,00 0,00 0,37 0,57 0,16 0,02 0,58 0,00 0,06 DAF 0 0,00 0,00 0,41 0,37 0,06 0,02 0,76 0,00 0,06 \\(q=2\\) LC 0 0,00 1,08 0,20 0,08 0,35 0,01 0,01 0,00 0,01 QL 0 0,00 0,03 0,22 0,05 2,08 0,00 0,01 0,02 0,07 CQ 0 0,00 0,00 0,37 0,66 0,13 0,02 0,56 0,00 0,06 DAF 0 0,00 0,00 0,40 0,77 0,02 0,02 0,68 0,00 0,05 Une application en ligne, disponible à ladresse https://aqlt.shinyapps.io/FiltersProperties/, permet de comparer les coefficients, les fonctions de gain et de déphasage entre les différentes méthodes et les différents noyaux. Filtres locaux polynomiaux (Proietti et Luati (2008)) Avantages : Modèles avec une interprétation simple. Le filtre asymétrique est indépendant de la date destimation. Toutefois, il dépend indirectement des données si le filtre est calibré sur lI-C ratio. Inconvénients : La timeliness nest pas contrôlée (mais peut être introduite dans le programme de minimisation). 5.2 Extension avec le critère de timeliness Un inconvénient de la méthode précédente est que le déphasage nest pas contrôlé. Il est en revanche possible de généraliser davantage la modélisation en ajoutant le critère de timeliness définit par Grun-Rehomme, Guggemos, et Ladiray (2018) dans léquation (5.2). Cest ce qui a été proposé par Jean Palate, puis codé en Java20 et intégré dans rjdfilters. En utilisant les mêmes notations que dans 5.1.4, \\(\\theta\\) le filtre symétrique et \\(v\\) le filtre asymétrique. Notons également \\(\\theta=\\begin{pmatrix}\\theta_p\\\\\\theta_f\\end{pmatrix}\\) avec \\(\\theta_p\\) de même longueur que \\(v\\), et \\(g=v-\\theta_p\\). Le critère de timeliness sécrit : \\[ T_g(v)=v&#39;Tv=g&#39;Tg+2\\theta_p&#39;Tg+\\theta_p&#39;T\\theta_p \\quad(T\\text{ étant symétrique)}. \\] De plus, la fonction objectif \\(\\varphi\\) de léquation (5.2) peut se réécrire : \\[\\begin{align*} \\varphi(v)&amp;=(v-\\theta_p)&#39;D_{p}(v-\\theta_p)+ \\theta_f&#39;D_{f}\\theta_f+ [\\delta&#39;(Z_{p}&#39;v-Z&#39;\\theta)]^{2}+ 2l&#39;(U_{p}&#39;v-U&#39;\\theta)\\\\ &amp;=g&#39;Qg-2Pg+2l&#39;(U_{p}&#39;v-U&#39;\\theta)+c\\quad\\text{avec } \\begin{cases} Q=D_p+Z_p\\delta\\delta&#39;Z&#39;_p \\\\ P=\\theta_fZ_f\\delta\\delta&#39;Z_p&#39;\\\\ c\\text{ une constante indépendante de }v \\end{cases}. \\end{align*}\\] En ajoutant le critère de timeliness, on obtient : \\[ \\widetilde\\varphi(v)=g&#39;\\widetilde Qg- 2\\widetilde Pg+2l&#39;(U_{p}&#39;v-U&#39;\\theta)+ \\widetilde c\\quad\\text{avec } \\begin{cases} \\widetilde Q=D_p+Z_p\\delta\\delta&#39;Z&#39;_p +\\alpha_TT\\\\ \\widetilde P=\\theta_fZ_f\\delta\\delta&#39;Z_p&#39;-\\alpha_T\\theta_pT\\\\ \\widetilde c\\text{ une constante indépendante de }v \\end{cases} \\] où \\(\\alpha_T\\) est le poids associé au critère de timeliness. Avec \\(\\alpha_T=0\\) on retrouve \\(\\varphi(v)\\). Cette extension permet donc de retrouver tous les filtres symétriques et asymétriques présentés dans la section précédente mais généralise également lapproche de Gray et Thomson (1996) présentée dans la section 5.3. Elle est proche de lapproche FRST. Cette extension sinscrit dans le cadre de la théorie générale définie dans la section 3.1. Cela revient en effet à minimiser une somme pondérée de lerreur quadratique de révision : \\[ \\E{\\left( \\sum_{i=-h}^h\\theta^s_{i}y_{t+s}-\\sum_{i=-h}^qv_iy_{t+s} \\right)^2} = I(v,\\,0,\\,y_t,\\,M_{\\theta^s} y_t) \\] et du critère de timeliness : \\[ T_g(\\theta) = J(f\\colon(\\rho,\\varphi)\\mapsto\\rho^2\\sin(\\varphi)^2,\\,\\omega_1, \\,\\omega_2) \\] sous une contrainte linéaire. 5.3 Régression polynomiale : Gray et Thomson 5.3.1 Filtres symétriques Lapproche de Gray et Thomson (1996) est proche de celles de Proietti et Luati (2008) et de Grun-Rehomme, Guggemos, et Ladiray (2018). De la même façon que pour les autres méthodes, ils considèrent que la série initiale \\(y_t\\) peut se décomposer entre une somme entre la tendance-cycle \\(g_t\\) et dun bruit blanc \\(\\varepsilon_t\\) de variance \\(\\sigma^2\\) : \\[y_t = g_t+\\varepsilon_t.\\] Toutefois, plutôt que de directement remplacer \\(g_t\\) par un polynôme local de degré \\(d\\), ils prennent en compte lerreur dapproximation de la tendance : \\[ g_t=\\sum_{j=0}^{d}\\beta_{j}t^{j}+\\xi_{t}, \\] où \\(\\xi_t\\) est un processus stochastique de moyenne nulle, autocorrélé mais non corrélé à \\(\\varepsilon_t\\). La tendance \\(g_t\\) est estimée par une moyenne mobile : \\[ \\hat{g}_{t}=\\sum_{s=-r}^{r}\\theta_{s}y_{t+s}. \\] Pour le filtre central, les auteurs cherchent à avoir un estimateur \\(\\hat g_t\\) qui soit sans biais (ce qui implique que \\(\\theta\\) conserve les tendances de degré \\(d\\)) et qui minimise une somme pondérée dun critère de fidelity et dun critère de smoothness : \\[\\begin{equation} Q=\\alpha\\underbrace{\\E{(\\hat{g}_{t}-g_{t})^{2}}}_{=F_{GT}}+ (1-\\alpha)\\underbrace{\\E{ (\\Delta^{p+1}\\hat{g}_{t})^{2}} }_{=S_{GT}} \\tag{5.3} \\end{equation}\\] La solution est un filtre symétrique qui peut sécrire sous la forme \\[ \\theta=E_{\\alpha}^{-1}X\\left[X&#39;E_{\\alpha}^{-1}X\\right]^{-1}e_{1}\\text{ avec }E_{\\alpha}=\\alpha\\left(\\sigma^{2}I+\\Omega\\right)+(1-\\alpha)\\left(\\sigma^{2}B_{p+1}+\\Gamma\\right) \\] où : \\[ \\begin{cases} \\Omega_{jk} &amp; =cov\\left(\\xi_{t+j}-\\xi_{t},\\xi_{t+k}-\\xi_{t}\\right)\\\\ \\Gamma_{jk} &amp; =cov\\left(\\Delta^{p+1}\\xi_{t+j},\\Delta^{p+1}\\xi_{t+k}\\right)\\\\ \\sigma^{2}\\left(B_{p+1}\\right)_{jk} &amp; =cov\\left(\\Delta^{p+1}\\varepsilon_{t+j},\\Delta^{p+1}\\varepsilon_{t+k}\\right) \\end{cases}. \\] Les deux critères utilisés dans le programme de minimisation (5.3) sont des cas particuliers du critère \\(I\\) défini dans léquation (3.1) : \\[\\begin{align*} F_{GT}(\\theta)&amp;=I(\\theta,0,y_t,M_\\theta y_t)\\\\ S_{GT}(\\theta)&amp;=I(\\theta,d+1,y_t,0). \\end{align*}\\] La théorie générale définie dans la section 3.1 permet donc de retrouver les filtres de Gray et Thomson (1996). En ne minimisant que la smoothness et avec \\(\\xi_t=0\\) on retrouve le filtre dHenderson. En ne minimisant que la fidelity, cette méthode est équivalente à lestimation de polynômes locaux par moindres carrés généralisés : on retrouve donc les filtres de Proietti et Luati (2008) avec \\(\\sigma^2=0\\) et \\(\\Omega =K^{-1}\\), ainsi que le filtre de Macaulay. Lavantage de la modélisation de Gray et Thomson est que le paramètre \\(\\xi_t\\) permet une spécification plus précise du modèle en prenant notamment en compte la corrélation entre les observations. Par exemple, McLaren et Steel (2001) ont étudié le lien entre le plan de sondage et lestimation de la composante tendance-cycle et de la composante saisonnière. Cette modélisation leur permet de prendre en compte, dans lestimation de la tendance-cycle, la structure de corrélation induite par le plan de sondage de lenquête emploi mensuelle de lAustralie (groupe de rotations avec une période de recouvrement). Cependant, les auteurs avertissent que dans leur simulations (et dans la modélisation de Gray et Thomson) la structure dautocorrélation de la variable aléatoire \\(\\xi_t\\) est supposée connue. Ce nest généralement pas le cas en pratique, où cette structure doit être estimée, ce qui rajoute de lincertitude dans les estimations. 5.3.2 Filtres asymétriques Lapproche retenue par Gray et Thomson (1996) est une approche de minimisation des révisions sous contraintes. Étant donné un filtre symétrique \\(\\theta^s\\) utilisé pour estimer la tendance au centre de la série, lobjectif est de chercher un filtre asymétrique \\(v=(v_{-h},\\dots,v_q)\\) de sorte à minimiser lerreur quadratique moyenne de révision : \\[ \\E{\\left(Y-\\hat Y\\right)^2} = \\E{\\left( \\sum_{i=-h}^h\\theta^s_iy_{t+s}-\\sum_{i=-h}^qv_iy_{t+s} \\right)^2}. \\] Les auteurs étudient deux cas : Dans le premier cas, ils cherchent un estimateur sans biais : cela implique que \\(v\\) conserve les mêmes tendances polynomiales que \\(\\theta^s\\). \\(\\hat Y\\) est alors le meilleur prédicteur linéaire sans biais  best linear unbiased predictor (BLUP)  de \\(Y\\). Dans le second cas, ils autorisent lestimateur à être biaisé mais imposent que ce biais soit constant dans le temps : si lon modélise localement la tendance par un polynôme de degré \\(d\\), cela implique que \\(v\\) conserve les tendances polynomiales de degré \\(d-1\\). \\(\\hat Y\\) est alors le meilleur prédicteur linéaire à biais constant  best linear time invariant predictor (BLIP)  de \\(Y\\). Cela permet notamment de reproduire les filtres asymétriques de Musgrave. La méthode utilisée est donc très proche de celle de Proietti et Luati (2008) : on retrouve dailleurs le filtre DAF avec \\(\\sigma^2=0\\) et \\(\\Omega =K^{-1}\\) et en utilisant la première méthode (estimation du BLUP) et les méthodes LC (filtre de Musgrave), QL et CQ avec la seconde méthode en utilisant respectivement \\(d=1\\), \\(d=2\\) et \\(d=3\\). La théorie générale définie dans la section 3.1 permet également de retrouver les filtres asymétriques puisquils sont construits en minimisant lerreur quadratique moyenne des révisions sous contraintes linéaires (préservation dun polynôme de degré \\(p\\)). Pour la construction des filtres asymétriques, une approche alternative pourrait être dutiliser la même méthode que celle utilisée pour construire les filtres symétriques. Cest-à-dire minimiser \\(Q\\) (équation (5.3)) sous contrainte que le filtre asymétrique fournisse un estimateur dans biais de la tendance. Comme discuté dans Gray et Thomson (1996), les auteurs ne retiennent pas cette méthode pour deux raisons : Il nest pas évident quil faudrait chercher à maintenir le même équilibre entre smoothness et fidelity en fin de série et au centre de la série. Le problème rencontré en fin de série est transitoire et disparaît au fur et à mesure que lon a de nouvelles observations. Minimiser des critères de révision serait donc préférable puisque cela reviendrait à minimiser le coût de la transition (mais dans le cas où lon ne minimise que la fidelity les deux méthodes sont équivalentes). Les valeurs de la fidelity et de la smoothness ne dépendent pas du temps au centre de la série mais en dépendent en fin de série. Ainsi, même si au centre de la série le choix des poids entre les deux critères contrôle indirectement le niveau des indicateurs, ce nest plus le cas en fin de série. De plus, en fin de série, cela pourrait introduire des déphasages plus importants car \\(S_{GT}\\) dépend du temps et des valeurs passées (du fait du lutilisation de lopérateur différence). Inversement, Grun-Rehomme, Guggemos, et Ladiray (2018) justifie de ne pas intégrer le critère de révision dans leur problème car ce critère est fortement corrélé à une combinaison fixée, donc non ajustable par lutilisateur, des critères fidelity et timeliness. Filtres locaux polynomiaux (Gray et Thomson (1996)) Avantages : Modèles généraux qui permettent de prendre en compte lautocorrélation entre les observations. Interprétation statistique des différentes méthodes. Le filtre asymétrique est indépendant de la date destimation. Toutefois, il dépend indirectement des données si le filtre est calibré sur lI-C ratio. Inconvénients : La timeliness nest pas contrôlée. La spécification du modèle (i.e., du paramètre \\(\\xi_t\\)) peut être compliquée : si la structure dautocorrélation est estimée à partir des données, cela rajoute de lincertitude dans les estimations, ce qui peut avoir des effets indésirables. 5.4 Reproducing Kernel Hilbert Space (RKHS) : approche de Dagum et Bianconcini La théorie des Reproducing Kernel Hilbert Space (RKHS)  espaces de Hilbert à noyau reproduisant  est une théorie générale dans lapprentissage statistique non-paramétrique qui permet denglober un grand nombre de méthodes. Cest par exemple le cas des méthodes de régression par moindres carrés pénalisés, des Support Vector Machine (SVM), du filtre dHodrick-Prescott (utilisé pour décomposer tendance et cycle) ou encore des moyennes mobiles telles que celle dHenderson. Ainsi, Dagum et Bianconcini (2008) utilise la théorie des RKHS pour approcher le filtre dHenderson et en dériver des filtres asymétriques associés. Un RKHS \\(\\mathbb{L}^{2}(f_{0})\\) est un espace de Hilbert caractérisé par un noyau qui permet de reproduire toutes les fonctions de cet espace. Il est caractérisé par une fonction de densité \\(f_0\\) et un produit scalaire \\(\\ps{\\cdot}{\\cdot}\\) définit par : \\[ \\left\\langle U(t),V(t)\\right\\rangle =\\E{U(t)V(t)}=\\int_{\\R}U(t)V(t)f_{0}(t)\\ud t\\quad \\forall U,V\\in\\mathbb{L}^{2}(f_{0}). \\] La fonction \\(f_0\\) pondère donc chaque valeur en fonction de sa position temporelle : il sagit de la version continue des noyaux définis dans la partie 5.1.2. Dans notre cas, on suppose que notre série initiale \\(y_t\\) est désaisonnalisée et peut sécrire comme la somme dune tendance-cycle, \\(TC_t\\), et dune composante irrégulière, \\(I_t\\) (qui peut être un bruit blanc ou suivre un modèle ARIMA) : \\(y_t=TC_t+I_t.\\) La tendance-cycle peut être déterministe ou stochastique. On suppose que cest une fonction régulière du temps, elle peut être localement approchée par un polynôme de degré \\(d\\) : \\[ TC_{t+j}=TC_t(j)=a_0+a_1j+\\dots+a_dj^d+\\varepsilon_{t+j},\\quad j\\in\\llbracket-h,h\\rrbracket, \\] où \\(\\varepsilon_t\\) est un bruit blanc non corrélé à \\(I_t\\). Les coefficients \\(a_0,\\dots,a_d\\) peuvent être estimés par projection des observations au voisinage de \\(y_t\\) sur le sous-espace \\(\\mathbb P_d\\) des polynômes de degré \\(d\\), ou, de manière équivalente, par minimisation de la distance entre \\(y_t\\) et \\(TC_t(j)\\) : \\[\\begin{equation} \\underset{TC\\in\\mathbb P_d}{\\min}\\lVert y -TC \\rVert^2 = \\underset{TC\\in\\mathbb P_d}{\\min}\\int_\\R (y(t+s)-TC_t(s))^2f_0(s)\\ud s. \\tag{5.4} \\end{equation}\\] Lespace \\(\\mathbb P_d\\) étant un espace de Hilbert à dimension finie, il admet un noyau reproduisant (voir, par exemple, Berlinet et Thomas-Agnan (2004)). Il existe ainsi une fonction \\(R_d(\\cdot,\\cdot)\\) telle que : \\[ \\forall P\\in \\mathbb P_d: \\forall t: R_d(t,\\cdot)\\in\\mathbb P_d\\quad\\text{et}\\quad P(t)=\\ps{R_d(t,\\cdot)}{P(\\cdot)}. \\] Le problème (5.4) admet une solution unique qui dépend dune fonction \\(K_{d+1}\\), appelée fonction de noyau (kernel function). Cette fonction est dite dordre \\(d+1\\) car elle conserve les polynômes de degré \\(d\\)21. Cette solution sécrit : \\[\\begin{equation} \\widehat{TC}(t)=\\int_\\R y(t-s)K_{d+1}(s) \\ud s. \\tag{5.5} \\end{equation}\\] Généralement \\(f_0(t) = 0\\) pour \\(\\lvert t \\rvert&gt;1\\). Cette solution sécrit alors : \\[\\begin{equation} \\widehat{TC}(t)=\\int_{[-1,1]} y(t-s)K_{d+1}(s) \\ud s. \\tag{5.6} \\end{equation}\\] On peut, par ailleurs, montrer que \\(K_{d+1}\\) sécrit en fonction \\(f_0\\) et du noyau reproduisant \\(K(\\cdot,\\cdot)\\) et que ce dernier peut sécrire en fonction de polynômes \\((P_i)_{i\\in \\llbracket 0, d-1 \\rrbracket}\\) qui forme une base orthonormée de \\(\\mathbb L^2(f_0)\\) (voir par exemple Berlinet (1993)) : \\[ K_{d+1}(t) = R_d(t,0)f_0(t) = \\sum_{i=0}^dP_i(t)P_i(0)f_0(t). \\] De plus, dans le cas discret, la solution (5.6) sécrit comme une somme pondérée au voisinage de \\(y_t\\) : \\[\\begin{equation} \\widehat{TC}_t=\\sum_{j=-h}^h w_j y_{t+j}\\quad \\text{où} \\quad w_j=\\frac{K_{d+1}(j/b)}{\\sum_{i=-h}^{^h}K_{d+1}(i/b)}. \\tag{5.7} \\end{equation}\\] Le paramètre \\(b\\) est choisi de sorte que les \\(2h+1\\) points autour de \\(y_t\\) soient utilisés avec un poids non nul. Pour les filtres asymétriques, la formule (5.7) est simplement adaptée au nombre dobservations connues : \\[\\begin{equation} \\forall j\\in\\left\\llbracket -h,q\\right\\rrbracket\\::\\: w_{a,j}=\\frac{K_{d+1}(j/b)}{\\sum_{i=-h}^{^q}K_{d+1}(i/b)}. \\tag{5.8} \\end{equation}\\] En utilisant \\(b=h+1\\) on retrouve les filtres symétriques obtenues par polynômes locaux. Comme notamment montré par Dagum et Bianconcini (2016), \\(K_{d+1}\\) peut sexprimer simplement à partir des moments de \\(f_0\\)22. Ainsi, notons \\(H_{d+1}\\) la matrice de Hankel associée aux moments de \\(f_0\\) : \\[ \\forall i,j\\in \\llbracket 0, d\\rrbracket: \\left(H_{d+1}\\right)_{i,j}=\\ps{X^i}{X^j}=\\int s^{i+j}f_0(s)\\ud s. \\] Notons également \\(H_{d+1}[1,t]\\) la matrice obtenue en remplaçant la première ligne de \\(H_{d+1}\\) par \\(\\begin{pmatrix} 1 &amp; t &amp; t^2 &amp; \\dots &amp; t^d\\end{pmatrix}\\). On a : \\[\\begin{equation} K_{d+1}(t)=\\frac{\\det{H_{d+1}[1,t]}}{\\det{H_{d+1}}}f_0(t). \\tag{5.9} \\end{equation}\\] Cest cette formule qui est utilisée dans le package rjdfilters pour calculer les différentes moyennes mobiles. Comme discuté dans la partie 5.1, le noyau dHenderson dépend de la fenêtre utilisée. Ainsi, tous les moments de léquation (5.9) doivent être recalculés pour chaque valeur de \\(h\\). Pour éviter cela, Dagum et Bianconcini (2008) suggèrent dutiliser le noyau quadratique (biweight) pour approcher le noyau dHenderson lorsque \\(h\\) est petit (\\(h&lt; 24\\)) et le noyau cubique (triweight) lorsque \\(h\\) est grand \\(h\\geq 24\\). Dans Dagum et Bianconcini (2015), les auteures suggèrent de faire une sélection optimale du paramètre \\(b\\), par exemple en minimisant lerreur quadratique moyenne (option \"frequencyresponse\" dans rjdfilters::rkhs_filter) : \\[ b_{q,\\Gamma}=\\underset{b_q\\in[h; 3h]}{\\min} 2\\int_{0}^{\\pi} \\lvert \\Gamma_s(\\omega)-\\Gamma_\\theta(\\omega)\\rvert^2\\ud \\omega. \\] Cela suppose en fait que la série entrée \\(y_t\\) est un bruit blanc. En supposant \\(y_t\\) stationnaire, les critères définis dans larticle originel peuvent donc être étendus en multipliant les quantités sous les intégrales par la densité spectrale de \\(y_t\\) notée \\(h\\) : \\[ b_{q,\\Gamma}=\\underset{b_q\\in[h; 3h]}{\\min} 2\\int_{0}^{\\pi} \\lvert \\Gamma_s(\\omega)-\\Gamma_\\theta(\\omega)\\rvert^2h(\\omega)\\ud \\omega. \\] Cette erreur quadratique moyenne peut également se décomposer en plusieurs termes (voir équation (4.1) de la section 4) qui peuvent également être minimisés : laccuracy qui correspond à la part de la révision liée aux différences de fonction de gain dans les fréquences liées à la tendance-cycle \\[ b_{q,G}=\\underset{b_q\\in[h; 3h]}{\\min} 2\\int_{0}^{\\omega_1} \\left(\\rho_s(\\omega)-\\rho_\\theta(\\omega)\\right)^{2} h(\\omega)\\ud \\omega \\] la smoothness qui correspond à la part de la révision liée aux différences de fonction de gain dans les fréquences liées aux résidus \\[ b_{q,s}=\\underset{b_q\\in[h; 3h]}{\\min} 2\\int_{\\omega_1}^{\\pi} \\left(\\rho_s(\\omega)-\\rho_\\theta(\\omega)\\right)^{2} h(\\omega)\\ud \\omega \\] la timeliness qui correspond à la part de la révision liée au déphasage \\[ b_{q,\\varphi}=\\underset{b_q\\in[h; 3h]}{\\min} 8\\int_{0}^{\\omega_1} \\rho_s(\\lambda)\\rho_\\theta(\\lambda)\\sin^{2}\\left(\\frac{\\varphi_\\theta(\\omega)}{2}\\right)h(\\omega)\\ud \\omega \\] Dans rjdfilters \\(h\\) peut être fixée à la densité spectrale dun bruit blanc (\\(h_{WN}(x)=1\\), comme cest le cas dans Dagum et Bianconcini (2015)) ou dune marche aléatoire (\\(h_{RW}(x)=\\frac{1}{2(1-\\cos(x))}\\)). Pour assurer une cohérence dans les définitions entre les différentes sections, les définitions de \\(b_{q,G}\\), \\(b_{q,\\Gamma}\\) et \\(b_{q,\\varphi}\\) ont été légèrement modifiées par rapport à celles définies dans Dagum et Bianconcini (2015) où : dans \\(b_{q,G}\\) le terme à minimiser est sous une racine carrée (sans impact sur le minimum) : \\[ b_{q,G}=\\underset{b_q}{\\min}\\sqrt{ 2\\int_{0}^{\\pi} \\left(\\rho_s(\\omega)-\\rho_\\theta(\\omega)\\right)^{2}\\ud \\omega} \\] \\(b_{q,s}\\) nest pas considéré, \\(b_{q,\\Gamma}\\) et \\(b_{q,G}\\) sont définis avec \\(\\omega_1=\\pi\\) \\(b_{q,\\varphi}\\) est défini par : \\[ b_{q,\\varphi}=\\underset{b_q}{\\min} \\sqrt{2\\int_{\\Omega_S} \\rho_s(\\lambda)\\rho_\\theta(\\lambda)\\sin^{2}\\left(\\frac{\\varphi_\\theta(\\omega)}{2}\\right)\\ud \\omega} \\] où \\(\\Omega_S=[0,2\\pi/32]\\) est lintervalle de fréquences associées aux cycles dau moins 16 mois. une formule différente est utilisée pour la fonction de réponse (\\(\\Gamma_\\theta(\\omega)=\\sum_{k=-p}^{+f} \\theta_k e^{2\\pi i \\omega k}\\)), ce qui conduit à des bornes dintégrales légèrement différentes, sans effet sur le résultat. Un des inconvénients de cette méthode est quil ny a pas unicité de la solution et donc quil y a parfois plusieurs extremum (uniquement pour le calcul de \\(b_{q,\\varphi}\\). Ainsi, la valeur optimal retenue par défaut par rjdfilters produit des discontinuités dans lestimation de la tendance-cycle. Par ailleurs, les valeurs de \\(b_{q,G}\\) varient fortement en fonction de si lon retient \\(\\omega_1=2\\pi/12\\) ou \\(\\omega_1=\\pi\\) (tableau 5.2). Par cohérence, simplicité, nous utiliserons dans cet article les valeurs optimales présentées dans Dagum et Bianconcini (2015). Table 5.2 : Fenêtres optimales pour les filtres asymétriques associés à un filtre symétrique de 13 termes (\\(h=6\\)) avec le noyau biweight. \\(q=0\\) \\(q=1\\) \\(q=2\\) \\(q=3\\) \\(q=4\\) \\(q=5\\) \\(b_{q,\\Gamma}\\) \\(\\omega_1 = \\pi\\) 9,54 7,88 7,07 6,88 6,87 6,94 \\(\\omega_1 = 2\\pi/12\\) 9,54 7,88 7,07 6,88 6,87 6,94 \\(b_{q,G}\\) \\(\\omega_1 = \\pi\\) 11,78 9,24 7,34 6,85 6,84 6,95 \\(\\omega_1 = 2\\pi/12\\) 8,61 7,64 6,01 6,01 6,01 6,59 \\(b_{q,\\varphi}\\) Valeurs de larticle (avec \\(\\omega_1 = 2\\pi/36\\)) 6,01 6,01 7,12 8,44 9,46 10,39 \\(\\omega_1 = 2\\pi/36\\) 6,01 6,01 7,21 8,47 9,46 6,01 \\(\\omega_1 = 2\\pi/12\\) 6,01 6,01 6,38 8,15 9,35 6,01 RKHS filters - Dagum et Bianconcini (2008) Avantages : Le filtre asymétrique est indépendant des données et de la date destimation. La méthode est généralisable à des séries avec des fréquences irrégulières (par exemple avec beaucoup de valeurs manquantes). Inconvénient : Il peut y avoir des problèmes de minimisation (notamment en minimisant la timeliness). Tous les critères utilisés de la sélection optimale du paramètre \\(b\\) pouvant sécrire comme cas particuliers du critère \\(J\\) défini dans léquation (3.1) (voir section 4), cette méthode sinscrit dans le cadre de la théorie générale définie dans la section 3.1 en imposant comme contrainte linéaire que les coefficients soient sous la forme \\(w_j=\\frac{K_{d+1}(j/b)}{\\sum_{i=-h}^{^p}K_{d+1}(i/b)}\\). 5.5 Liens entre les différentes méthodes 5.5.1 Critères de Gray et Thomson et ceux de Grun-Rehomme et alii Les critères \\(F_g\\) et \\(S_g\\) peuvent se déduire de \\(F_{GT}\\) et \\(S_{GT}\\). Les approches de Gray et Thomson (1996) et Grun-Rehomme, Guggemos, et Ladiray (2018) sont donc équivalentes pour la construction de filtres symétriques. Notons \\(x_{t}=\\begin{pmatrix}1 &amp; t &amp; t^{2} &amp; \\cdots &amp; t^{d}\\end{pmatrix}\\), \\(\\beta_{t}=\\begin{pmatrix}\\beta_{0} &amp; \\cdots &amp; \\beta^{d}\\end{pmatrix}&#39;\\). Pour le critère de fidelity : \\[ \\hat{g}_{t}-g_{t}=\\left(\\sum_{j=-h}^{+h}\\theta_{j}x_{t+j}-x_{t}\\right)\\beta+\\sum_{j=-h}^{+h}\\theta_{j}\\varepsilon_{t+j}+\\sum_{j=-h}^{+h}\\theta_{j}(\\xi_{t+j}-\\xi_{t}), \\] Si \\(\\theta\\) préserve les polynômes de degré \\(d\\) alors \\(\\sum_{j=-h}^{+h}\\theta_{j}x_{t+j}=x_{t}\\). où \\(x_{t}=\\begin{pmatrix}1 &amp; t &amp; t^{2} &amp; \\cdots &amp; t^{d}\\end{pmatrix}\\). Puis, comme \\(\\xi_{t}\\) et \\(\\varepsilon_{t}\\) sont de moyenne nulle et sont non corrélés : \\[ F_{GT}(\\theta)=\\E{(\\hat{g}_{t}-g_{t})^{2}}=\\theta^{&#39;}\\left(\\sigma^{2}I+\\Omega\\right)\\theta. \\] Si \\(\\xi_t=0\\) alors \\(\\Omega=0\\) et \\(F_{GT}(\\theta)=F_g(\\theta)\\). Pour la smoothness on a : \\[ \\nabla^{q}\\hat{g}_{t}=\\sum_{j=h}^{h}\\theta_{j}\\underbrace{\\nabla^{q}\\left(\\left(x_{j}-x_{0}\\right)\\beta\\right)}_{=0\\text{ si }q\\geq d+1}+\\sum_{j=h}^{h}\\theta_{j}\\nabla^{q}\\varepsilon_{t+j}+\\sum_{j=h}^{h}\\theta_{j}\\nabla^{q}\\xi_{t+j}. \\] Doù pour \\(q=d+1\\) : \\[ S_{GT}(\\theta)=\\E{(\\nabla^{q}\\hat{g}_{t})^{2}}=\\theta^{&#39;}\\left(\\sigma^{2}B_{q}+\\Gamma_{q}\\right)\\theta. \\] On peut par ailleurs montrer que pour toute série temporelle \\(X_t\\), \\[ \\nabla^{q}(M_{\\theta}X_{t})=\\left(-1\\right)^{q}\\sum_{k\\in\\Z}\\left(\\nabla^{q}\\theta_{k}\\right)X_{t+k-q} \\] avec \\(\\theta_k=0\\) pour \\(|k|\\geq h+1\\). Avec \\(\\xi_t=0\\) on trouve donc que \\(S_{GT}(\\theta)=\\sigma^2S_g(\\theta)\\). 5.5.2 Équivalence avec les moindres carrés pondérés Du fait de la forme des filtres obtenus par la méthode de Grun-Rehomme, Guggemos, et Ladiray (2018), lorsque les contraintes imposées sont la préservation des tendances de degré \\(d\\), celle-ci est équivalente à une estimation locale dune tendance polynomiale de degré \\(d\\) par moindres carrés généralisés. En effet, dans ce cas, la solution est \\(\\hat \\theta = \\Sigma^{-1}X_p&#39;\\left(X_p\\Sigma^{-1}X_p&#39;\\right)^{-1}e_1\\) avec \\(\\Sigma=\\alpha F+\\beta S+ \\gamma T\\), et cest lestimation de la constante obtenue par moindres carrés généralisés lorsque la variance des résidus est \\(\\Sigma\\). Léquivalence entre les deux méthodes peut donc se voir comme un cas particulier de léquivalence entre les moindres carrés pondérés et les moindres carrés généralisés. Cest par exemple le cas des filtres symétriques dHenderson qui peuvent sobtenir par les deux méthodes. Dans ce sens, Henderson (1916) a montré que les poids \\(w=(w_{-p},\\dots w_{f})\\) associés à une moyenne mobile issue de la régression polynomiale locale par moindres carrés pondérés pouvaient sécrire sous la forme : \\[ w_i = \\kappa_i P\\left(\\frac{i}{p+f+1}\\right)\\text{ où }P\\text{ est un polynôme de degré }d. \\] Il a également montré linverse : toute moyenne mobile \\(\\theta=(\\theta_{-p},\\dots, \\theta_{f})\\) qui préserve les tendances de degré \\(d\\) et dont le diagramme des coefficients change au plus \\(d\\) fois de signes peut être obtenue par une régression polynomiale locale de degré \\(p\\) estimée par moindres carrés pondérés. Pour cela il suffit de trouver un polynôme \\(P\\left(\\frac{X}{p+f+1}\\right)\\) de degré inférieur ou égal à \\(d\\) et dont les changements de signes coïncident avec les changements de signes de \\(\\theta\\). Le noyau associé est alors \\(\\kappa_i=\\frac{ \\theta_i}{P\\left(\\frac{i}{p+f+1}\\right)}\\). Cest le cas de tous les filtres symétriques issues de lapproche FST et de la majorité des filtres asymétriques. Lannexe C présente les quelques poids pour lesquels il ny a pas équivalence. Plus récemment, Luati et Proietti (2011) se sont intéressés aux cas déquivalences entre les moindres carrés pondérés et les moindres carrés généralisés pour déterminer des noyaux optimaux (au sens de Gauss-Markov). Ils montrent que le noyau dEpanechnikov est le noyau optimal associé à la régression polynomiale locale où le résidu, \\(\\varepsilon_t\\), est un processus moyenne mobile (MA) non inversible dordre 1 (i.e., \\(\\varepsilon_t=(1-B)\\xi_t\\), avec \\(\\xi_t\\) un bruit blanc). Dans ce cas, la matrice \\(\\Sigma\\) de variance-covariance correspond à la matrice obtenue par le critère de smoothness avec le paramètre \\(q=2\\) (\\(\\sum_{j}(\\nabla^{2}\\theta_{j})^{2} = \\theta&#39;\\Sigma\\theta\\)) : il y a donc équivalence avec lapproche FST. De même, le noyau dHenderson est le noyau optimal associé à la régression polynomiale locale où le résidu est un processus moyenne mobile (MA) non inversible dordre 2 (i.e., \\(\\varepsilon_t=(1-B)^2\\xi_t\\), avec \\(\\xi_t\\) un bruit blanc). 5.5.3 RKHS et polynômes locaux Comme montré dans la section précédente, la théorie des espaces de Hilbert à noyau reproduisant permet de reproduire les filtres symétriques par approximation polynomiale locale. Comme le montrent Luati et Proietti (2011), cette théorie permet donc également de reproduire les filtres directs asymétriques (DAF), qui sont équivalents à lapproximation polynomiale locale mais en utilisant une fenêtre destimation asymétrique. Cependant, ils ne peuvent pas être obtenus par la formalisation de Dagum et Bianconcini (2008) mais par une discrétisation différente de la formule (5.9) : \\[ K_{d+1}(t)=\\frac{\\det{H_{d+1}[1,t]}}{\\det{H_{d+1}}}f_0(t). \\] Dans le cas discret, \\(f_0(t)\\) est remplacé par \\(\\kappa_j\\) et en remplaçant les moments théoriques par les moments empiriques \\(H_{d+1}\\) devient \\(X&#39;_pK_pX_p\\) et les coefficients du filtre asymétrique sont obtenus en utilisant la formule : \\[ w_{a,j}=\\frac{\\det{X&#39;_pK_pX_p[1,j]} }{ \\det{X&#39;_pK_pX_p} }\\kappa_j. \\] En effet, la règle de Cramer permet de trouver une solution explicite à léquation des moindres carrés \\((X&#39;_pK_pX_p)\\hat \\beta=X&#39;_pK_py_p\\) où \\(\\hat \\beta_0=\\hat m_t\\) : \\[ \\hat \\beta_0 = \\frac{\\det{X&#39;_pK_pX_p[1,b]}}{\\det{X&#39;_pK_pX_p}}f_0(t) \\quad\\text{où}\\quad b=X&#39;_pK_py_p. \\] Comme \\(b=\\sum_{j=-h}^qx_j\\kappa_jy_{t+j}\\) il vient : \\[ \\det{X&#39;_pK_pX_p[1,b]} = \\sum_{j=-h}^q\\det{X&#39;_pK_pX_p[1,x_j]}\\kappa_jy_{t+j}. \\] Et enfin : \\[ \\hat \\beta_0 = \\hat m_t= \\sum_{j=-h}^q\\frac{\\det{X&#39;_pK_pX_p[1,j]} }{ \\det{X&#39;_pK_pX_p} }\\kappa_j y_{t+j}. \\] Références "],["sec-comparison.html", "Chapitre 6 Comparaison des différentes méthodes 6.1 Méthodologie 6.2 Séries simulées 6.3 Série réelle", " Chapitre 6 Comparaison des différentes méthodes Les différentes méthode de construction de moyennes mobiles asymétriques sont comparées sur des données simulées et des données réelles. Pour toutes les séries, un filtre symétrique de 13 termes est utilisé. Ces méthodes sont également comparées aux estimations obtenues en prolongeant la série par un modèle ARIMA23 puis en appliquant un filtre symétrique de Henderson de 13 termes. 6.1 Méthodologie En suivant une méthodologie proche de celle de Darne et Dagum (2009), neuf séries mensuelles sont simulées entre janvier 1960 et décembre 2020 avec différent niveaux de variabilité. Chaque série simulée \\(y_t= C_t+ T_t + I_t\\) peut sécrire comme la somme de trois composantes : le cycle \\(C_t = \\rho [\\cos (2 \\pi t / \\lambda) +\\sin (2 \\pi t / \\lambda)]\\), \\(\\lambda\\) est fixé à 72 (cycles de 6 ans, il y a donc 19 points de retournement détectables) ; la tendance \\(T_t = T_{t-1} + \\nu_t\\) avec \\(\\nu_t \\sim \\mathcal{N}(0, \\sigma_\\nu^2)\\), \\(\\sigma_\\nu\\) étant fixé ) \\(0,08\\) ; et lirrégulier \\(I_t = e_t\\) avec \\(e_t \\sim \\mathcal{N}(0, \\sigma_e^2)\\). Pour les différentes simulations, nous faisons varier les paramètre \\(\\rho\\) et \\(\\sigma_e^2\\) afin davoir davoir des séries avec différents rapports signal sur bruit : Fort rapport signal sur bruit (cest-à-dire un I-C ratio faible et une faible variabilité) : \\(\\sigma_e^2=0,2\\) et \\(\\rho = 3,0,\\, 3,5\\) ou \\(4,0\\) (I-C ratio compris entre 0,9 et 0,7) ; Rapport signal sur bruit moyen (cest-à-dire un I-C ratio moyen et une variabilité moyenne) : \\(\\sigma_e^2=0,3\\) et \\(\\rho = 1,5,\\, 2,0\\) ou \\(3,0\\) (I-C ratio compris entre 2,3 et 1,4) ; Faible rapport signal sur bruit (cest-à-dire un I-C ratio fort et une forte variabilité) : \\(\\sigma_e^2=0,4\\) et \\(\\rho = 0,5,\\, 0,7\\) ou \\(1,0\\) (I-C ratio compris entre 8,9 et 5,2). Pour chaque série et chaque date, la tendance-cycle est estimée en utilisant les différentes méthodes présentées dans de ce rapport. Pour les régressions polynomiales locales, les filtres asymétriques sont calibrés en utilisant lI-C ratio estimé à chaque date (en appliquant un filtre de Henderson de 13 termes) et pour la méthode FST, un quadrillage du plan est réalisé avec un pas de \\(0,05\\) et avec comme contraintes linéaires la préservation des polynômes de degrés 0 à 3. Trois critères de qualité sont également calculés : Calcul du déphasage dans la détection des points de retournement. La définition Zellner, Hong, et Min (1991) est utilisée pour déterminer les points de retournement : on parle de redressement (upturn) lorsque lon passe dune phase de récession à une phase dexpansion de léconomie. Cest le cas à la date \\(t\\) lorsque \\(y_{t-3}\\geq y_{t-2}\\geq y_{t-1}&lt;y_t\\leq y_{t+1}\\). on parle de ralentissement (downturn) lorsque lon passe dune phase dexpansion à une phase de récession. Cest le cas à la date \\(t\\) lorsque \\(y_{t-3}\\leq y_{t-2}\\leq y_{t-1}&gt;y_t\\geq y_{t+1}\\). Le déphasage est souvent définit comme le nombre de mois nécessaires pour détecter le bon point de retournement (i.e., le point de retournement sur la composante cyclique). Nous utilisons ici un critère légèrement modifié : le déphasage est définit comme le nombre de mois nécessaires pour détecter le bon point de retournement sans aucune révision future. Il peut en effet arriver que le bon point de retournement soit détecté par des filtres asymétriques mais ne le soit pas avec lestimation finale avec un filtre symétrique (cest le cas de 41 points de retournements sur lensemble des 9 séries avec les filtres asymétriques de Musgrave) ou quil y ait des révisions dans les estimations successives (cest le cas de 7 points de retournements sur lensemble des 9 séries avec les filtres asymétriques de Musgrave). Finalement, relativement peu de points de retournement sont détectés à la bonne date avec lestimation finale. Avec le filtre de Henderson de 13 termes, 18 sont correctement détectés sur les séries avec une faible variabilité (sur les 57 possibles), 11 sur les séries à variabilité moyenne et 12 sur les séries à forte variabilité. Calcul de deux critères de révisions : la moyenne des écarts relatifs entre la \\(q\\) estimation et la dernière estimation \\(MAE_{fe}(q)\\) et la moyenne des écarts relatifs entre la \\(q\\) et la \\(q+1\\) estimation \\(MAE_{qe}(q)\\) \\[ MAE_{fe}(q)=\\mathbb E\\left[ \\left|\\frac{ y_{t|t+q} - y_{t|last} }{ y_{t|last} }\\right| \\right] \\quad\\text{et}\\quad MAE_{qe}(q)=\\mathbb E\\left[ \\left|\\frac{ y_{t|t+q} - y_{t|t+q+1} }{ y_{t|t+q+1} }\\right| \\right] \\] 6.2 Séries simulées 6.2.1 Comparaison des filtres polynomiaux locaux et des filtres RKHS Du fait du fort degré de liberté de lapproche FST (dans le choix des différents paramètres), dans cette section, on ne compare les méthodes issues lapproche polynomiale locale utilisant le noyau de Henderson24 et les filtres RKHS \\(b_{q,\\Gamma}\\), \\(b_{q,G}\\) et \\(b_{q,\\varphi}\\). En termes de déphasage dans la détection des points de retournement, cest le filtre \\(b_{q,G}\\) qui semble donner les meilleurs résultats quelle que soit la variabilité de la série (figure 6.1), avec des performances légèrement meilleurs que celles obtenues en prolongeant la série grâce à un modèle ARIMA. Étonnement, cest le filtre \\(b_{q,\\varphi}\\) qui minimise le déphasage qui donne les moins bons résultats. Cela peut sexpliquer par le fait que la courbe des coefficients des moyennes mobiles asymétriques sont assez éloignées des coefficients du filtre symétrique : il y a donc potentiellement beaucoup de révisions dans la détection des points de retournement. Dans les filtres polynomiaux locaux, les filtres LC (filtres de Musgrave) semblent donner les meilleurs résultats lorsque la variabilité est moyenne ou forte (avec des résultats proches du filtre QL dans ce second pas) mais de moins bons résultats lorsque la variabilité est faible. Figure 6.1 : Distribution des déphasages sur les séries simulées. Table 6.1 : Moyenne des écarts relatifs des révisions pour les différents filtres sur les séries à variabilité moyenne. Méthode \\(q=0\\) \\(q=1\\) \\(q=2\\) \\(q=3\\) \\(q=4\\) \\(q=5\\) \\(MAE_{fe}(q) = \\mathbb E\\\\left[\\\\left|(y_{t|t+q} - y_{t|last})/y_{t|last}\\\\right|\\\\right]\\) LC 0,21 0,10 0,03 0,03 0,03 0,01 QL 0,33 0,10 0,04 0,04 0,03 0,01 CQ 0,45 0,13 0,13 0,09 0,06 0,02 DAF 0,47 0,15 0,15 0,09 0,06 0,02 \\(b_{q,\\Gamma}\\) 0,63 0,21 0,03 0,09 0,09 0,04 \\(b_{q,G}\\) 0,83 0,37 0,03 0,09 0,09 0,04 \\(b_{q,\\varphi}\\) 0,31 0,11 0,03 0,05 0,07 0,09 ARIMA 0,22 0,10 0,03 0,03 0,03 0,01 \\(MAE_{ce}(q)=\\mathbb E\\\\left[ \\\\left|(y_{t|t+q} - y_{t|t+q+1})/y_{t|t+q+1}\\\\right| \\\\right]\\) LC 0,19 0,10 0,02 0,01 0,07 0,01 QL 0,29 3,46 0,00 0,03 0,04 0,01 CQ 0,43 0,02 0,10 0,07 0,05 0,02 DAF 0,66 0,24 0,11 0,14 0,06 0,02 \\(b_{q,\\Gamma}\\) 0,38 0,32 0,09 0,00 0,41 0,04 \\(b_{q,G}\\) 0,70 0,46 0,10 0,00 0,43 0,04 \\(b_{q,\\varphi}\\) 0,22 0,16 0,08 0,05 0,03 0,09 ARIMA 0,21 0,13 0,02 0,02 0,25 0,01 Concernant les révisions, la variabilité de la série a peu dimpact sur les performances respectives des différentes méthodes mais joue sur les ordres de grandeurs. Globalement, les filtres LC minimisent toujours les révisions (voir tableau 6.1) et les révisions sont plus importantes avec les filtres CQ, DAF et les filtres RKHS autres que \\(b_{q,\\varphi}\\). Pour les filtres QL, il y a une forte révision entre la deuxième et la troisième estimation : cela peut venir du fait que pour la deuxième estimation (lorsque lon connait un point dans le futur), le filtre QL associe un poids plus important à lestimation en \\(t+1\\) quà lestimation en \\(t\\), ce qui crée une discontinuité. Pour les filtres polynomiaux autres que le filtre LC, les révisions importantes à la première estimation étaient prévisibles au vu de la courbe des coefficients : un poids très important est associé à lobservation courante et il y une forte discontinuité entre la moyenne mobile utilisée pour lestimation en temps réelle (lorsquaucun point dans le futur nest connu) et les autres moyennes mobiles. Le prolongement de la série par un modèle ARIMA donnent révisions avec les dernières estimations du même ordre de grandeur que le filtre LC mais des révisions plus importantes entre les estimations consécutives (on pouvait sy attendre comme souligné dans la section 2.3.1). En somme, par rapport au filtre LC, la réduction du déphasage du filtre \\(b_{q,G}\\) se fait au coût de révisions 4 fois plus importantes lorsque la variabilité de la série est moyenne. Pour les séries à forte variabilité, les révisions sont du même ordre de grandeur mais lécart est bien plus important pour les séries à faible variabilité. 6.2.2 Comparaison avec lapproche FST Pour le choix des poids dans lapproche FST, lidée retenue dans cette étude est de faire un quadrillage du plan \\([0,1]^3\\) avec un pas de 0,05 et en imposant \\(\\alpha + \\beta + \\gamma = 1\\)25. Pour chaque combinaison de poids, quatre ensembles de moyennes mobiles sont construits en forçant dans la minimisation la préservation de polynômes de degré 0 à 3. Le filtre symétrique utilisé est toujours celui de Henderson. Ces différentes moyennes mobiles sont ensuite comparées relativement aux performances des filtres LC et, par simplification, uniquement sur les séries simulées à variabilité moyenne. Figure 6.2 : Médiane du déphasage relatif des filtres FST par rapport au filtre LC selon les poids sur les séries simulées à variabilité moyenne. En termes de déphasage, en médiane, les filtres qui sont plus performants que les filtres LC sont ceux qui préservent le polynômes de degré 2 et ayant un poids associé à la fidelity (\\(\\beta\\)) inférieur à 0,5 et ceux qui préservent les polynômes de degré 3 (figure 6.2). En revanche, une analyse plus fine des résultats montre quen moyenne le déphasage est plus élevé quavec la méthode LC pour tous les filtres FST mais les résultats sont quasiment équivalents (entre 1,0 et 1,1) pour les filtres qui préservent les polynômes de degré 2 avec \\(\\alpha = \\beta =0,05\\) et \\(\\alpha = 0,05, \\, \\beta =0\\) et ceux qui préservent les polynômes de degré 3 avec \\(\\beta=0\\). Figure 6.3 : Moyenne des écarts relatifs des révisions entre la première et la dernière estimation (\\(MAE_{fe}(0)\\)), comparativement aux révisions du filtre LC sur les séries à variabilité moyenne. Figure 6.4 : Moyenne des écarts relatifs des révisions entre la première et la deuxième estimation (\\(MAE_{ce}(0)\\)), comparativement aux révisions du filtre LC sur les séries à variabilité moyenne. En termes de révisions (figures 6.3 et 6.4), les révisions entre la première et dernière estimation et entre la première et deuxième estimation sont inférieures à celles du filtre LC lorsque les filtres préservent les polynômes de degré 1. En revanche, avec les filtres qui préserve les polynômes de degré 3 les révisions entre la première et dernière estimation sont en moyenne plus de deux fois plus importante quavec le filtre LC et sont modérément plus élevées (rapport entre 1 et 2). En somme, même si une étude plus approfondie devrait être menée, pour létude de la méthode FST il parait opportun de se concentrer sur les filtres qui préserve les polynômes de degré 2 avec un poids associé au critère fidelity élevé et un poids faible associé au critère smoothness. 6.3 Série réelle Les différentes méthodes sont également comparées sur le point de retournement davril 2020 sur les ventes au détail des États-Unis (série RETAILx de la base FRED-MD McCracken et Ng (2016), utilisée en logarithme). Cest une série avec une variabilité moyenne. Les résultats des différentes estimations sont tracées dans les figures 6.5 à 6.8. Pour lapproche FST, on ne retient que les filtres obtenus avec les poids \\(\\begin{pmatrix}\\alpha&amp;\\beta&amp;\\gamma\\end{pmatrix} = \\begin{pmatrix}0,05 &amp;0,00&amp;0,95\\end{pmatrix},\\, \\begin{pmatrix}0,00 &amp;0,05&amp;0,95\\end{pmatrix},\\, \\begin{pmatrix}0,05 &amp;0,05&amp;0,90\\end{pmatrix}\\) ou \\(\\begin{pmatrix}0 &amp;0&amp;1\\end{pmatrix}\\) et en préservant les tendances de degré 2 ou 3. Figure 6.5 : Estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec les méthodes polynomiales locales. Figure 6.6 : Estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec les RKHS et en prolongeant la série par modèle ARIMA. Figure 6.7 : Estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec la méthode FST et \\(\\begin{pmatrix}\\alpha&amp;\\beta&amp;\\gamma\\end{pmatrix} = \\begin{pmatrix}0,05 &amp;0,00&amp;0,95\\end{pmatrix}\\) ou \\(\\begin{pmatrix}0,05 &amp;0,05&amp;0,90\\end{pmatrix}\\). Figure 6.8 : Estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec la méthode FST et \\(\\begin{pmatrix}\\alpha&amp;\\beta&amp;\\gamma\\end{pmatrix} =\\begin{pmatrix}0,00 &amp;0,05&amp;0,95\\end{pmatrix}\\) ou \\(\\begin{pmatrix}0 &amp;0&amp;1\\end{pmatrix}\\). Même si presque toutes les méthodes ont un déphasage dau plus 3 mois sur ce point de retournement, les estimations intermédiaires différent grandement. Il y a par exemple nettement plus de variabilités dans les estimations en temps réel pour les méthode QL, CQ et DAF et les filtres FST (figures 6.5, 6.7 et 6.8), avec des estimations intermédiaires qui semblent peu plausibles (notamment pour la première estimation du mois davril 2020). Les premières estimations avec le filtre LC ne capte pas la remontée en mai et juin 2020 : cela peut provenir du fait quen fin de série, la tendance modélisée est linéaire. Concernant les filtres RKHS, les estimations intermédiaires du filtre \\(b_{q,\\varphi}\\) semblent très erratiques, ce qui sexplique par le fait que les moyennes mobiles asymétriques utilisées lorsque lon se rapproche du cas symétrique sont éloignées de la moyenne mobile symétrique dHenderson. En revanche, les filtres \\(b_{q,\\Gamma}\\) et \\(b_{q,G}\\) donnent des estimations intermédiaires proches de lestimation finale, tout en minimisant le déphasage. La qualité des estimations intermédiaires peut également être analysée grâces aux prévisions implicites des différentes méthodes. Pour rappel, il sagit des prévisions de la série brute qui, en appliquant le filtre symétrique de Henderson sur la série prolongée, donne les mêmes estimations que les moyennes mobiles asymétriques. Lannexe D rassemble les différentes prévisions implicites des filtres étudiés dans cette section. Les prévisions implicites des filtres polynomiaux autres que LC, des filtres FST et du filtre \\(b_{q,\\varphi}\\) sont très peu plausibles et très éloignés des valeurs futures. Le contrecoup en mai 2020 suite à la baisse davril 2020 ne sont pas prévus par le modèle ARIMA mais le sont par les filtres LC et RKHS, en revanche, les prévisions à lhorizon de 6 mois peuvent être assez éloignées des valeurs attendues. Références "],["conclusion.html", "Conclusion", " Conclusion Pour lanalyse conjoncturel, la majorité des statisticiens fait directement ou indirectement appel à des méthodes dextraction de la tendance-cycle. Elles sont par exemple utilisées pour réduire le bruit dun indicateur afin den améliorer son analyse, et les modèles utilisées (comme les modèles de prévision) utilisent généralement sur des séries désaisonnalisées qui sappuient sur ces méthodes. Cette étude fait une première revue de la littérature des méthodes de construction des filtres asymétriques pour lextraction de la tendance-cycle, utilisées pour lestimation en temps réel (i.e., lestimation des derniers points connus). Toutes ces méthodes peuvent se voir comme des cas particuliers dune théorie générale de construction des moyennes mobiles. Elles sont par ailleurs facilement mobilisables et comparables grâce au package rjdfilters. Celui-ci permet dutiliser plusieurs outils, comme la construction des prévisions implicites, qui peuvent aider les statisticiens à évaluer la qualité des estimations récentes des différents filtres. La comparaison des différentes méthodes, bien que perfectible, permet de tirer quelques enseignements pour la construction de ces moyennes mobiles. Premièrement, en fin de période, chercher à conserver des tendances polynomiales de degré supérieur à un (filtres QL, CQ et DAF et certains filtres FST) semble introduire de la variance dans les estimations (et donc plus de révisions) sans gain significatif en termes de détection de point de retournement. Il faut en revanche que la longueur du filtre utilisé soit adapté à la variabilité de la série : si le filtre utilisé est trop long (cest-à-dire si la variabilité de la série est « moyenne »), conserver des tendances polynomiale de degré au plus 1 (méthode LC) produit de moins bons résultats en termes de détection des points de retournement. Deuxièmement, la théorie des RKHS semble permettre la construction de filtres qui pourraient donner un bon compromis entre minimisation du déphasage et révisions (filtre \\(b_{q,G}\\)). Toutefois, leur calibration peut être sujette à des problèmes doptimisation et conduire à des estimations intermédiaires erratiques (filtre \\(b_{q,\\varphi}\\)). Enfin, la moins bonne performance apparente de certains filtres basés sur lapproche FST ou les RKHS pourrait provenir de lutilisation de filtres sous-optimaux lorsque que lon sapproche du cas dutilisation du filtre symétrique. Sur lapproche FST par exemple, rien ne justifie que lon devrait utiliser les mêmes poids entre fidélité, lissage et temporalité pour la construction de toutes les moyennes mobiles asymétriques. Plus détudes devraient être faites pour savoir si, pour la construction des filtres asymétriques minimisant le déphasage, on pourrait se concentrer uniquement sur les ceux utilisés lorsque peu dobservations futures sont disponibles. Cela impliquerait notamment de revoir la méthodologie et les indicateurs utilisés. Cette étude pourrait être étendue de plusieurs manières. Tout dabord, elle nest pas exhaustive et pourrait donc être complétée. Parmi les approches étudiées, lextension proposée aux méthodes polynomiales locales afin dajouter un critère sur le déphasage pourrait donner des résultats prometteurs. Sur les méthodes FST et DAF, des méthodes de réduction de dimension pourraient être mobilisées afin détudier les paramètres les plus déterminants sur les performances des filtres. Parmi les approches récentes non étudiées, nous pouvons citer Vasyechko et Grun-Rehomme (2014) qui utilisent le noyau dEpanechnikov pour construire des filtres asymétriques de 13 termes (i.e., en utilisant un nombre de points dans le passé différent du filtre symétrique), et Feng et Schäfer (2021) qui proposent, en fin de période, lutilisation de poids optimaux (au sens de lerreur quadratique moyenne) dans les régressions polynomiales locales. Parmi les pistes dextension, on pourrait sintéresser à limpact de la longueur des filtres dans la détection des points de retournement. En effet, les filtres asymétriques sont calibrés avec des indicateurs calculés pour lestimation des filtres symétriques (par exemple pour déterminer automatiquement sa longueur), alors quune estimation locale pourrait être préférée. Par ailleurs, nous nous sommes concentrés uniquement sur les séries mensuelles dont le filtre symétrique est de 13 termes, mais les résultats peuvent être différents si le filtre symétrique étudié est plus long/court et si lon étudie des séries à dautres fréquences (trimestrielles ou journalières par exemple). Une autre piste pourrait être détudier limpact des points atypiques : les moyennes mobiles, comme tout opérateur linéaire, sont très sensibles à la présence des points atypiques. Pour limiter leur impact, dans X-13ARIMA une forte correction des points atypiques est effectuée sur la composante irrégulière avant dappliquer les filtres pour extraire la tendance-cycle. Cela amène donc à étudier limpact de ces points sur lestimation de la tendance-cycle et des points de retournement, mais aussi à explorer de nouveaux types de filtres asymétriques basés sur des méthodes robustes (comme les régressions locales robustes ou les médianes mobiles). Références "],["an-diag.html", "A Synthèse des liens entre les différentes méthodes de construction de moyennes mobiles", " A Synthèse des liens entre les différentes méthodes de construction de moyennes mobiles Figure A.1 : Synthèse des méthodes de construction de moyennes mobiles symétriques \\(\\theta=(\\theta_{-h},\\dots,\\theta_{h})\\) de \\(2h+1\\) termes. \\(X = X_d = \\begin{pmatrix} x_0 \\quad\\cdots \\quad x_d \\end{pmatrix}\\) avec \\(x_i&#39;=\\begin{pmatrix} (-h)^i \\quad \\cdots \\quad (h)^i\\end{pmatrix}\\). Figure A.2 : Synthèse des méthodes de construction de moyennes mobiles asymétriques \\(\\theta=(\\theta_{-h},\\dots,\\theta_{q})\\), \\(0\\leq q&lt; h\\) avec \\(\\theta^s\\) le filtre symétrique de référence de \\(2h+1\\) termes. \\(X_d = \\begin{pmatrix} x_0 \\quad\\cdots \\quad x_d \\end{pmatrix}\\) avec \\(x_i&#39;=\\begin{pmatrix} (-h)^i \\quad \\cdots \\quad (q)^i\\end{pmatrix}\\) et \\(X=X_d\\) avec \\(q=h\\). "],["an-graphs.html", "B Coefficients, fonctions de gain et de déphasage", " B Coefficients, fonctions de gain et de déphasage Figure B.1 : Coefficients, fonctions de gain et de déphasage pour le filtre Linear-Constant (LC) avec \\(I/C=3,5\\). Figure B.2 : Coefficients, fonctions de gain et de déphasage pour le filtre Quadratic-Linear (QL) avec \\(I/C=3,5\\). Figure B.3 : Coefficients, fonctions de gain et de déphasage pour le filtre Cubic-Quadratic (QL) avec \\(I/C=3,5\\). Figure B.4 : Coefficients, fonctions de gain et de déphasage pour le filtre asymétrique direct (DAF) avec \\(I/C=3,5\\). Figure B.5 : Coefficients, fonctions de gain et de déphasage pour le filtre RKHS \\(b_{q,\\Gamma}\\). Figure B.6 : Coefficients, fonctions de gain et de déphasage pour le filtre RKHS \\(b_{q,G}\\). Figure B.7 : Coefficients, fonctions de gain et de déphasage pour le filtre RKHS \\(b_{q,\\varphi}\\). "],["an-equivfstlp.html", "C Équivalence entre lapproche FST et les filtres polynomiaux locaux", " C Équivalence entre lapproche FST et les filtres polynomiaux locaux Dans cette annexe sont tracés les rares poids pour lesquels lapproche FST nest pas équivalente à lapproche polynomiale locale pour \\(h=6\\) (filtre symétrique de 13 termes). Lorsquun graphique nest pas affiché cest que tous les filtres FST sont équivalents une approche polynomiale locale par moindre carrés pondérés. Par exemple pour les filtres associés au filtre symétrique de 13 termes (\\(h=6\\), figure C.1) il ny a des graphiques que pour les filtres utilisés en temps réel (\\(q=0\\)) et lorsque ce filtre conserve les constantes (\\(d=0\\)), les droites (\\(d=1\\)) et les polynômes de degré 2 (\\(d=2\\)). Dans tous les autres cas (i.e., dès que lon connait au moins un point dans le futur, \\(q\\geq 1\\)), il y a équivalence pour tous les poids testés26. La smoothness est calculée avec le paramètre \\(q=3\\) (\\(S_g(\\theta) = \\sum_{j}(\\nabla^{3}\\theta_{j})^{2}\\)), comme pour le filtre symétrique dHenderson. Figure C.1 : Ensemble des poids pour lesquels la méthode FST nest pas équivalente aux moindres carrés pondérés pour \\(h=6\\) (filtre symétrique de 13 termes), sous contrainte de préservation des polynômes de degré au plus 3 (\\(d=0,1,2,3\\)). Un quadrillage de 200 points de lintervalle \\([0,1]\\) a été effectuée et on ne garde que lensemble des poids tels que leur somme fasse 1. "],["an-implicitforecasts.html", "D Prévisions implicites pour séries RETAILx", " D Prévisions implicites pour séries RETAILx Cette annexe montre les prévisions implicites associées aux différentes estimations de la tendance-cycle sur les ventes au détail des États-Unis autour du point de retournement davril 2020. Figure D.1 : Prévisions implicites associées aux estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec les méthodes polynomiales locales. Figure D.2 : Prévisions implicites associées aux estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec les RKHS et en prolongeant la série par modèle ARIMA. Figure D.3 : Prévisions implicites associées aux estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec la méthode FST et \\(\\begin{pmatrix}\\alpha&amp;\\beta&amp;\\gamma\\end{pmatrix} = \\begin{pmatrix}0,05 &amp;0,00&amp;0,95\\end{pmatrix}\\) ou \\(\\begin{pmatrix}0,05 &amp;0,05&amp;0,90\\end{pmatrix}\\). Figure D.4 : Prévisions implicites associées aux estimations successives de la tendance-cycle des ventes au détail aux États-Unis avec la méthode FST et \\(\\begin{pmatrix}\\alpha&amp;\\beta&amp;\\gamma\\end{pmatrix} =\\begin{pmatrix}0,00 &amp;0,05&amp;0,95\\end{pmatrix}\\) ou \\(\\begin{pmatrix}0 &amp;0&amp;1\\end{pmatrix}\\). "],["références.html", "Références", " Références "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
