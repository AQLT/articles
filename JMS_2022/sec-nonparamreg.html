<!DOCTYPE html>
<html lang="fr" xml:lang="fr">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 5 Régression non paramétrique et régression polynomiale locale | Estimation en temps réel de la tendance-cycle : apport de l’utilisation des filtres asymétriques dans la détection des points de retournement</title>
  <meta name="description" content="Journées de méthodologie statistique de l’Insee (JMS) / Mars 2022 / PARIS" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 5 Régression non paramétrique et régression polynomiale locale | Estimation en temps réel de la tendance-cycle : apport de l’utilisation des filtres asymétriques dans la détection des points de retournement" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Journées de méthodologie statistique de l’Insee (JMS) / Mars 2022 / PARIS" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 5 Régression non paramétrique et régression polynomiale locale | Estimation en temps réel de la tendance-cycle : apport de l’utilisation des filtres asymétriques dans la détection des points de retournement" />
  
  <meta name="twitter:description" content="Journées de méthodologie statistique de l’Insee (JMS) / Mars 2022 / PARIS" />
  

<meta name="author" content="Alain Quartier-la-Tente" />


<meta name="date" content="2022-03-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec-WildiMcLeroy.html"/>
<link rel="next" href="sec-comparison.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">JMS 2022</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Résumé</a></li>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="sec-SAtoTCE.html"><a href="sec-SAtoTCE.html"><i class="fa fa-check"></i><b>1</b> De la désaisonnalisation à l’estimation tendance-cycle</a></li>
<li class="chapter" data-level="2" data-path="sec-propMM.html"><a href="sec-propMM.html"><i class="fa fa-check"></i><b>2</b> Quelques propriétés sur les moyennes mobiles</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec-propMM.html"><a href="sec-propMM.html#gain-et-fonction-de-déphasage"><i class="fa fa-check"></i><b>2.1</b> Gain et fonction de déphasage</a></li>
<li class="chapter" data-level="2.2" data-path="sec-propMM.html"><a href="sec-propMM.html#propriétés-souhaitables-dune-moyenne-mobile"><i class="fa fa-check"></i><b>2.2</b> Propriétés souhaitables d’une moyenne mobile</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="sec-propMM.html"><a href="sec-propMM.html#préservation-de-tendances"><i class="fa fa-check"></i><b>2.2.1</b> Préservation de tendances</a></li>
<li class="chapter" data-level="2.2.2" data-path="sec-propMM.html"><a href="sec-propMM.html#réduction-du-bruit"><i class="fa fa-check"></i><b>2.2.2</b> Réduction du bruit</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="sec-propMM.html"><a href="sec-propMM.html#sec-mmasym"><i class="fa fa-check"></i><b>2.3</b> Estimation en temps réel et moyennes mobiles asymétriques</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="sec-propMM.html"><a href="sec-propMM.html#subec:mmetprev"><i class="fa fa-check"></i><b>2.3.1</b> Moyennes mobiles asymétriques et prévision</a></li>
<li class="chapter" data-level="2.3.2" data-path="sec-propMM.html"><a href="sec-propMM.html#indicateurs-de-qualité-des-moyennes-mobiles-asymétriques"><i class="fa fa-check"></i><b>2.3.2</b> Indicateurs de qualité des moyennes mobiles asymétriques</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sec-theoriegen.html"><a href="sec-theoriegen.html"><i class="fa fa-check"></i><b>3</b> D’une théorie générale sur la construction des filtres asymétriques à l’approche FST</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec-theoriegen.html"><a href="sec-theoriegen.html#subsec-theoriegen"><i class="fa fa-check"></i><b>3.1</b> Théorie générale de construction des filtres asymétriques</a></li>
<li class="chapter" data-level="3.2" data-path="sec-theoriegen.html"><a href="sec-theoriegen.html#subsec-GuggemosEtAl"><i class="fa fa-check"></i><b>3.2</b> Approche <em>Fidelity-Smoothness-Timeliness</em> (FST)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sec-WildiMcLeroy.html"><a href="sec-WildiMcLeroy.html"><i class="fa fa-check"></i><b>4</b> Filtres dépendant des données : trilemme ATS</a></li>
<li class="chapter" data-level="5" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html"><i class="fa fa-check"></i><b>5</b> Régression non paramétrique et régression polynomiale locale</a>
<ul>
<li class="chapter" data-level="5.1" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#sec-proietti"><i class="fa fa-check"></i><b>5.1</b> Régression polynomiale : approche de Proietti et Luati</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#filtres-symétriques"><i class="fa fa-check"></i><b>5.1.1</b> Filtres symétriques</a></li>
<li class="chapter" data-level="5.1.2" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#sec-kernels"><i class="fa fa-check"></i><b>5.1.2</b> Les différents noyaux</a></li>
<li class="chapter" data-level="5.1.3" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#sec-sympolyfilter"><i class="fa fa-check"></i><b>5.1.3</b> Quelques filtres symétriques particuliers</a></li>
<li class="chapter" data-level="5.1.4" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#subsec-lppasymf"><i class="fa fa-check"></i><b>5.1.4</b> Filtres asymétriques</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#subsec-lptimeliness"><i class="fa fa-check"></i><b>5.2</b> Extension avec le critère de <em>timeliness</em></a></li>
<li class="chapter" data-level="5.3" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#subsec-graythomson"><i class="fa fa-check"></i><b>5.3</b> Régression polynomiale : Gray et Thomson</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#filtres-symétriques-1"><i class="fa fa-check"></i><b>5.3.1</b> Filtres symétriques</a></li>
<li class="chapter" data-level="5.3.2" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#filtres-asymétriques"><i class="fa fa-check"></i><b>5.3.2</b> Filtres asymétriques</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#sec-rkhs"><i class="fa fa-check"></i><b>5.4</b> Reproducing Kernel Hilbert Space (RKHS) : approche de Dagum et Bianconcini</a></li>
<li class="chapter" data-level="5.5" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#subsec-equivlpfst"><i class="fa fa-check"></i><b>5.5</b> Liens entre les différentes méthodes</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#critères-de-gray-et-thomson-et-ceux-de-grun-rehomme-et-alii"><i class="fa fa-check"></i><b>5.5.1</b> Critères de Gray et Thomson et ceux de Grun-Rehomme <em>et alii</em></a></li>
<li class="chapter" data-level="5.5.2" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#équivalence-avec-les-moindres-carrés-pondérés"><i class="fa fa-check"></i><b>5.5.2</b> Équivalence avec les moindres carrés pondérés</a></li>
<li class="chapter" data-level="5.5.3" data-path="sec-nonparamreg.html"><a href="sec-nonparamreg.html#rkhs-et-polynômes-locaux"><i class="fa fa-check"></i><b>5.5.3</b> RKHS et polynômes locaux</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sec-comparison.html"><a href="sec-comparison.html"><i class="fa fa-check"></i><b>6</b> Comparaison des différentes méthodes</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec-comparison.html"><a href="sec-comparison.html#méthodologie"><i class="fa fa-check"></i><b>6.1</b> Méthodologie</a></li>
<li class="chapter" data-level="6.2" data-path="sec-comparison.html"><a href="sec-comparison.html#séries-simulées"><i class="fa fa-check"></i><b>6.2</b> Séries simulées</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="sec-comparison.html"><a href="sec-comparison.html#comparaison-des-filtres-polynomiaux-locaux-et-des-filtres-rkhs"><i class="fa fa-check"></i><b>6.2.1</b> Comparaison des filtres polynomiaux locaux et des filtres RKHS</a></li>
<li class="chapter" data-level="6.2.2" data-path="sec-comparison.html"><a href="sec-comparison.html#comparaison-avec-lapproche-fst"><i class="fa fa-check"></i><b>6.2.2</b> Comparaison avec l’approche FST</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="sec-comparison.html"><a href="sec-comparison.html#série-réelle"><i class="fa fa-check"></i><b>6.3</b> Série réelle</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="appendix"><span><b>Annexe</b></span></li>
<li class="chapter" data-level="A" data-path="an-diag.html"><a href="an-diag.html"><i class="fa fa-check"></i><b>A</b> Synthèse des liens entre les différentes méthodes de construction de moyennes mobiles</a></li>
<li class="chapter" data-level="B" data-path="an-graphs.html"><a href="an-graphs.html"><i class="fa fa-check"></i><b>B</b> Coefficients, fonctions de gain et de déphasage</a></li>
<li class="chapter" data-level="C" data-path="an-equivfstlp.html"><a href="an-equivfstlp.html"><i class="fa fa-check"></i><b>C</b> Équivalence entre l’approche FST et les filtres polynomiaux locaux</a></li>
<li class="chapter" data-level="D" data-path="an-implicitforecasts.html"><a href="an-implicitforecasts.html"><i class="fa fa-check"></i><b>D</b> Prévisions implicites pour séries <code>RETAILx</code></a></li>
<li class="chapter" data-level="" data-path="références.html"><a href="références.html"><i class="fa fa-check"></i>Références</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Estimation en temps réel de la tendance-cycle : apport de l’utilisation des filtres asymétriques dans la détection des points de retournement</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!-- <script type="text/x-mathjax-config"> -->
    <!-- MathJax.Hub.Config({ -->
            <!--   TeX: { -->
                    <!--     Macros: { -->
                            <!--       NN: "{\\mathbb{N}}", -->
                            <!--       ZZ: "{\\mathbb{Z}}", -->
                            <!--       QQ: "{\\mathbb{Q}}", -->
                            <!--       RR: "{\\mathbb{R}}", -->
                            <!--       shiftset: "{\\mathcal{D}}", -->
                            <!--       dx: ["{\\mathrm{d}^{#1}\\mspace{-1mu}\\mathord{#2}}", 2, ""], -->
                            <!--       indic: "{\\unicode{x1D7D9}}", -->
                            <!--       prob: "\\mathop{\\mathbb{P}}", -->
                            <!--       esp: "\\mathop{\\mathbb{E}}", -->
                            <!--       var: "\\mathop{\\mathbb{V}\\text{ar}}", -->
                            <!--       cov: "\\mathop{\\mathbb{C}\\text{ov}}", -->
                            <!--       PP: ["{\\prob\\left({#1}\\right)}", 1], -->
                            <!--       EE: ["{\\esp\\left[{#1}\\right]}", 1], -->
                            <!--       VV: ["{\\var\\left[{#1}\\right]}", 1], -->
                            <!--       CC: ["{\\cov\\left[{#1}\\right]}", 1], -->
                            <!--       normal: ["{\\mathcal{N}\\left({#1},{#2}\\right)}", 2], -->
                            <!--       ou: ["{#1}_{\\text{ou}}", 1], -->
                            <!--       oui: ["{#1}_{\\text{ou},#2}", 2], -->
                            <!--       pv: "{\\mathfrak{p}}", -->
                            <!--       qv: "{\\mathfrak{q}}", -->
                            <!--       zs: "{\\mathfrak{z}}", -->
                            <!--       ts: "{\\mathfrak{t}}", -->
                            <!--       sign: "{\\mathfrak{s}}", -->
                            <!--       shifts: "{\\delta}", -->
                            <!--       optim: "{\\beta}", -->
                            <!--       param: "{\\theta}", -->
                            <!--       unif: ["{\\mathcal{U}\\left({#1}\\right)}", 1], -->
                            <!--       argmin: "\\mathop{\\mathrm{argmin}}", -->
                            <!--       diag: "\\mathop{\\mathrm{Diag}}", -->
                            <!--       rang: "\\mathop{\\mathrm{rang}}", -->
                            <!--       pa: "\\mathop{\\mathrm{pa}}", -->
                            <!--       mrca: "\\mathop{\\mathrm{mrca}}", -->
                            <!--       desc: "\\mathop{\\mathrm{desc}}", -->
                            <!--       warning: ["\\color{red}{{#1}}", 1] -->
                            <!--     } -->
                    <!--   } -->
            <!-- }); -->
    <!-- </script> -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {Macros: {
            E: "{\\mathbb{E}}"
        },
        Augment: {
        Definitions: {
          delimiter: {
            "\\llbracket": '\u27E6',
            '\\rrbracket': '\u27E7'
          }}
        }}
    });
</script>
    <body>
    <div style="display:none" aria-hidden="true">
    \(
        \newcommand\R{\mathbb{R}}
        \newcommand\Z{\mathbb{Z}}
        \newcommand\LL{\mathbb{L}}
        \newcommand{\E}[1]{\mathbb{E}\left[#1\right]}
        \newcommand{\V}[1]{\mathbb{V}\left[#1\right]}
        \newcommand{\ps}[2]{\left\langle #1 \,,\, #2 \right\rangle}
        \newcommand\1{\mathbb{1}}
        \newcommand\N{\mathbb{N}}
        \newcommand\Norm{\mathcal{N}}
        \newcommand{\transp}[1]{{}^t\!#1}
        \newcommand\ud{\,\mathrm{d}}
        \DeclareMathOperator*{\argmax}{argmax}
        \DeclareMathOperator*{\argmin}{argmin}
        \DeclareMathOperator{\e}{e}
        \DeclareMathOperator{\Cov}{Cov}
        \DeclareMathOperator{\Determinant}{det}
        \newcommand{\determinant}[1]{\Determinant\left(#1\right)}
    \)
    </div>
    </body>
            
<div id="sec-nonparamreg" class="section level1" number="5">
<h1><span class="header-section-number">Chapitre 5</span> Régression non paramétrique et régression polynomiale locale</h1>
<p>Comme notamment montré par <span class="citation"><a href="#ref-Loader1999" role="doc-biblioref">Loader</a> (<a href="#ref-Loader1999" role="doc-biblioref">1999</a>)</span>, la régression locale est un cas particulier de la régression non paramétrique.
Supposons que l’on ait un ensemble de points <span class="math inline">\((x_i,y_i)_{1\leq i\leq n}\)</span>.
La régression non paramétrique consiste à supposer qu’il existe une fonction <span class="math inline">\(\mu\)</span>, à estimer, telle que <span class="math inline">\(y_i=\mu(x_i)+\varepsilon_i\)</span> avec <span class="math inline">\(\varepsilon_i\)</span> un terme d’erreur.
D’après le théorème de Taylor, pour tout point <span class="math inline">\(x_0\)</span>, si <span class="math inline">\(\mu\)</span> est différentiable <span class="math inline">\(d\)</span> fois, alors :
<span class="math display">\[
\forall x \::\:\mu(x) = \mu(x_0) + \mu&#39;(x_0)(x-x_0)+\dots +
\frac{\mu^{(d)}(x_0)}{d!}(x-a)^d+R_d(x),
\]</span>
où <span class="math inline">\(R_d\)</span> est un terme résiduel négligeable au voisinage de <span class="math inline">\(x_0\)</span>.
Dans un voisinage <span class="math inline">\(h(x_0)\)</span> autour de <span class="math inline">\(x_0\)</span>, <span class="math inline">\(\mu\)</span> peut être approchée par un polynôme de degré <span class="math inline">\(d\)</span>.
La quantité <span class="math inline">\(h(x_0)\)</span> est appelée <em>fenêtre</em> (<em>bandwidth</em>).
Si <span class="math inline">\(\varepsilon_i\)</span> est un bruit blanc, on peut donc estimer par les moindres carrés <span class="math inline">\(\mu(x_0)\)</span> en utilisant les observations qui sont dans <span class="math inline">\(\left[x_0-h(x_0),x_0+h(x_0)\right]\)</span>.</p>
<div id="sec-proietti" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Régression polynomiale : approche de Proietti et Luati</h2>
<div id="filtres-symétriques" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Filtres symétriques</h3>
<p>Reprenons maintenant les notations de <span class="citation"><a href="#ref-proietti2008" role="doc-biblioref">Proietti et Luati</a> (<a href="#ref-proietti2008" role="doc-biblioref">2008</a>)</span> : supposons que notre série temporelle <span class="math inline">\(y_t\)</span> peut être décomposée en
<span class="math display">\[
y_t=\mu_t+\varepsilon_t,
\]</span>
où <span class="math inline">\(\mu_t\)</span> est la tendance et <span class="math inline">\(\varepsilon_{t}\overset{i.i.d}{\sim}\mathcal{N}(0,\sigma^{2})\)</span> est le bruit<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a>.
La tendance <span class="math inline">\(\mu_t\)</span> est localement approchée par un polynôme de degré <span class="math inline">\(d\)</span>, de sorte que dans un voisinage <span class="math inline">\(h\)</span> de <span class="math inline">\(t\)</span> <span class="math inline">\(\mu_t\simeq m_{t}\)</span> avec :
<span class="math display">\[
\forall j\in\left\llbracket -h,h\right\rrbracket :\:
y_{t+j}=m_{t+j}+\varepsilon_{t+j},\quad m_{t+j}=\sum_{i=0}^{d}\beta_{i}j^{i}.
\]</span>
Le problème d’extraction de la tendance est équivalent à l’estimation de <span class="math inline">\(m_t=\beta_0\)</span>.
En notation matricielle :
<span class="math display">\[
\underbrace{\begin{pmatrix}y_{t-h}\\
y_{t-(h-1)}\\
\vdots\\
y_{t}\\
\vdots\\
y_{t+(h-1)}\\
y_{t+h}
\end{pmatrix}}_{y}=\underbrace{\begin{pmatrix}1 &amp; -h &amp; h^{2} &amp; \cdots &amp; (-h)^{d}\\
1 &amp; -(h-1) &amp; (h-1)^{2} &amp; \cdots &amp; (-(h-1))^{d}\\
\vdots &amp; \vdots &amp; \vdots &amp; \cdots &amp; \vdots\\
1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \vdots &amp; \cdots &amp; \vdots\\
1 &amp; h-1 &amp; (h-1)^{2} &amp; \cdots &amp; (h-1)^{d}\\
1 &amp; h &amp; h^{2} &amp; \cdots &amp; h^{d}
\end{pmatrix}}_{X}\underbrace{\begin{pmatrix}\beta_{0}\\
\beta_{1}\\
\vdots\\
\vdots\\
\vdots\\
\vdots\\
\beta_{d}
\end{pmatrix}}_{\beta}+\underbrace{\begin{pmatrix}\varepsilon_{t-h}\\
\varepsilon_{t-(h-1)}\\
\vdots\\
\varepsilon_{t}\\
\vdots\\
\varepsilon_{t+(h-1)}\\
\varepsilon_{t+h}
\end{pmatrix}}_{\varepsilon}
\]</span></p>
<p>Pour estimer <span class="math inline">\(\beta\)</span> il faut <span class="math inline">\(H\geq d+1\)</span> et l’estimation est faite par moindres carrés pondérés — <em>weighted least squares</em> (WLS) —, ce qui revient à minimiser la fonction objectif suivante :
<span class="math display">\[
S(\hat{\beta}_{0},\dots,\hat{\beta}_{d})=\sum_{j=-h}^{h}\kappa_{j}(y_{t+j}-\hat{\beta}_{0}-\hat{\beta}_{1}j-\dots-\hat{\beta}_{d}j^{d})^{2}
\]</span>
où <span class="math inline">\(\kappa_j\)</span> est un ensemble de poids appelés <em>noyaux</em> (<em>kernel</em>).
On a <span class="math inline">\(\kappa_j\geq 0:\kappa_{-j}=\kappa_j\)</span>, et en notant <span class="math inline">\(K=diag(\kappa_{-h},\dots,\kappa_{h})\)</span>, l’estimateur <span class="math inline">\(\beta\)</span> peut s’écrire <span class="math inline">\(\hat{\beta}=(X&#39;KX)^{1}X&#39;Ky\)</span>.
Avec <span class="math inline">\(e_{1}=\begin{pmatrix}1&amp;0&amp;\cdots&amp;0\end{pmatrix}&#39;\)</span>, l’estimateur de la tendance peut donc s’écrire :
<span class="math display">\[
\hat{m}_{t}=e_{1}\hat{\beta}=\theta&#39;y=\sum_{j=-h}^{h}\theta_{j}y_{t-j}\text{ avec }\theta=KX(X&#39;KX)^{-1}e_{1}
\]</span>
En somme, l’estimation de la tendance <span class="math inline">\(\hat{m}_{t}\)</span> est obtenue en appliquant une moyenne mobile symétrique <span class="math inline">\(\theta\)</span> à <span class="math inline">\(y_t\)</span><a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>.
De plus, <span class="math inline">\(X&#39;\theta=e_{1}\)</span> donc :
<span class="math display">\[
\sum_{j=-h}^{h}\theta_{j}=1,\quad\forall r\in\left\llbracket 1,d\right\rrbracket :\sum_{j=-h}^{h}j^{r}\theta_{j}=0.
\]</span>
Ainsi, la moyenne mobile <span class="math inline">\(\theta\)</span> préserve les polynômes de degré <span class="math inline">\(d\)</span>.</p>
<p>Concernant le choix des paramètres, l’idée générale qui prévaut est que le choix entre ces différents noyaux est secondaire<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> et qu’il vaut mieux se concentrer sur deux autres paramètres :</p>
<ul>
<li><p>le degré du polynôme <span class="math inline">\(d\)</span> : s’il est trop petit on risque d’avoir des estimations biaisées de la tendance-cycle et s’il est trop grand on risque d’avoir une trop grande variance dans les estimations (du fait d’un sur-ajustement) ;</p></li>
<li><p>le nombre de voisins <span class="math inline">\(H=2h+1\)</span> (ou la fenêtre <span class="math inline">\(h\)</span>) : s’il est trop petit alors trop peu de données seront utilisées pour les estimations (ce qui conduira à une grande variance dans les estimations) et s’il est trop grand alors l’approximation polynomiale sera vraisemblablement fausse ce qui conduira à avoir des estimations biaisées.</p></li>
</ul>
</div>
<div id="sec-kernels" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Les différents noyaux</h3>
<p>Dans les problèmes d’extraction du signal, les observations sont généralement pondérées par rapport à leur distance à la date <span class="math inline">\(t\)</span> : pour estimer la tendance-cycle à la date <span class="math inline">\(t\)</span>, on accorde généralement plus d’importance aux observations qui sont proches de <span class="math inline">\(t\)</span>.</p>
<p>Dans le cas continu, un noyau <span class="math inline">\(K\)</span> est une fonction positive, paire et intégrable telle que <span class="math inline">\(\int_{-\infty}^{+\infty}\kappa(u) \ud u=1\)</span> et <span class="math inline">\(\kappa(u)=\kappa(-u)\)</span>.
Dans le cas discret, un noyau est un ensemble de poids <span class="math inline">\(\kappa_j\)</span>, <span class="math inline">\(j=0,\pm1,\dots,\pm h\)</span> avec <span class="math inline">\(\kappa_j \geq0\)</span> et <span class="math inline">\(\kappa_j=\kappa_{-j}\)</span>.</p>
<p>Une classe importante de noyaux est celle des noyaux Beta.
Dans le cas discret, à un facteur multiplicatif près (de sorte que <span class="math inline">\(\sum_{j=-h}^h\kappa_j=1\)</span>) :
<span class="math display">\[
\kappa_j = \left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^r
\right)^s,\quad\text{avec }r&gt;0,s\geq 0
\]</span>
Cette classe englobe la majorité des noyaux présentés dans cette étude, à l’exception des noyaux d’Henderson, trapézoïdal et gaussien.
Les principaux noyaux (qui sont également implémentés dans <code>rjdfilters</code>) sont :</p>
<div class="multicols">
<ul>
<li><p><span class="math inline">\(r=1,s=0\)</span> noyau uniforme :
<span class="math display">\[\kappa_j^U=1\]</span></p></li>
<li><p><span class="math inline">\(r=s=1\)</span> noyau triangulaire :
<span class="math display">\[\kappa_j^T=\left(
1-
\left\lvert
\frac j {h+1}
\right\lvert
\right)\]</span></p></li>
<li><p><span class="math inline">\(r=2,s=1\)</span> noyau d’Epanechnikov (ou parabolique) :
<span class="math display">\[\kappa_j^E=\left(
1-
\left\lvert
\frac j {h+1}
\right\lvert^2
\right)\]</span></p></li>
<li><p><span class="math inline">\(r=s=2\)</span> noyau quadratique (<em>biweight</em>) :
<span class="math display">\[\kappa_j^{BW}=\left(
1-
\left\lvert
\frac j {h+1}
\right\lvert^2
\right)^2\]</span></p></li>
<li><p><span class="math inline">\(r = 2, s = 3\)</span> noyau cubique (<em>triweight</em>) :
<span class="math display">\[\kappa_j^{TW}=\left(
1-
\left\lvert
\frac j {h+1}
\right\lvert^2
\right)^3\]</span></p></li>
<li><p><span class="math inline">\(r = s = 3\)</span> noyau tricube :
<span class="math display">\[\kappa_j^{TC}=\left(
1-
\left\lvert
\frac j {h+1}
\right\lvert^3
\right)^3\]</span></p></li>
<li><p>noyau d’Henderson (voir partie <a href="sec-nonparamreg.html#sec-sympolyfilter">5.1.3</a> pour plus de détails) :
<span class="math display">\[
\kappa_{j}=\left[1-\frac{j^2}{(h+1)^2}\right]
\left[1-\frac{j^2}{(h+2)^2}\right]
\left[1-\frac{j^2}{(h+3)^2}\right]
\]</span></p></li>
<li><p>noyau trapézoïdal :
<span class="math display">\[
\kappa_j^{TP}=
\begin{cases}
\frac{1}{3(2h-1)} &amp; \text{ if }j=\pm h 
\\
\frac{2}{3(2h-1)} &amp; \text{ if }j=\pm (h-1)\\
\frac{1}{2h-1}&amp; \text{ otherwise}
\end{cases}
\]</span></p></li>
<li><p>noyau gaussien<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a>:
<span class="math display">\[
\kappa_j^G=\exp\left(
-\frac{
j^2
}{
2\sigma^2h^2
}\right)
\]</span></p></li>
</ul>
</div>
<!-- Let $x\in ]0,1[$ and $f_x(a,b)=\left(1-x^{a}\right)^{b}$. We have: -->
<!-- \begin{align*} -->
<!-- \frac{\partial}{\partial a}f(a,b) &=-a\ln (x)x^a(1-x^{a})^{b}>0 \\ -->
<!-- \frac{\partfial}{\partial b}f(a,b)&=\ln(1-x^{a})(1-x^{a})^{b} <0 -->
<!-- \end{align*} -->
<!-- So: -->
<p>Les noyaux d’Henderson, trapézoïdal et gaussien sont particuliers :</p>
<ul>
<li><p>Les fonctions noyau d’Henderson et trapézoïdal changent avec la fenêtre (les autres dépendent uniquement du rapport <span class="math inline">\(j/h+1\)</span>).</p></li>
<li><p>Pour les noyaux trapézoïdal et gaussien, d’autres définitions pourraient être utilisées et sont donc définis arbitrairement.<br />
Le noyau trapézoïdal est implémenté dans <code>rjdfilters</code> car il permet de calculer les moyennes mobiles utilisées dans l’algorithme X-13ARIMA pour l’extraction des composantes saisonnières.
Il n’est pas adapté dans le cas de l’extraction de la tendance-cycle.</p></li>
</ul>
</div>
<div id="sec-sympolyfilter" class="section level3" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Quelques filtres symétriques particuliers</h3>
<p>Lorsque <span class="math inline">\(p=0\)</span> (ajustement local par une constante) on obtient l’estimateur de <strong>Nadaraya-Watson</strong> (ou l’estimateur par noyaux).</p>
<p>Avec le noyau uniforme on obtient le filtre de <span class="citation"><a href="#ref-macaulay1931smoothing" role="doc-biblioref">Macaulay et al.</a> (<a href="#ref-macaulay1931smoothing" role="doc-biblioref">1931</a>)</span>.
Lorsque <span class="math inline">\(p=0\)</span> ou <span class="math inline">\(p=1\)</span>, on retrouve la moyenne arithmétique : <span class="math inline">\(w_j=w=\frac{1}{2h+1}\)</span>.</p>
<p>Le noyau d’<strong>Epanechnikov</strong> est souvent recommandé comme le noyau optimal car il minimise l’erreur quadratique moyenne de l’estimation par polynômes locaux.</p>
<p>Le <strong>Loess</strong>, <em>locally estimated scatterplot smoothing</em> (utilisé dans la méthode STL), est une régression locale pondérée qui utilise le noyau tricube.</p>
<p>Le <strong>filtre d’Henderson</strong> est un cas particulier de l’approximation locale cubique (<span class="math inline">\(p=3\)</span>), couramment utilisée pour l’extraction de la tendance-cycle (c’est par exemple le filtre utilisé dans le logiciel de désaisonnalisation X-13ARIMA).
Pour une fenêtre fixée, Henderson a trouvé le noyau qui donnait l’estimation la plus lisse de la tendance.
Il montre l’équivalence entre les trois problèmes suivants :</p>
<ol style="list-style-type: decimal">
<li>minimiser la variance de la différence d’ordre trois de la série lissée par l’application d’une moyenne mobile ;<br />
</li>
<li>minimiser la somme du carré de la différence d’ordre trois des coefficients du filtre, c’est le critère de lissage (<em>smoothness</em>) : <span class="math inline">\(S=\sum_j(\nabla^{3}\theta_{j})^{2}\)</span> ;<br />
</li>
<li>estimer une tendance localement cubique par les moindres carrés pondérés, où les poids sont choisis de sorte à minimiser la <em>smoothness</em> (cela conduit au noyau présenté dans la section <a href="sec-nonparamreg.html#sec-kernels">5.1.2</a>).</li>
</ol>
<p>Le filtre d’Henderson étant couramment utilisé pour l’extraction de la tendance-cycle, nous nous intéresserons uniquement aux filtres issus du noyau d’Henderson.</p>
</div>
<div id="subsec-lppasymf" class="section level3" number="5.1.4">
<h3><span class="header-section-number">5.1.4</span> Filtres asymétriques</h3>
<p>Comme mentionné dans la partie <a href="sec-propMM.html#subec:mmetprev">2.3.1</a>, pour l’estimation en temps réel, plusieurs approches peuvent être utilisées :</p>
<ol style="list-style-type: decimal">
<li><p>Construire un filtre asymétrique par approximation polynomiale locale sur les observations disponibles (<span class="math inline">\(y_{t}\)</span> pour <span class="math inline">\(t\in\left\llbracket n-h,n\right\rrbracket\)</span>).</p></li>
<li><p>Appliquer les filtres symétriques sur les séries prolongées par prévision <span class="math inline">\(\hat{y}_{n+l\mid n},l\in\left\llbracket 1,h\right\rrbracket\)</span>.</p></li>
<li><p>Construire des filtres asymétriques qui minimisent l’erreur quadratique moyenne de révision sous des contraintes de reproduction de tendances polynomiales.</p></li>
</ol>
<p><span class="citation"><a href="#ref-proietti2008" role="doc-biblioref">Proietti et Luati</a> (<a href="#ref-proietti2008" role="doc-biblioref">2008</a>)</span> montrent que les deux premières approches sont équivalentes lorsque les prévisions sont faites par extrapolation polynomiale de degré <span class="math inline">\(d\)</span>.
Elles sont également équivalentes à la troisième approche sous les mêmes contraintes que celles du filtre symétrique.
Cette méthode est appelée <em>direct asymmetric filter</em> (DAF).
Même si les estimations sont sans biais, c’est au coût d’une plus grande variance dans les estimations.</p>
<p>Pour résoudre le problème de la variance des estimations des filtres temps réel, <span class="citation"><a href="#ref-proietti2008" role="doc-biblioref">Proietti et Luati</a> (<a href="#ref-proietti2008" role="doc-biblioref">2008</a>)</span> proposent une méthode générale pour construire les filtres asymétriques qui permet de faire un compromis biais-variance.
Il s’agit d’une généralisation des filtres asymétriques de <span class="citation"><a href="#ref-musgrave1964set" role="doc-biblioref">Musgrave</a> (<a href="#ref-musgrave1964set" role="doc-biblioref">1964</a>)</span> (utilisés dans l’algorithme de désaisonnalisation X-13ARIMA).</p>
<p>On modélise ici la série en entrée par :
<span class="math display" id="eq:lpgeneralmodel">\[\begin{equation}
y=U\gamma+Z\delta+\varepsilon,\quad
\varepsilon\sim\mathcal{N}(0,D)
\tag{5.1}
\end{equation}\]</span>
où <span class="math inline">\([U,Z]\)</span> est de rang plein et forme un sous-ensemble des colonnes de <span class="math inline">\(X\)</span>.
L’objectif est de trouver un filtre <span class="math inline">\(v\)</span> qui minimisent l’erreur quadratique moyenne de révision (au filtre symétrique <span class="math inline">\(\theta\)</span>) sous certaines contraintes.
Ces contraintes sont représentées par la matrice <span class="math inline">\(U=\begin{pmatrix}U_{p}&#39;&amp;U_{f}&#39;\end{pmatrix}&#39;\)</span> : <span class="math inline">\(U_p&#39;v=U&#39;\theta\)</span> (avec <span class="math inline">\(U_p\)</span> la matrice <span class="math inline">\((h+q+1)\times (d+1)\)</span> qui contient les observations de la matrice <span class="math inline">\(U\)</span> connues lors de l’estimation par le filtre asymétrique).
Le problème est équivalent à trouver <span class="math inline">\(v\)</span> qui minimise :
<span class="math display" id="eq:lppasym">\[\begin{equation}
\varphi(v)=
\underbrace{
  \underbrace{(v-\theta_{p})&#39;D_{p}(v-\theta_{p})+
  \theta_{f}&#39;D_{f}\theta_{f}}_\text{variance de l&#39;erreur de révision}+
  \underbrace{[\delta&#39;(Z_{p}&#39;v-Z&#39;\theta)]^{2}}_{biais^2}
}_\text{Erreur quadratique moyenne de révision}+
\underbrace{2l&#39;(U_{p}&#39;v-U&#39;\theta)}_{\text{contraintes}}
\tag{5.2}
\end{equation}\]</span>
où <span class="math inline">\(l\)</span> est le vecteur des multiplicateurs de Lagrange.</p>
<p>Lorsque <span class="math inline">\(U=X\)</span>, la contrainte équivaut à préserver les polynômes de degré <span class="math inline">\(d\)</span> : on retrouve les filtres directs asymétriques (DAF) lorsque <span class="math inline">\(D=K^{-1}\)</span>.</p>
<p>Lorsque <span class="math inline">\(U=\begin{pmatrix}1&amp;\cdots&amp;1\end{pmatrix}&#39;\)</span>, <span class="math inline">\(Z=\begin{pmatrix}-h&amp;\cdots&amp;+h\end{pmatrix}&#39;\)</span>, <span class="math inline">\(\delta=\delta_1\)</span>, <span class="math inline">\(D=\sigma^2I\)</span> et lorsque le filtre symétrique est le filtre d’Henderson, on retrouve les filtres asymétriques de Musgrave.
Ce filtre suppose, que pour l’estimation en temps réel, les données sont générées par un processus linéaire et que les filtres asymétriques préservent les constantes (<span class="math inline">\(\sum v_i=\sum \theta_i=1\)</span>).
Ces filtres asymétriques dépendent du rapport <span class="math inline">\(\delta_1/\sigma\)</span>, qui est lié à l’I-C ratio <span class="math inline">\(R=\frac{\bar{I}}{\bar{C}}=\frac{\sum\lvert I_t-I_{t-1}\rvert}{\sum\lvert C_t-C_{t-1}\rvert}\)</span> (<span class="math inline">\(\delta_1/\sigma=2/(R\sqrt{\pi})\)</span>), qui est notamment utilisé dans X-13ARIMA pour déterminer la longueur du filtre d’Henderson<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a>.</p>
<p>Lorsque <span class="math inline">\(U\)</span> correspond aux <span class="math inline">\(d^*+1\)</span> premières colonnes de <span class="math inline">\(X\)</span>, <span class="math inline">\(d^*&lt;d\)</span>, la contrainte consiste à reproduire des tendances polynomiales de degré <span class="math inline">\(d^*\)</span>.
Cela introduit du bais mais réduit la variance.
Ainsi, <span class="citation"><a href="#ref-proietti2008" role="doc-biblioref">Proietti et Luati</a> (<a href="#ref-proietti2008" role="doc-biblioref">2008</a>)</span> proposent trois classes de filtres asymétriques :</p>
<ol style="list-style-type: decimal">
<li><p><em>Linear-Constant</em> (LC) : <span class="math inline">\(y_t\)</span> linéaire (<span class="math inline">\(d=1\)</span>) et <span class="math inline">\(v\)</span> préserve les constantes (<span class="math inline">\(d^*=0\)</span>).
On obtient le filtre de Musgrave avec le filtre d’Henderson comme filtre symétrique.</p></li>
<li><p><em>Quadratic-Linear</em> (QL) : <span class="math inline">\(y_t\)</span> quadratique (<span class="math inline">\(d=2\)</span>) et <span class="math inline">\(v\)</span> préserve les tendances linéaires (<span class="math inline">\(d^*=1\)</span>).</p></li>
<li><p><em>Cubic-Quadratic</em> (CQ) : <span class="math inline">\(y_t\)</span> cubic (<span class="math inline">\(d=3\)</span>) et <span class="math inline">\(v\)</span> préserve les tendances quadratiques (<span class="math inline">\(d^*=2\)</span>).</p></li>
</ol>
<p>Le tableau <a href="sec-nonparamreg.html#tab:criteriaLp">5.1</a> compare les critères de qualité des différentes méthodes en utilisant le filtre d’Henderson et <span class="math inline">\(h=6\)</span> (filtre symétrique de 13 termes).
Pour les filtres en temps réel (<span class="math inline">\(q=0\)</span>), plus le filtre asymétrique est complexe (en termes de préservation polynomiale), moins la <em>timeliness</em> est élevée et plus la <em>fidelity</em>/<em>smoothness</em> est grande : la réduction du déphasage se fait au détriment d’une augmentation de la variance.
Ce résultat varie lorsque <span class="math inline">\(q\)</span> augmente : pour <span class="math inline">\(q=2\)</span> le filtre QL a une plus grande <em>timeliness</em> que le filtre LC.
Ce résultat étonnant souligne le fait que le déphasage n’est pas contrôlé par l’approche de <span class="citation"><a href="#ref-proietti2008" role="doc-biblioref">Proietti et Luati</a> (<a href="#ref-proietti2008" role="doc-biblioref">2008</a>)</span>.</p>
<p>En termes de révision, (<span class="math inline">\(A_w+S_w+T_w+R_w\)</span>), les filtres LC et QL donnent toujours de meilleurs résultats que les filtres CQ et DAF.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:criteriaLp">Table 5.1 : </span>Critères de qualité des filters asymétriques (<span class="math inline">\(q=0,1,2\)</span>) calculés par polynômes locaux en utilisant le noyau d’Henderson avec <span class="math inline">\(h=6\)</span> et <span class="math inline">\(R=3,5\)</span>.
</caption>
<thead>
<tr>
<th style="text-align:center;">
Method
</th>
<th style="text-align:center;">
<span class="math inline">\(b_c\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(b_l\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(b_q\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(F_g\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(S_g\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(T_g \times 10^{-3}\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(A_w\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(S_w\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(T_w\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(R_w\)</span>
</th>
</tr>
</thead>
<tbody>
<tr grouplength="4">
<td colspan="11" style="border-bottom: 1px solid;">
<strong><span class="math inline">\(q=0\)</span></strong>
</td>
</tr>
<tr>
<td style="text-align:center;padding-left: 2em;" indentlevel="1">
LC
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
-0,41
</td>
<td style="text-align:center;">
-2,16
</td>
<td style="text-align:center;">
0,39
</td>
<td style="text-align:center;">
1,27
</td>
<td style="text-align:center;">
30,34
</td>
<td style="text-align:center;">
0,10
</td>
<td style="text-align:center;">
0,49
</td>
<td style="text-align:center;">
0,41
</td>
<td style="text-align:center;">
0,55
</td>
</tr>
<tr>
<td style="text-align:center;padding-left: 2em;" indentlevel="1">
QL
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
-0,47
</td>
<td style="text-align:center;">
0,71
</td>
<td style="text-align:center;">
5,15
</td>
<td style="text-align:center;">
0,05
</td>
<td style="text-align:center;">
0,07
</td>
<td style="text-align:center;">
1,89
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,11
</td>
</tr>
<tr>
<td style="text-align:center;padding-left: 2em;" indentlevel="1">
CQ
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,91
</td>
<td style="text-align:center;">
11,94
</td>
<td style="text-align:center;">
0,01
</td>
<td style="text-align:center;">
0,02
</td>
<td style="text-align:center;">
2,23
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,10
</td>
</tr>
<tr>
<td style="text-align:center;padding-left: 2em;" indentlevel="1">
DAF
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,94
</td>
<td style="text-align:center;">
14,20
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,01
</td>
<td style="text-align:center;">
2,18
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,10
</td>
</tr>
<tr grouplength="4">
<td colspan="11" style="border-bottom: 1px solid;">
<strong><span class="math inline">\(q=1\)</span></strong>
</td>
</tr>
<tr>
<td style="text-align:center;padding-left: 2em;" indentlevel="1">
LC
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
-0,12
</td>
<td style="text-align:center;">
-0,52
</td>
<td style="text-align:center;">
0,27
</td>
<td style="text-align:center;">
0,43
</td>
<td style="text-align:center;">
4,80
</td>
<td style="text-align:center;">
0,01
</td>
<td style="text-align:center;">
0,12
</td>
<td style="text-align:center;">
0,06
</td>
<td style="text-align:center;">
0,11
</td>
</tr>
<tr>
<td style="text-align:center;padding-left: 2em;" indentlevel="1">
QL
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
-0,06
</td>
<td style="text-align:center;">
0,29
</td>
<td style="text-align:center;">
0,71
</td>
<td style="text-align:center;">
0,69
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,19
</td>
<td style="text-align:center;">
0,01
</td>
<td style="text-align:center;">
0,04
</td>
</tr>
<tr>
<td style="text-align:center;padding-left: 2em;" indentlevel="1">
CQ
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,37
</td>
<td style="text-align:center;">
0,57
</td>
<td style="text-align:center;">
0,16
</td>
<td style="text-align:center;">
0,02
</td>
<td style="text-align:center;">
0,58
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,06
</td>
</tr>
<tr>
<td style="text-align:center;padding-left: 2em;" indentlevel="1">
DAF
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,41
</td>
<td style="text-align:center;">
0,37
</td>
<td style="text-align:center;">
0,06
</td>
<td style="text-align:center;">
0,02
</td>
<td style="text-align:center;">
0,76
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,06
</td>
</tr>
<tr grouplength="4">
<td colspan="11" style="border-bottom: 1px solid;">
<strong><span class="math inline">\(q=2\)</span></strong>
</td>
</tr>
<tr>
<td style="text-align:center;padding-left: 2em;" indentlevel="1">
LC
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
1,08
</td>
<td style="text-align:center;">
0,20
</td>
<td style="text-align:center;">
0,08
</td>
<td style="text-align:center;">
0,35
</td>
<td style="text-align:center;">
0,01
</td>
<td style="text-align:center;">
0,01
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,01
</td>
</tr>
<tr>
<td style="text-align:center;padding-left: 2em;" indentlevel="1">
QL
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,03
</td>
<td style="text-align:center;">
0,22
</td>
<td style="text-align:center;">
0,05
</td>
<td style="text-align:center;">
2,08
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,01
</td>
<td style="text-align:center;">
0,02
</td>
<td style="text-align:center;">
0,07
</td>
</tr>
<tr>
<td style="text-align:center;padding-left: 2em;" indentlevel="1">
CQ
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,37
</td>
<td style="text-align:center;">
0,66
</td>
<td style="text-align:center;">
0,13
</td>
<td style="text-align:center;">
0,02
</td>
<td style="text-align:center;">
0,56
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,06
</td>
</tr>
<tr>
<td style="text-align:center;padding-left: 2em;" indentlevel="1">
DAF
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,40
</td>
<td style="text-align:center;">
0,77
</td>
<td style="text-align:center;">
0,02
</td>
<td style="text-align:center;">
0,02
</td>
<td style="text-align:center;">
0,68
</td>
<td style="text-align:center;">
0,00
</td>
<td style="text-align:center;">
0,05
</td>
</tr>
</tbody>
</table>
<p>Une application en ligne, disponible à l’adresse <a href="https://aqlt.shinyapps.io/FiltersProperties/" class="uri">https://aqlt.shinyapps.io/FiltersProperties/</a>, permet de comparer les coefficients, les fonctions de gain et de déphasage entre les différentes méthodes et les différents noyaux.</p>
<div class="summary_box">
<div class="title">
<p>Filtres locaux polynomiaux (<span class="citation"><a href="#ref-proietti2008" role="doc-biblioref">Proietti et Luati</a> (<a href="#ref-proietti2008" role="doc-biblioref">2008</a>)</span>)</p>
</div>
<p><strong>Avantages</strong> :</p>
<ul>
<li><p>Modèles avec une interprétation simple.</p></li>
<li><p>Le filtre asymétrique est indépendant de la date d’estimation.
Toutefois, il dépend indirectement des données si le filtre est calibré sur l’I-C ratio.</p></li>
</ul>
<p><strong>Inconvénients</strong> :</p>
<ul>
<li>La <em>timeliness</em> n’est pas contrôlée (mais peut être introduite dans le programme de minimisation).</li>
</ul>
</div>
</div>
</div>
<div id="subsec-lptimeliness" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Extension avec le critère de <em>timeliness</em></h2>
<p>Un inconvénient de la méthode précédente est que le déphasage n’est pas contrôlé.
Il est en revanche possible de généraliser d’avantage la modélisation en ajoutant le critère de <em>timeliness</em> définit par <span class="citation"><a href="#ref-ch15HBSA" role="doc-biblioref">Grun-Rehomme, Guggemos, et Ladiray</a> (<a href="#ref-ch15HBSA" role="doc-biblioref">2018</a>)</span> dans l’équation <a href="sec-nonparamreg.html#eq:lppasym">(5.2)</a>.
C’est ce qui a été proposé par Jean Palate, puis codé en Java<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a> et intégré dans <code>rjdfilters</code>.</p>
<p>En utilisant les mêmes notations que dans <a href="sec-nonparamreg.html#subsec-lppasymf">5.1.4</a>, <span class="math inline">\(\theta\)</span> le filtre symétrique et <span class="math inline">\(v\)</span> le filtre asymétrique.
Notons également <span class="math inline">\(\theta=\begin{pmatrix}\theta_p\\\theta_f\end{pmatrix}\)</span> avec <span class="math inline">\(\theta_p\)</span> de même longueur que <span class="math inline">\(v\)</span>, et <span class="math inline">\(g=v-\theta_p\)</span>.
Le critère de <em>timeliness</em> s’écrit :
<span class="math display">\[
T_g(v)=v&#39;Tv=g&#39;Tg+2\theta_p&#39;Tg+\theta_p&#39;T\theta_p
\quad(T\text{ étant symétrique)}.
\]</span>
De plus, la fonction objectif <span class="math inline">\(\varphi\)</span> de l’équation <a href="sec-nonparamreg.html#eq:lppasym">(5.2)</a> peut se réécrire :
<span class="math display">\[\begin{align*}
\varphi(v)&amp;=(v-\theta_p)&#39;D_{p}(v-\theta_p)+
  \theta_f&#39;D_{f}\theta_f+
  [\delta&#39;(Z_{p}&#39;v-Z&#39;\theta)]^{2}+
2l&#39;(U_{p}&#39;v-U&#39;\theta)\\
&amp;=g&#39;Qg-2Pg+2l&#39;(U_{p}&#39;v-U&#39;\theta)+c\quad\text{avec }
\begin{cases}
Q=D_p+Z_p\delta\delta&#39;Z&#39;_p \\
P=\theta_fZ_f\delta\delta&#39;Z_p&#39;\\
c\text{ une constante indépendante de }v
\end{cases}.
\end{align*}\]</span></p>
<p>En ajoutant le critère de <em>timeliness</em>, on obtient :
<span class="math display">\[
\widetilde\varphi(v)=g&#39;\widetilde Qg-
2\widetilde Pg+2l&#39;(U_{p}&#39;v-U&#39;\theta)+
\widetilde c\quad\text{avec }
\begin{cases}
\widetilde Q=D_p+Z_p\delta\delta&#39;Z&#39;_p +\alpha_TT\\
\widetilde P=\theta_fZ_f\delta\delta&#39;Z_p&#39;-\alpha_T\theta_pT\\
\widetilde c\text{ une constante indépendante de }v
\end{cases}
\]</span>
où <span class="math inline">\(\alpha_T\)</span> est le poids associé au critère de <em>timeliness</em>.
Avec <span class="math inline">\(\alpha_T=0\)</span> on retrouve <span class="math inline">\(\varphi(v)\)</span>.
Cette extension permet donc de retrouver tous les filtres symétriques et asymétriques présentés dans la section précédente mais généralise également l’approche de <span class="citation"><a href="#ref-GrayThomson1996" role="doc-biblioref">Gray et Thomson</a> (<a href="#ref-GrayThomson1996" role="doc-biblioref">1996</a>)</span> présentée dans la section <a href="sec-nonparamreg.html#subsec-graythomson">5.3</a>. Elle est proche de l’approche FRST.</p>
<p>Cette extension s’inscrit dans le cadre de la théorie générale définie dans la section <a href="sec-theoriegen.html#subsec-theoriegen">3.1</a>.
Cela revient en effet à minimiser une somme pondérée de l’erreur quadratique de révision :
<span class="math display">\[
\E{\left( \sum_{i=-h}^h\theta^s_{i}y_{t+s}-\sum_{i=-h}^qv_iy_{t+s} \right)^2}
= I(v,\,0,\,y_t,\,M_{\theta^s} y_t)
\]</span>
et du critère de <em>timeliness</em> :
<span class="math display">\[
T_g(\theta) = J(f\colon(\rho,\varphi)\mapsto\rho^2\sin(\varphi)^2,\,\omega_1, \,\omega_2)
\]</span>
sous une contrainte linéaire.</p>
</div>
<div id="subsec-graythomson" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Régression polynomiale : Gray et Thomson</h2>
<div id="filtres-symétriques-1" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Filtres symétriques</h3>
<p>L’approche de <span class="citation"><a href="#ref-GrayThomson1996" role="doc-biblioref">Gray et Thomson</a> (<a href="#ref-GrayThomson1996" role="doc-biblioref">1996</a>)</span> est proche de celles de <span class="citation"><a href="#ref-proietti2008" role="doc-biblioref">Proietti et Luati</a> (<a href="#ref-proietti2008" role="doc-biblioref">2008</a>)</span> et de <span class="citation"><a href="#ref-ch15HBSA" role="doc-biblioref">Grun-Rehomme, Guggemos, et Ladiray</a> (<a href="#ref-ch15HBSA" role="doc-biblioref">2018</a>)</span>.
De la même façon que pour les autres méthodes, ils considèrent que la série initiale <span class="math inline">\(y_t\)</span> peut se décomposer entre une somme entre la tendance-cycle <span class="math inline">\(g_t\)</span> et d’un bruit blanc <span class="math inline">\(\varepsilon_t\)</span> de variance <span class="math inline">\(\sigma^2\)</span> :
<span class="math display">\[y_t = g_t+\varepsilon_t.\]</span>
Toutefois, plutôt que de directement remplacer <span class="math inline">\(g_t\)</span> par un polynôme local de degré <span class="math inline">\(d\)</span>, ils prennent en compte l’erreur d’approximation de la tendance :
<span class="math display">\[
g_t=\sum_{j=0}^{d}\beta_{j}t^{j}+\xi_{t},
\]</span>
où <span class="math inline">\(\xi_t\)</span> est un processus stochastique de moyenne nulle, autocorrélé mais non corrélé à <span class="math inline">\(\varepsilon_t\)</span>.</p>
<p>La tendance <span class="math inline">\(g_t\)</span> est estimée par une moyenne mobile :
<span class="math display">\[
\hat{g}_{t}=\sum_{s=-r}^{r}\theta_{s}y_{t+s}.
\]</span></p>
<p>Pour le filtre central, les auteurs cherchent à avoir un estimateur <span class="math inline">\(\hat g_t\)</span> qui soit sans biais (ce qui implique que <span class="math inline">\(\theta\)</span> conserve les tendances de degré <span class="math inline">\(d\)</span>) et qui minimise une somme pondérée d’un critère de <em>fidelity</em> et d’un critère de <em>smoothness</em> :
<span class="math display" id="eq:graythomsonindicators">\[\begin{equation}
Q=\alpha\underbrace{\E{(\hat{g}_{t}-g_{t})^{2}}}_{=F_{GT}}+
(1-\alpha)\underbrace{\E{ (\Delta^{p+1}\hat{g}_{t})^{2}} }_{=S_{GT}}
\tag{5.3}
\end{equation}\]</span>
La solution est un filtre symétrique qui peut s’écrire sous la forme
<span class="math display">\[
\theta=E_{\alpha}^{-1}X\left[X&#39;E_{\alpha}^{-1}X\right]^{-1}e_{1}\text{ avec }E_{\alpha}=\alpha\left(\sigma^{2}I+\Omega\right)+(1-\alpha)\left(\sigma^{2}B_{p+1}+\Gamma\right)
\]</span>
où :
<span class="math display">\[
\begin{cases}
\Omega_{jk} &amp; =cov\left(\xi_{t+j}-\xi_{t},\xi_{t+k}-\xi_{t}\right)\\
\Gamma_{jk} &amp; =cov\left(\Delta^{p+1}\xi_{t+j},\Delta^{p+1}\xi_{t+k}\right)\\
\sigma^{2}\left(B_{p+1}\right)_{jk} &amp; =cov\left(\Delta^{p+1}\varepsilon_{t+j},\Delta^{p+1}\varepsilon_{t+k}\right)
\end{cases}.
\]</span>
Les deux critères utilisés dans le programme de minimisation <a href="sec-nonparamreg.html#eq:graythomsonindicators">(5.3)</a> sont des cas particuliers du critère <span class="math inline">\(I\)</span> défini dans l’équation <a href="sec-theoriegen.html#eq:theoriegen1">(3.1)</a> :
<span class="math display">\[\begin{align*}
F_{GT}(\theta)&amp;=I(\theta,0,y_t,M_\theta y_t)\\
S_{GT}(\theta)&amp;=I(\theta,d+1,y_t,0).
\end{align*}\]</span>
La théorie générale définie dans la section <a href="sec-theoriegen.html#subsec-theoriegen">3.1</a> permet donc de retrouver les filtres de <span class="citation"><a href="#ref-GrayThomson1996" role="doc-biblioref">Gray et Thomson</a> (<a href="#ref-GrayThomson1996" role="doc-biblioref">1996</a>)</span>.</p>
<p>En ne minimisant que la <em>smoothness</em> et avec <span class="math inline">\(\xi_t=0\)</span> on retrouve le filtre d’Henderson.
En ne minimisant que la <em>fidelity</em>, cette méthode est équivalente à l’estimation de polynômes locaux par moindres carrés généralisés : on retrouve donc les filtres de <span class="citation"><a href="#ref-proietti2008" role="doc-biblioref">Proietti et Luati</a> (<a href="#ref-proietti2008" role="doc-biblioref">2008</a>)</span> avec <span class="math inline">\(\sigma^2=0\)</span> et <span class="math inline">\(\Omega =K^{-1}\)</span>, ainsi que le filtre de Macaulay.</p>
<p>L’avantage de la modélisation de Gray et Thomson est que le paramètre <span class="math inline">\(\xi_t\)</span> permet une spécification plus précise du modèle en prenant notamment en compte la corrélation entre les observations.
Par exemple, <span class="citation"><a href="#ref-mclaren2001rotation" role="doc-biblioref">McLaren et Steel</a> (<a href="#ref-mclaren2001rotation" role="doc-biblioref">2001</a>)</span> ont étudié le lien entre le plan de sondage et l’estimation de la composante tendance-cycle et de la composante saisonnière.
Cette modélisation leur permet de prendre en compte, dans l’estimation de la tendance-cycle, la structure de corrélation induite par le plan de sondage de l’enquête emploi mensuelle de l’Australie (groupe de rotations avec une période de recouvrement).
Cependant, les auteurs avertissent que dans leur simulations (et dans la modélisation de Gray et Thomson) la structure d’autocorrélation de la variable aléatoire <span class="math inline">\(\xi_t\)</span> est supposée connue.
Ce n’est généralement pas le cas en pratique, où cette structure doit être estimée, ce qui rajoute de l’incertitude dans les estimations.</p>
</div>
<div id="filtres-asymétriques" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Filtres asymétriques</h3>
<p>L’approche retenue par <span class="citation"><a href="#ref-GrayThomson1996" role="doc-biblioref">Gray et Thomson</a> (<a href="#ref-GrayThomson1996" role="doc-biblioref">1996</a>)</span> est une approche de minimisation des révisions sous contraintes.
Étant donné un filtre symétrique <span class="math inline">\(\theta^s\)</span> utilisé pour estimer la tendance au centre de la série, l’objectif est de chercher un filtre asymétrique <span class="math inline">\(v=(v_{-h},\dots,v_q)\)</span> de sorte à minimiser l’erreur quadratique moyenne de révision :
<span class="math display">\[
\E{\left(Y-\hat Y\right)^2} = 
\E{\left( \sum_{i=-h}^h\theta^s_iy_{t+s}-\sum_{i=-h}^qv_iy_{t+s} \right)^2}.
\]</span>
Les auteurs étudient deux cas :</p>
<ol style="list-style-type: decimal">
<li><p>Dans le premier cas, ils cherchent un estimateur sans biais : cela implique que <span class="math inline">\(v\)</span> conserve les mêmes tendances polynomiales que <span class="math inline">\(\theta^s\)</span>.
<span class="math inline">\(\hat Y\)</span> est alors le meilleur prédicteur linéaire sans biais — <em>best linear unbiased predictor</em> (BLUP) — de <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Dans le second cas, ils autorisent l’estimateur à être biaisé mais imposent que ce biais soit constant dans le temps : si l’on modélise localement la tendance par un polynôme de degré <span class="math inline">\(d\)</span>, cela implique que <span class="math inline">\(v\)</span> conserve les tendances polynomiales de degré <span class="math inline">\(d-1\)</span>.
<span class="math inline">\(\hat Y\)</span> est alors le meilleur prédicteur linéaire à biais constant — <em>best linear time invariant predictor</em> (BLIP) — de <span class="math inline">\(Y\)</span>.
Cela permet notamment de reproduire les filtres asymétriques de Musgrave.</p></li>
</ol>
<p>La méthode utilisée est donc très proche de celle de <span class="citation"><a href="#ref-proietti2008" role="doc-biblioref">Proietti et Luati</a> (<a href="#ref-proietti2008" role="doc-biblioref">2008</a>)</span> : on retrouve d’ailleurs le filtre DAF avec <span class="math inline">\(\sigma^2=0\)</span> et <span class="math inline">\(\Omega =K^{-1}\)</span> et en utilisant la première méthode (estimation du BLUP) et les méthodes LC (filtre de Musgrave), QL et CQ avec la seconde méthode en utilisant respectivement <span class="math inline">\(d=1\)</span>, <span class="math inline">\(d=2\)</span> et <span class="math inline">\(d=3\)</span>.</p>
<p>La théorie générale définie dans la section <a href="sec-theoriegen.html#subsec-theoriegen">3.1</a> permet également de retrouver les filtres asymétriques puisqu’ils sont construits en minimisant l’erreur quadratique moyenne des révisions sous contraintes linéaires (préservation d’un polynôme de degré <span class="math inline">\(p\)</span>).</p>
<div class="remarque">
<p>Pour la construction des filtres asymétriques, une approche alternative pourrait être d’utiliser la même méthode que celle utilisée pour construire les filtres symétriques.
C’est-à-dire minimiser <span class="math inline">\(Q\)</span> (équation <a href="sec-nonparamreg.html#eq:graythomsonindicators">(5.3)</a>) sous contrainte que le filtre asymétrique fournisse un estimateur dans biais de la tendance.
Comme discuté dans <span class="citation"><a href="#ref-GrayThomson1996" role="doc-biblioref">Gray et Thomson</a> (<a href="#ref-GrayThomson1996" role="doc-biblioref">1996</a>)</span>, les auteurs ne retiennent pas cette méthode pour deux raisons :</p>
<ul>
<li><p>Il n’est pas évident qu’il faudrait chercher à maintenir le même équilibre entre <em>smoothness</em> et <em>fidelity</em> en fin de série et au centre de la série.
Le problème rencontré en fin de série est transitoire et disparaît au fur et à mesure que l’on a de nouvelles observations.
Minimiser des critères de révision serait donc préférable puisque cela reviendrait à minimiser le coût de la transition (mais dans le cas où l’on ne minimise que la <em>fidelity</em> les deux méthodes sont équivalentes).</p></li>
<li><p>Les valeurs de la <em>fidelity</em> et de la <em>smoothness</em> ne dépendent pas du temps au centre de la série mais en dépendent en fin de série.
Ainsi, même si au centre de la série le choix des poids entre les deux critères contrôle indirectement le niveau des indicateurs, ce n’est plus le cas en fin de série.
De plus, en fin de série, cela pourrait introduire des déphasages plus importants car <span class="math inline">\(S_{GT}\)</span> dépend du temps et des valeurs passées (du fait du l’utilisation de l’opérateur différence).</p></li>
</ul>
<p>Inversement, <span class="citation"><a href="#ref-ch15HBSA" role="doc-biblioref">Grun-Rehomme, Guggemos, et Ladiray</a> (<a href="#ref-ch15HBSA" role="doc-biblioref">2018</a>)</span> justifie de ne pas intégrer le critère de révision dans leur problème car ce critère est fortement corrélé à une combinaison fixée, donc non ajustable par l’utilisateur, des critères <em>fidelity</em> et <em>timeliness</em>.</p>
</div>
<div class="summary_box">
<div class="title">
<p>Filtres locaux polynomiaux (<span class="citation"><a href="#ref-GrayThomson1996" role="doc-biblioref">Gray et Thomson</a> (<a href="#ref-GrayThomson1996" role="doc-biblioref">1996</a>)</span>)</p>
</div>
<p><strong>Avantages</strong> :</p>
<ul>
<li><p>Modèles généraux qui permettent de prendre en compte l’autocorrélation entre les observations.</p></li>
<li><p>Interprétation statistique des différentes méthodes.</p></li>
<li><p>Le filtre asymétrique est indépendant de la date d’estimation.
Toutefois, il dépend indirectement des données si le filtre est calibré sur l’I-C ratio.</p></li>
</ul>
<p><strong>Inconvénients</strong> :</p>
<ul>
<li><p>La <em>timeliness</em> n’est pas contrôlée.</p></li>
<li><p>La spécification du modèle (i.e., du paramètre <span class="math inline">\(\xi_t\)</span>) peut être compliquée : si la structure d’autocorrélation est estimée à partir des données, cela rajoute de l’incertitude dans les estimations, ce qui peut avoir des effets indésirables.</p></li>
</ul>
</div>
</div>
</div>
<div id="sec-rkhs" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Reproducing Kernel Hilbert Space (RKHS) : approche de Dagum et Bianconcini</h2>
<p>La théorie des <em>Reproducing Kernel Hilbert Space</em> (RKHS) — espaces de Hilbert à noyau reproduisant — est une théorie générale dans l’apprentissage statistique non-paramétrique qui permet d’englober un grand nombre de méthodes.
C’est par exemple le cas des méthodes de régression par moindres carrés pénalisés, des Support Vector Machine (SVM), du filtre d’Hodrick-Prescott (utilisé pour décomposer tendance et cycle) ou encore des moyennes mobiles telles que celle d’Henderson.
Ainsi, <span class="citation"><a href="#ref-dagumbianconcini2008" role="doc-biblioref">Dagum et Bianconcini</a> (<a href="#ref-dagumbianconcini2008" role="doc-biblioref">2008</a>)</span> utilise la théorie des RKHS pour approcher le filtre d’Henderson et en dériver des filtres asymétriques associés.</p>
<p>Un RKHS <span class="math inline">\(\mathbb{L}^{2}(f_{0})\)</span> est un espace de Hilbert caractérisé par un noyau qui permet de reproduire toutes les fonctions de cet espace.
Il est caractérisé par une fonction de densité <span class="math inline">\(f_0\)</span> et un produit scalaire <span class="math inline">\(\ps{\cdot}{\cdot}\)</span> définit par :
<span class="math display">\[
\left\langle U(t),V(t)\right\rangle =\E{U(t)V(t)}=\int_{\R}U(t)V(t)f_{0}(t)\ud t\quad
\forall U,V\in\mathbb{L}^{2}(f_{0}).
\]</span>
La fonction <span class="math inline">\(f_0\)</span> pondère donc chaque valeur en fonction de sa position temporelle : il s’agit de la version continue des noyaux définis dans la partie <a href="sec-nonparamreg.html#sec-kernels">5.1.2</a>.</p>
<p>Dans notre cas, on suppose que notre série initiale <span class="math inline">\(y_t\)</span> est désaisonnalisée et peut s’écrire comme la somme d’une tendance-cycle, <span class="math inline">\(TC_t\)</span>, et d’une composante irrégulière, <span class="math inline">\(I_t\)</span> (qui peut être un bruit blanc ou suivre un modèle ARIMA) :
<span class="math inline">\(y_t=TC_t+I_t.\)</span>
La tendance-cycle peut être déterministe ou stochastique. On suppose que c’est une fonction régulière du temps, elle peut être localement approchée par un polynôme de degré <span class="math inline">\(d\)</span> :
<span class="math display">\[
TC_{t+j}=TC_t(j)=a_0+a_1j+\dots+a_dj^d+\varepsilon_{t+j},\quad
j\in\llbracket-h,h\rrbracket,
\]</span>
où <span class="math inline">\(\varepsilon_t\)</span> est un bruit blanc non corrélé à <span class="math inline">\(I_t\)</span>.</p>
<p>Les coefficients <span class="math inline">\(a_0,\dots,a_d\)</span> peuvent être estimés par projection des observations au voisinage de <span class="math inline">\(y_t\)</span> sur le sous-espace <span class="math inline">\(\mathbb P_d\)</span> des polynômes de degré <span class="math inline">\(d\)</span>, ou, de manière équivalente, par minimisation de la distance entre <span class="math inline">\(y_t\)</span> et <span class="math inline">\(TC_t(j)\)</span> :
<span class="math display" id="eq:mintcrkhs">\[\begin{equation}
\underset{TC\in\mathbb P_d}{\min}\lVert y -TC \rVert^2 = 
\underset{TC\in\mathbb P_d}{\min}\int_\R (y(t+s)-TC_t(s))^2f_0(s)\ud s.
\tag{5.4}
\end{equation}\]</span>
L’espace <span class="math inline">\(\mathbb P_d\)</span> étant un espace de Hilbert à dimension finie, il admet un noyau reproduisant (voir, par exemple, <span class="citation"><a href="#ref-berlinet2004" role="doc-biblioref">Berlinet et Thomas-Agnan</a> (<a href="#ref-berlinet2004" role="doc-biblioref">2004</a>)</span>).
Il existe ainsi une fonction <span class="math inline">\(R_d(\cdot,\cdot)\)</span> telle que :
<span class="math display">\[
\forall P\in \mathbb P_d: \forall t:
R_d(t,\cdot)\in\mathbb P_d\quad\text{et}\quad
P(t)=\ps{R_d(t,\cdot)}{P(\cdot)}.
\]</span></p>
<p>Le problème <a href="sec-nonparamreg.html#eq:mintcrkhs">(5.4)</a> admet une solution unique qui dépend d’une fonction <span class="math inline">\(K_{d+1}\)</span>, appelée <em>fonction de noyau</em> (<em>kernel function</em>).
Cette fonction est dite d’ordre <span class="math inline">\(d+1\)</span> car elle conserve les polynômes de degré <span class="math inline">\(d\)</span><a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a>.
Cette solution s’écrit :
<span class="math display" id="eq:rkhssoltc">\[\begin{equation}
\widehat{TC}(t)=\int_\R y(t-s)K_{d+1}(s) \ud s.
\tag{5.5}
\end{equation}\]</span>
Généralement <span class="math inline">\(f_0(t) = 0\)</span> pour <span class="math inline">\(\lvert t \rvert&gt;1\)</span>.
Cette solution s’écrit alors :
<span class="math display" id="eq:rkhssoltc2">\[\begin{equation}
\widehat{TC}(t)=\int_{[-1,1]} y(t-s)K_{d+1}(s) \ud s.
\tag{5.6}
\end{equation}\]</span>
On peut, par ailleurs, montrer que <span class="math inline">\(K_{d+1}\)</span> s’écrit en fonction <span class="math inline">\(f_0\)</span> et du noyau reproduisant <span class="math inline">\(K(\cdot,\cdot)\)</span> et que ce dernier peut s’écrire en fonction de polynômes <span class="math inline">\((P_i)_{i\in \llbracket 0, d-1 \rrbracket}\)</span> qui forme une base orthonormée de <span class="math inline">\(\mathbb L^2(f_0)\)</span> (voir par exemple <span class="citation"><a href="#ref-berlinet1993" role="doc-biblioref">Berlinet</a> (<a href="#ref-berlinet1993" role="doc-biblioref">1993</a>)</span>) :
<span class="math display">\[
K_{d+1}(t) = R_d(t,0)f_0(t) = \sum_{i=0}^dP_i(t)P_i(0)f_0(t).
\]</span></p>
<p>De plus, dans le cas discret, la solution <a href="sec-nonparamreg.html#eq:rkhssoltc2">(5.6)</a> s’écrit comme une somme pondérée au voisinage de <span class="math inline">\(y_t\)</span> :
<span class="math display" id="eq:rkhssym">\[\begin{equation}
\widehat{TC}_t=\sum_{j=-h}^h w_j y_{t+j}\quad
\text{où} \quad
w_j=\frac{K_{d+1}(j/b)}{\sum_{i=-h}^{^h}K_{d+1}(i/b)}.
\tag{5.7}
\end{equation}\]</span>
Le paramètre <span class="math inline">\(b\)</span> est choisi de sorte que les <span class="math inline">\(2h+1\)</span> points autour de <span class="math inline">\(y_t\)</span> soient utilisés avec un poids non nul.</p>
<p>Pour les filtres asymétriques, la formule <a href="sec-nonparamreg.html#eq:rkhssym">(5.7)</a> est simplement adaptée au nombre d’observations connues :
<span class="math display" id="eq:rkhsasym">\[\begin{equation}
\forall j\in\left\llbracket -h,q\right\rrbracket\::\: w_{a,j}=\frac{K_{d+1}(j/b)}{\sum_{i=-h}^{^q}K_{d+1}(i/b)}.
\tag{5.8}
\end{equation}\]</span>
En utilisant <span class="math inline">\(b=h+1\)</span> on retrouve les filtres symétriques obtenues par polynômes locaux.</p>
<p>Comme notamment montré par <span class="citation"><a href="#ref-dagumbianconcini2016seasonal" role="doc-biblioref">Dagum et Bianconcini</a> (<a href="#ref-dagumbianconcini2016seasonal" role="doc-biblioref">2016</a>)</span>, <span class="math inline">\(K_{d+1}\)</span> peut s’exprimer simplement à partir des moments de <span class="math inline">\(f_0\)</span><a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a>.
Ainsi, notons <span class="math inline">\(H_{d+1}\)</span> la matrice de Hankel associée aux moments de <span class="math inline">\(f_0\)</span> :
<span class="math display">\[
\forall i,j\in \llbracket 0, d\rrbracket:
\left(H_{d+1}\right)_{i,j}=\ps{X^i}{X^j}=\int s^{i+j}f_0(s)\ud s.
\]</span>
Notons également <span class="math inline">\(H_{d+1}[1,t]\)</span> la matrice obtenue en remplaçant la première ligne de <span class="math inline">\(H_{d+1}\)</span> par <span class="math inline">\(\begin{pmatrix} 1 &amp; t &amp; t^2 &amp; \dots &amp; t^d\end{pmatrix}\)</span>.
On a :
<span class="math display" id="eq:rkhskernelfun">\[\begin{equation}
K_{d+1}(t)=\frac{\det{H_{d+1}[1,t]}}{\det{H_{d+1}}}f_0(t).
\tag{5.9}
\end{equation}\]</span>
C’est cette formule qui est utilisée dans le <em>package</em> <code>rjdfilters</code> pour calculer les différentes moyennes mobiles.</p>
<p>Comme discuté dans la partie <a href="sec-nonparamreg.html#sec-proietti">5.1</a>, le noyau d’Henderson dépend de la fenêtre utilisée.
Ainsi, tous les moments de l’équation <a href="sec-nonparamreg.html#eq:rkhskernelfun">(5.9)</a> doivent être recalculés pour chaque valeur de <span class="math inline">\(h\)</span>.
Pour éviter cela, <span class="citation"><a href="#ref-dagumbianconcini2008" role="doc-biblioref">Dagum et Bianconcini</a> (<a href="#ref-dagumbianconcini2008" role="doc-biblioref">2008</a>)</span> suggèrent d’utiliser le noyau quadratique (<em>biweight</em>) pour approcher le noyau d’Henderson lorsque <span class="math inline">\(h\)</span> est petit (<span class="math inline">\(h&lt; 24\)</span>) et le noyau cubique (<em>triweight</em>) lorsque <span class="math inline">\(h\)</span> est grand <span class="math inline">\(h\geq 24\)</span>.</p>
<p>Dans <span class="citation"><a href="#ref-dagumbianconcini2015new" role="doc-biblioref">Dagum et Bianconcini</a> (<a href="#ref-dagumbianconcini2015new" role="doc-biblioref">2015</a>)</span>, les auteures suggèrent de faire une sélection optimale du paramètre <span class="math inline">\(b\)</span>, par exemple en minimisant l’erreur quadratique moyenne (option <code>"frequencyresponse"</code> dans <code>rjdfilters::rkhs_filter</code>) :
<span class="math display">\[
b_{q,\Gamma}=\underset{b_q\in[h; 3h]}{\min}
2\int_{0}^{\pi}
\lvert \Gamma_s(\omega)-\Gamma_\theta(\omega)\rvert^2\ud \omega.
\]</span>
Cela suppose en fait que la série entrée <span class="math inline">\(y_t\)</span> est un bruit blanc.
En supposant <span class="math inline">\(y_t\)</span> stationnaire, les critères définis dans l’article originel peuvent donc être étendus en multipliant les quantités sous les intégrales par la densité spectrale de <span class="math inline">\(y_t\)</span> notée <span class="math inline">\(h\)</span> :
<span class="math display">\[
b_{q,\Gamma}=\underset{b_q\in[h; 3h]}{\min}
2\int_{0}^{\pi}
\lvert \Gamma_s(\omega)-\Gamma_\theta(\omega)\rvert^2h(\omega)\ud \omega.
\]</span>
Cette erreur quadratique moyenne peut également se décomposer en plusieurs termes (voir équation <a href="sec-WildiMcLeroy.html#eq:msedef">(4.1)</a> de la section <a href="sec-WildiMcLeroy.html#sec-WildiMcLeroy">4</a>) qui peuvent également être minimisés :</p>
<ul>
<li><p>l’<em>accuracy</em> qui correspond à la part de la révision liée aux différences de fonction de gain dans les fréquences liées à la tendance-cycle
<span class="math display">\[
b_{q,G}=\underset{b_q\in[h; 3h]}{\min}
2\int_{0}^{\omega_1}
\left(\rho_s(\omega)-\rho_\theta(\omega)\right)^{2} h(\omega)\ud \omega
\]</span></p></li>
<li><p>la <em>smoothness</em> qui correspond à la part de la révision liée aux différences de fonction de gain dans les fréquences liées aux résidus
<span class="math display">\[
b_{q,s}=\underset{b_q\in[h; 3h]}{\min}
2\int_{\omega_1}^{\pi}
\left(\rho_s(\omega)-\rho_\theta(\omega)\right)^{2} h(\omega)\ud \omega
\]</span></p></li>
<li><p>la <em>timeliness</em> qui correspond à la part de la révision liée au déphasage
<span class="math display">\[
b_{q,\varphi}=\underset{b_q\in[h; 3h]}{\min}
8\int_{0}^{\omega_1}
\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)h(\omega)\ud \omega
\]</span>
Dans <code>rjdfilters</code> <span class="math inline">\(h\)</span> peut être fixée à la densité spectrale d’un bruit blanc (<span class="math inline">\(h_{WN}(x)=1\)</span>, comme c’est le cas dans <span class="citation"><a href="#ref-dagumbianconcini2015new" role="doc-biblioref">Dagum et Bianconcini</a> (<a href="#ref-dagumbianconcini2015new" role="doc-biblioref">2015</a>)</span>) ou d’une marche aléatoire (<span class="math inline">\(h_{RW}(x)=\frac{1}{2(1-\cos(x))}\)</span>).</p></li>
</ul>
<div class="remarque">
<p>Pour assurer une cohérence dans les définitions entre les différentes sections, les définitions de <span class="math inline">\(b_{q,G}\)</span>, <span class="math inline">\(b_{q,\Gamma}\)</span> et <span class="math inline">\(b_{q,\varphi}\)</span> ont été légèrement modifiées par rapport à celles définies dans <span class="citation"><a href="#ref-dagumbianconcini2015new" role="doc-biblioref">Dagum et Bianconcini</a> (<a href="#ref-dagumbianconcini2015new" role="doc-biblioref">2015</a>)</span> où :</p>
<ul>
<li><p>dans <span class="math inline">\(b_{q,G}\)</span> le terme à minimiser est sous une racine carrée (sans impact sur le minimum) :
<span class="math display">\[
b_{q,G}=\underset{b_q}{\min}\sqrt{
2\int_{0}^{\pi}
\left(\rho_s(\omega)-\rho_\theta(\omega)\right)^{2}\ud \omega}
\]</span></p></li>
<li><p><span class="math inline">\(b_{q,s}\)</span> n’est pas considéré, <span class="math inline">\(b_{q,\Gamma}\)</span> et <span class="math inline">\(b_{q,G}\)</span> sont définis avec <span class="math inline">\(\omega_1=\pi\)</span></p></li>
<li><p><span class="math inline">\(b_{q,\varphi}\)</span> est défini par :
<span class="math display">\[
b_{q,\varphi}=\underset{b_q}{\min}
\sqrt{2\int_{\Omega_S}
\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)\ud \omega}
\]</span>
où <span class="math inline">\(\Omega_S=[0,2\pi/32]\)</span> est l’intervalle de fréquences associées aux cycles d’au moins 16 mois.</p></li>
<li><p>une formule différente est utilisée pour la fonction de réponse (<span class="math inline">\(\Gamma_\theta(\omega)=\sum_{k=-p}^{+f} \theta_k e^{2\pi i \omega k}\)</span>), ce qui conduit à des bornes d’intégrales légèrement différentes, sans effet sur le résultat.</p></li>
</ul>
</div>
<p>Un des inconvénients de cette méthode est qu’il n’y a pas unicité de la solution et donc qu’il y a parfois plusieurs extremum (uniquement pour le calcul de <span class="math inline">\(b_{q,\varphi}\)</span>.
Ainsi, la valeur optimal retenue par défaut par <code>rjdfilters</code> produit des discontinuités dans l’estimation de la tendance-cycle.<br />
Par ailleurs, les valeurs de <span class="math inline">\(b_{q,G}\)</span> varient fortement en fonction de si l’on retient <span class="math inline">\(\omega_1=2\pi/12\)</span> ou <span class="math inline">\(\omega_1=\pi\)</span> (tableau <a href="sec-nonparamreg.html#tab:optimalbwrkhs">5.2</a>).
Par cohérence, simplicité, nous utiliserons dans cet article les valeurs optimales présentées dans <span class="citation"><a href="#ref-dagumbianconcini2015new" role="doc-biblioref">Dagum et Bianconcini</a> (<a href="#ref-dagumbianconcini2015new" role="doc-biblioref">2015</a>)</span>.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:optimalbwrkhs">Table 5.2 : </span>Fenêtres optimales pour les filtres asymétriques associés à un filtre symétrique de 13 termes (<span class="math inline">\(h=6\)</span>) avec le noyau biweight.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
<span class="math inline">\(q=0\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(q=1\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(q=2\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(q=3\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(q=4\)</span>
</th>
<th style="text-align:center;">
<span class="math inline">\(q=5\)</span>
</th>
</tr>
</thead>
<tbody>
<tr grouplength="2">
<td colspan="7" style="border-bottom: 1px solid;">
<strong><span class="math inline">\(b_{q,\Gamma}\)</span></strong>
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
<span class="math inline">\(\omega_1 = \pi\)</span>
</td>
<td style="text-align:center;">
9,54
</td>
<td style="text-align:center;">
7,88
</td>
<td style="text-align:center;">
7,07
</td>
<td style="text-align:center;">
6,88
</td>
<td style="text-align:center;">
6,87
</td>
<td style="text-align:center;">
6,94
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
<span class="math inline">\(\omega_1 = 2\pi/12\)</span>
</td>
<td style="text-align:center;">
9,54
</td>
<td style="text-align:center;">
7,88
</td>
<td style="text-align:center;">
7,07
</td>
<td style="text-align:center;">
6,88
</td>
<td style="text-align:center;">
6,87
</td>
<td style="text-align:center;">
6,94
</td>
</tr>
<tr grouplength="2">
<td colspan="7" style="border-bottom: 1px solid;">
<strong><span class="math inline">\(b_{q,G}\)</span></strong>
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
<span class="math inline">\(\omega_1 = \pi\)</span>
</td>
<td style="text-align:center;">
11,78
</td>
<td style="text-align:center;">
9,24
</td>
<td style="text-align:center;">
7,34
</td>
<td style="text-align:center;">
6,85
</td>
<td style="text-align:center;">
6,84
</td>
<td style="text-align:center;">
6,95
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
<span class="math inline">\(\omega_1 = 2\pi/12\)</span>
</td>
<td style="text-align:center;">
8,61
</td>
<td style="text-align:center;">
7,64
</td>
<td style="text-align:center;">
6,01
</td>
<td style="text-align:center;">
6,01
</td>
<td style="text-align:center;">
6,01
</td>
<td style="text-align:center;">
6,59
</td>
</tr>
<tr grouplength="3">
<td colspan="7" style="border-bottom: 1px solid;">
<strong><span class="math inline">\(b_{q,\varphi}\)</span></strong>
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
Valeurs de l’article (avec <span class="math inline">\(\omega_1 = 2\pi/36\)</span>)
</td>
<td style="text-align:center;">
6,01
</td>
<td style="text-align:center;">
6,01
</td>
<td style="text-align:center;">
7,12
</td>
<td style="text-align:center;">
8,44
</td>
<td style="text-align:center;">
9,46
</td>
<td style="text-align:center;">
10,39
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
<span class="math inline">\(\omega_1 = 2\pi/36\)</span>
</td>
<td style="text-align:center;">
6,01
</td>
<td style="text-align:center;">
6,01
</td>
<td style="text-align:center;">
7,21
</td>
<td style="text-align:center;">
8,47
</td>
<td style="text-align:center;">
9,46
</td>
<td style="text-align:center;">
6,01
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
<span class="math inline">\(\omega_1 = 2\pi/12\)</span>
</td>
<td style="text-align:center;">
6,01
</td>
<td style="text-align:center;">
6,01
</td>
<td style="text-align:center;">
6,38
</td>
<td style="text-align:center;">
8,15
</td>
<td style="text-align:center;">
9,35
</td>
<td style="text-align:center;">
6,01
</td>
</tr>
</tbody>
</table>
<div class="summary_box">
<div id="testrkhs" class="title">
<p>RKHS filters - <span class="citation"><a href="#ref-dagumbianconcini2008" role="doc-biblioref">Dagum et Bianconcini</a> (<a href="#ref-dagumbianconcini2008" role="doc-biblioref">2008</a>)</span></p>
</div>
<p><strong>Avantages</strong> :</p>
<ul>
<li><p>Le filtre asymétrique est indépendant des données et de la date d’estimation.</p></li>
<li><p>La méthode est généralisable à des séries avec des fréquences irrégulières (par exemple avec beaucoup de valeurs manquantes).</p></li>
</ul>
<p><strong>Inconvénient</strong> :</p>
<ul>
<li>Il peut y avoir des problèmes de minimisation (notamment en minimisant la <em>timeliness</em>).</li>
</ul>
</div>
<p>Tous les critères utilisés de la sélection optimale du paramètre <span class="math inline">\(b\)</span> pouvant s’écrire comme cas particuliers du critère <span class="math inline">\(J\)</span> défini dans l’équation <a href="sec-theoriegen.html#eq:theoriegen1">(3.1)</a> (voir section <a href="sec-WildiMcLeroy.html#sec-WildiMcLeroy">4</a>), cette méthode s’inscrit dans le cadre de la théorie générale définie dans la section <a href="sec-theoriegen.html#subsec-theoriegen">3.1</a> en imposant comme contrainte linéaire que les coefficients soient sous la forme <span class="math inline">\(w_j=\frac{K_{d+1}(j/b)}{\sum_{i=-h}^{^p}K_{d+1}(i/b)}\)</span>.</p>
</div>
<div id="subsec-equivlpfst" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Liens entre les différentes méthodes</h2>
<div id="critères-de-gray-et-thomson-et-ceux-de-grun-rehomme-et-alii" class="section level3" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Critères de Gray et Thomson et ceux de Grun-Rehomme <em>et alii</em></h3>
<p>Les critères <span class="math inline">\(F_g\)</span> et <span class="math inline">\(S_g\)</span> peuvent se déduire de <span class="math inline">\(F_{GT}\)</span> et <span class="math inline">\(S_{GT}\)</span>.
Les approches de <span class="citation"><a href="#ref-GrayThomson1996" role="doc-biblioref">Gray et Thomson</a> (<a href="#ref-GrayThomson1996" role="doc-biblioref">1996</a>)</span> et <span class="citation"><a href="#ref-ch15HBSA" role="doc-biblioref">Grun-Rehomme, Guggemos, et Ladiray</a> (<a href="#ref-ch15HBSA" role="doc-biblioref">2018</a>)</span> sont donc équivalentes pour la construction de filtres symétriques.</p>
<p>Notons <span class="math inline">\(x_{t}=\begin{pmatrix}1 &amp; t &amp; t^{2} &amp; \cdots &amp; t^{d}\end{pmatrix}\)</span>, <span class="math inline">\(\beta_{t}=\begin{pmatrix}\beta_{0} &amp; \cdots &amp; \beta^{d}\end{pmatrix}&#39;\)</span>.</p>
<p>Pour le critère de <em>fidelity</em> :
<span class="math display">\[
\hat{g}_{t}-g_{t}=\left(\sum_{j=-h}^{+h}\theta_{j}x_{t+j}-x_{t}\right)\beta+\sum_{j=-h}^{+h}\theta_{j}\varepsilon_{t+j}+\sum_{j=-h}^{+h}\theta_{j}(\xi_{t+j}-\xi_{t}),
\]</span>
Si <span class="math inline">\(\theta\)</span> préserve les polynômes de degré <span class="math inline">\(d\)</span> alors <span class="math inline">\(\sum_{j=-h}^{+h}\theta_{j}x_{t+j}=x_{t}\)</span>. où <span class="math inline">\(x_{t}=\begin{pmatrix}1 &amp; t &amp; t^{2} &amp; \cdots &amp; t^{d}\end{pmatrix}\)</span>.
Puis, comme <span class="math inline">\(\xi_{t}\)</span> et <span class="math inline">\(\varepsilon_{t}\)</span> sont de moyenne nulle et sont non corrélés :
<span class="math display">\[
F_{GT}(\theta)=\E{(\hat{g}_{t}-g_{t})^{2}}=\theta^{&#39;}\left(\sigma^{2}I+\Omega\right)\theta.
\]</span>
Si <span class="math inline">\(\xi_t=0\)</span> alors <span class="math inline">\(\Omega=0\)</span> et <span class="math inline">\(F_{GT}(\theta)=F_g(\theta)\)</span>.</p>
<p>Pour la <em>smoothness</em> on a :
<span class="math display">\[
\nabla^{q}\hat{g}_{t}=\sum_{j=h}^{h}\theta_{j}\underbrace{\nabla^{q}\left(\left(x_{j}-x_{0}\right)\beta\right)}_{=0\text{ si }q\geq d+1}+\sum_{j=h}^{h}\theta_{j}\nabla^{q}\varepsilon_{t+j}+\sum_{j=h}^{h}\theta_{j}\nabla^{q}\xi_{t+j}.
\]</span>
D’où pour <span class="math inline">\(q=d+1\)</span> :
<span class="math display">\[
S_{GT}(\theta)=\E{(\nabla^{q}\hat{g}_{t})^{2}}=\theta^{&#39;}\left(\sigma^{2}B_{q}+\Gamma_{q}\right)\theta.
\]</span>
On peut par ailleurs montrer que pour toute série temporelle <span class="math inline">\(X_t\)</span>,
<span class="math display">\[
\nabla^{q}(M_{\theta}X_{t})=\left(-1\right)^{q}\sum_{k\in\Z}\left(\nabla^{q}\theta_{k}\right)X_{t+k-q}
\]</span>
avec <span class="math inline">\(\theta_k=0\)</span> pour <span class="math inline">\(|k|\geq h+1\)</span>.
Avec <span class="math inline">\(\xi_t=0\)</span> on trouve donc que <span class="math inline">\(S_{GT}(\theta)=\sigma^2S_g(\theta)\)</span>.</p>
</div>
<div id="équivalence-avec-les-moindres-carrés-pondérés" class="section level3" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Équivalence avec les moindres carrés pondérés</h3>
<p>Du fait de la forme des filtres obtenus par la méthode de <span class="citation"><a href="#ref-ch15HBSA" role="doc-biblioref">Grun-Rehomme, Guggemos, et Ladiray</a> (<a href="#ref-ch15HBSA" role="doc-biblioref">2018</a>)</span>, lorsque les contraintes imposées sont la préservation des tendances de degré <span class="math inline">\(d\)</span>, celle-ci est équivalente à une estimation locale d’une tendance polynomiale de degré <span class="math inline">\(d\)</span> par moindres carrés généralisés.
En effet, dans ce cas, la solution est <span class="math inline">\(\hat \theta = \Sigma^{-1}X_p&#39;\left(X_p\Sigma^{-1}X_p&#39;\right)^{-1}e_1\)</span> avec <span class="math inline">\(\Sigma=\alpha F+\beta S+ \gamma T\)</span>, et c’est l’estimation de la constante obtenue par moindres carrés généralisés lorsque la variance des résidus est <span class="math inline">\(\Sigma\)</span>.
L’équivalence entre les deux méthodes peut donc se voir comme un cas particulier de l’équivalence entre les moindres carrés pondérés et les moindres carrés généralisés.
C’est par exemple le cas des filtres symétriques d’Henderson qui peuvent s’obtenir par les deux méthodes.</p>
<p>Dans ce sens, <span class="citation"><a href="#ref-henderson1916note" role="doc-biblioref">Henderson</a> (<a href="#ref-henderson1916note" role="doc-biblioref">1916</a>)</span> a montré que les poids <span class="math inline">\(w=(w_{-p},\dots w_{f})\)</span> associés à une moyenne mobile issue de la régression polynomiale locale par moindres carrés pondérés pouvaient s’écrire sous la forme :
<span class="math display">\[
w_i = \kappa_i P\left(\frac{i}{p+f+1}\right)\text{ où }P\text{ est un polynôme de degré }d.
\]</span>
Il a également montré l’inverse : toute moyenne mobile <span class="math inline">\(\theta=(\theta_{-p},\dots, \theta_{f})\)</span> qui préserve les tendances de degré <span class="math inline">\(d\)</span> et dont le diagramme des coefficients change au plus <span class="math inline">\(d\)</span> fois de signes peut être obtenue par une régression polynomiale locale de degré <span class="math inline">\(p\)</span> estimée par moindres carrés pondérés.
Pour cela il suffit de trouver un polynôme <span class="math inline">\(P\left(\frac{X}{p+f+1}\right)\)</span> de degré inférieur ou égal à <span class="math inline">\(d\)</span> et dont les changements de signes coïncident avec les changements de signes de <span class="math inline">\(\theta\)</span>.
Le noyau associé est alors <span class="math inline">\(\kappa_i=\frac{ \theta_i}{P\left(\frac{i}{p+f+1}\right)}\)</span>.
C’est le cas de tous les filtres symétriques issues de l’approche FST et de la majorité des filtres asymétriques.
L’annexe <a href="an-equivfstlp.html#an-equivfstlp">C</a> présente les quelques poids pour lesquels il n’y a pas équivalence.</p>
<p>Plus récemment, <span class="citation"><a href="#ref-LuatiProietti2011" role="doc-biblioref">Luati et Proietti</a> (<a href="#ref-LuatiProietti2011" role="doc-biblioref">2011</a>)</span> se sont intéressés aux cas d’équivalences entre les moindres carrés pondérés et les moindres carrés généralisés pour déterminer des noyaux optimaux (au sens de Gauss-Markov).
Ils montrent que le noyau d’Epanechnikov est le noyau optimal associé à la régression polynomiale locale où le résidu, <span class="math inline">\(\varepsilon_t\)</span>, est un processus moyenne mobile (MA) non inversible d’ordre 1 (i.e., <span class="math inline">\(\varepsilon_t=(1-B)\xi_t\)</span>, avec <span class="math inline">\(\xi_t\)</span> un bruit blanc).
Dans ce cas, la matrice <span class="math inline">\(\Sigma\)</span> de variance-covariance correspond à la matrice obtenue par le critère de <em>smoothness</em> avec le paramètre <span class="math inline">\(q=2\)</span> (<span class="math inline">\(\sum_{j}(\nabla^{2}\theta_{j})^{2} = \theta&#39;\Sigma\theta\)</span>) : il y a donc équivalence avec l’approche FST.
De même, le noyau d’Henderson est le noyau optimal associé à la régression polynomiale locale où le résidu est un processus moyenne mobile (MA) non inversible d’ordre 2 (i.e., <span class="math inline">\(\varepsilon_t=(1-B)^2\xi_t\)</span>, avec <span class="math inline">\(\xi_t\)</span> un bruit blanc).</p>
</div>
<div id="rkhs-et-polynômes-locaux" class="section level3" number="5.5.3">
<h3><span class="header-section-number">5.5.3</span> RKHS et polynômes locaux</h3>
<p>Comme montré dans la section précédente, la théorie des espaces de Hilbert à noyau reproduisant permet de reproduire les filtres symétriques par approximation polynomiale locale.
Comme le montrent <span class="citation"><a href="#ref-LuatiProietti2011" role="doc-biblioref">Luati et Proietti</a> (<a href="#ref-LuatiProietti2011" role="doc-biblioref">2011</a>)</span>, cette théorie permet donc également de reproduire les filtres directs asymétriques (DAF), qui sont équivalents à l’approximation polynomiale locale mais en utilisant une fenêtre d’estimation asymétrique.
Cependant, ils ne peuvent pas être obtenus par la formalisation de <span class="citation"><a href="#ref-dagumbianconcini2008" role="doc-biblioref">Dagum et Bianconcini</a> (<a href="#ref-dagumbianconcini2008" role="doc-biblioref">2008</a>)</span> mais par une discrétisation différente de la formule <a href="sec-nonparamreg.html#eq:rkhskernelfun">(5.9)</a> :
<span class="math display">\[
K_{d+1}(t)=\frac{\det{H_{d+1}[1,t]}}{\det{H_{d+1}}}f_0(t).
\]</span>
Dans le cas discret, <span class="math inline">\(f_0(t)\)</span> est remplacé par <span class="math inline">\(\kappa_j\)</span> et en remplaçant les moments théoriques par les moments empiriques <span class="math inline">\(H_{d+1}\)</span> devient <span class="math inline">\(X&#39;_pK_pX_p\)</span> et les coefficients du filtre asymétrique sont obtenus en utilisant la formule :
<span class="math display">\[
w_{a,j}=\frac{\det{X&#39;_pK_pX_p[1,j]}
}{
\det{X&#39;_pK_pX_p}
}\kappa_j.
\]</span>
En effet, la règle de Cramer permet de trouver une solution explicite à l’équation des moindres carrés <span class="math inline">\((X&#39;_pK_pX_p)\hat \beta=X&#39;_pK_py_p\)</span> où <span class="math inline">\(\hat \beta_0=\hat m_t\)</span> :
<span class="math display">\[
\hat \beta_0 = \frac{\det{X&#39;_pK_pX_p[1,b]}}{\det{X&#39;_pK_pX_p}}f_0(t)
\quad\text{où}\quad b=X&#39;_pK_py_p.
\]</span>
Comme <span class="math inline">\(b=\sum_{j=-h}^qx_j\kappa_jy_{t+j}\)</span> il vient :
<span class="math display">\[
\det{X&#39;_pK_pX_p[1,b]} = \sum_{j=-h}^q\det{X&#39;_pK_pX_p[1,x_j]}\kappa_jy_{t+j}.
\]</span>
Et enfin :
<span class="math display">\[
\hat \beta_0 = \hat m_t= \sum_{j=-h}^q\frac{\det{X&#39;_pK_pX_p[1,j]}
}{
\det{X&#39;_pK_pX_p}
}\kappa_j y_{t+j}.
\]</span></p>

</div>
</div>
</div>
<h3>Références</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-berlinet1993" class="csl-entry">
Berlinet, Alain. 1993. <span>« Hierarchies of higher order kernels »</span>. <em>Probability Theory and Related Fields</em> 94: 489‑504.
</div>
<div id="ref-berlinet2004" class="csl-entry">
Berlinet, Alain, et Christine Thomas-Agnan. 2004. <em>Reproducing Kernel Hilbert Spaces in Probability and Statistics</em>. Springer.
</div>
<div id="ref-cleveland1996smoothing" class="csl-entry">
Cleveland, William S., et Clive Loader. 1996. <span>« Smoothing by local regression: Principles and methods »</span>. In <em>Statistical theory and computational aspects of smoothing</em>, 10‑49. Springer.
</div>
<div id="ref-dagumbianconcini2008" class="csl-entry">
Dagum, Estela Bee, et Silvia Bianconcini. 2008. <span>« <span>The Henderson Smoother in Reproducing Kernel Hilbert Space</span> »</span>. <em>Journal of Business &amp; Economic Statistics</em> 26: 536‑45. <a href="https://ideas.repec.org/a/bes/jnlbes/v26y2008p536-545.html">https://ideas.repec.org/a/bes/jnlbes/v26y2008p536-545.html</a>.
</div>
<div id="ref-dagumbianconcini2015new" class="csl-entry">
———. 2015. <span>« <span>A new set of asymmetric filters for tracking the short-term trend in real-time</span> »</span>. <em>The Annals of Applied Statistics</em> 9 (3): 1433‑58. <a href="https://doi.org/&quot;10.1214/15-AOAS856&quot;">https://doi.org/"10.1214/15-AOAS856"</a>.
</div>
<div id="ref-dagumbianconcini2016seasonal" class="csl-entry">
———. 2016. <em>Seasonal adjustment methods and real time trend-cycle estimation</em>. Springer.
</div>
<div id="ref-GrayThomson1996" class="csl-entry">
Gray, Alistair, et Peter Thomson. 1996. <span>« Design of Moving-Average Trend Filters using Fidelity and Smoothness Criteria »</span>. In <em>Athens Conference on Applied Probability and Time Series Analysis</em>, édité par P. M. Robinson et Murray Rosenblatt, 205‑19. New York, NY: Springer New York.
</div>
<div id="ref-ch15HBSA" class="csl-entry">
Grun-Rehomme, Michel, Fabien Guggemos, et Dominique Ladiray. 2018. <span>« Asymmetric Moving Averages Minimizing Phase Shift »</span>. <em>Handbook on Seasonal Adjustment</em>. <a href="https://ec.europa.eu/eurostat/web/products-manuals-and-guidelines/-/KS-GQ-18-001">ec.europa.eu/eurostat/web/products-manuals-and-guidelines/-/KS-GQ-18-001</a>.
</div>
<div id="ref-henderson1916note" class="csl-entry">
Henderson, Robert. 1916. <span>« Note on graduation by adjusted average »</span>. <em>Transactions of the actuarial society of America</em> 17: 43‑48.
</div>
<div id="ref-Loader1999" class="csl-entry">
Loader, Clive. 1999. <em>Local regression and likelihood</em>. New York: Springer-Verlag.
</div>
<div id="ref-LuatiProietti2011" class="csl-entry">
Luati, Alessandra, et Tommaso Proietti. 2011. <span>« On the equivalence of the weighted least squares and the generalised least squares estimators, with applications to kernel smoothing »</span>. <em>Annals of the Institute of Statistical Mathematics</em> 63 (4): 851‑71. <a href="https://doi.org/10.1007/s10463-009-0267-8">https://doi.org/10.1007/s10463-009-0267-8</a>.
</div>
<div id="ref-macaulay1931smoothing" class="csl-entry">
Macaulay, Frederick R et al. 1931. <span>« The smoothing of time series »</span>. <em>NBER Books</em>.
</div>
<div id="ref-mclaren2001rotation" class="csl-entry">
McLaren, Craig H, et David G Steel. 2001. <span>« Rotation patterns and trend estimation for repeated surveys using rotation group estimates »</span>. <em>Statistica Neerlandica</em> 55 (2): 221‑38. <a href="https://documents.uow.edu.au/~craigmc/sn_2001.pdf">https://documents.uow.edu.au/~craigmc/sn_2001.pdf</a>.
</div>
<div id="ref-musgrave1964set" class="csl-entry">
Musgrave, John C. 1964. <span>« A set of end weights to end all end weights »</span>. <em>US Census Bureau [custodian]</em>. <a href="https://www.census.gov/ts/papers/Musgrave1964a.pdf">https://www.census.gov/ts/papers/Musgrave1964a.pdf</a>.
</div>
<div id="ref-proietti2008" class="csl-entry">
Proietti, Tommaso, et Alessandra Luati. 2008. <span>« Real time estimation in local polynomial regression, with application to trend-cycle analysis »</span>. <em>Ann. Appl. Stat.</em> 2 (4): 1523‑53. <a href="https://doi.org/10.1214/08-AOAS195">https://doi.org/10.1214/08-AOAS195</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="15">
<li id="fn15"><p>La série est donc désaisonnalisée.<a href="sec-nonparamreg.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>
<span class="math inline">\(\theta\)</span> est symétrique du fait de la symétrie des noyaux <span class="math inline">\(\kappa_j\)</span>.<a href="sec-nonparamreg.html#fnref16" class="footnote-back">↩︎</a></p></li>
<li id="fn17"><p>
Voir par exemple <span class="citation"><a href="#ref-cleveland1996smoothing" role="doc-biblioref">Cleveland et Loader</a> (<a href="#ref-cleveland1996smoothing" role="doc-biblioref">1996</a>)</span> ou <span class="citation"><a href="#ref-Loader1999" role="doc-biblioref">Loader</a> (<a href="#ref-Loader1999" role="doc-biblioref">1999</a>)</span>.
Les seules contraintes souhaitées sur le noyau est qu’il accorde un poids plus important à l’estimation centrale (<span class="math inline">\(\kappa_0\)</span>) et qu’il décroit vers 0 lorsque l’on s’éloigne de l’estimation centrale.
Le noyau uniforme est donc à éviter.<a href="sec-nonparamreg.html#fnref17" class="footnote-back">↩︎</a></p></li>
<li id="fn18"><p>
Dans <code>rjdfilters</code> <span class="math inline">\(\sigma^2\)</span> est fixé arbitrairement à <span class="math inline">\(\sigma^2=0,25\)</span>.<a href="sec-nonparamreg.html#fnref18" class="footnote-back">↩︎</a></p></li>
<li id="fn19"><p>
Dans la majorité des cas un filtre de 13 termes est utilisé.
Si le ratio est grand alors un filtre de 23 termes est utilisé (pour supprimer d’avantage de bruit) et si le ratio est petit un filtre de 9 termes est utilisé.<a href="sec-nonparamreg.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p><a href="https://github.com/palatej/jdemetra-core" class="uri">https://github.com/palatej/jdemetra-core</a>.<a href="sec-nonparamreg.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>
C’est-à-dire <span class="math inline">\(\int_\R K_{d+1}(s)\ud s = 1\)</span> et <span class="math inline">\(\int_\R K_{d+1}(s) s^i\ud s = 1\)</span> pour <span class="math inline">\(i\in \llbracket 1, d\rrbracket\)</span>.<a href="sec-nonparamreg.html#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn22"><p>
Cela vient en fait du procédé d’orthonomalisation de Gram-Schmidt.<a href="sec-nonparamreg.html#fnref22" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-WildiMcLeroy.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec-comparison.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["AQLT_JMS_2022.pdf", "AQLT_JMS_2022.docx", "AQLT_JMS_2022.tex"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
